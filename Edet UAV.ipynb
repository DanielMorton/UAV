{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Edet UAV.ipynb","provenance":[{"file_id":"1UZS4QMn7dfZr5WHN1v9rmW4N6fHvUdKJ","timestamp":1620701338792},{"file_id":"1zI8F6NUxqRJqGiupvQJwYC-aNEb8BhFG","timestamp":1619725859470}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1Eaq8QeWzHEGHE9Zi7QdbPDKU-DNNQxRb","authorship_tag":"ABX9TyN7vGkg96O6qTfXKLm6/Nc1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rh5IZKuZHYF3"},"source":["Code used to train a representative EfficientDet model using seven videos as validation and the other 25 for training.\n","\n","Variations on the basic model usually involve changing the preprocessing steps."]},{"cell_type":"code","metadata":{"id":"s5bQ_ZWS-Rsd"},"source":["import numpy as np\n","import pandas as pd\n","\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","import time\n","import random\n","import warnings\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from datetime import datetime\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.model_selection import train_test_split\n","\n","DRIVE = \"/content/drive/MyDrive/CNN/UAV\"\n","IMAGE_ZIP = \"images.zip\"\n","IMAGES = f\"{DRIVE}/{IMAGE_ZIP}\"\n","ANNOTATION_DIR = f\"{DRIVE}/annotations\"\n","BASE_IMAGE_DIR = f\"/content/images\"\n","ANNOTATION_FILES = [f\"{ANNOTATION_DIR}/{d}\" for d in os.listdir(ANNOTATION_DIR) if \".csv\" in d]\n","EDETS = [d for d in os.listdir(DRIVE) if '.pth' in d]\n","ANNOTATION_FILES.sort()\n","EDETS.sort()\n","os.environ[\"DRIVE\"] = DRIVE\n","os.environ[\"IMAGE_ZIP\"] = IMAGE_ZIP\n","os.environ[\"IMAGES\"] = IMAGES\n","os.environ[\"ANNOTATION_DIR\"] = ANNOTATION_DIR\n","os.environ[\"BASE_IMAGE_DIR\"] = BASE_IMAGE_DIR\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7EuXViOuE5TU"},"source":["Load Image Data and install packages."]},{"cell_type":"code","metadata":{"id":"sBgksHlA33K4"},"source":["%%bash\n","\n","cp $IMAGES /content\n","unzip -q $IMAGE_ZIP\n","rm $IMAGE_ZIP\n","\n","pip install -U -q albumentations\n","pip install -q omegaconf\n","pip install -q timm\n","pip install -q effdet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6i3XIPcSX4dT"},"source":["IMAGE_DIRS = [f\"{BASE_IMAGE_DIR}/{d}\" for d in os.listdir(BASE_IMAGE_DIR) if d != '.DS_Store']\n","IMAGE_DIRS.sort()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDZaZ-dWKPWp"},"source":["import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n","from effdet.efficientdet import HeadNet\n","\n","TRAIN_SIZE = 512\n","VAL_SIZE = 512\n","NUM_CLASSES = 1\n","BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","D_SIZE = 0\n","\n","EPOCH = 0\n","MAX_EPOCH = 100\n","\n","LOG_LR = 4\n","COEFF_LR = 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ewnMofgQFxwX"},"source":["Set global configurations. The most important is project_folder, followed by the learning rate parameters.\n","\n","Model will print output every 250 steps."]},{"cell_type":"code","metadata":{"id":"Ypr_Eh1T-9nh"},"source":["class TrainGlobalConfig:\n","  def __init__(self, d_size,\n","               num_workers,\n","               project_folder,\n","               batch_size, n_epochs,\n","               log_lr, coeff_lr):\n","\n","    self.num_workers = num_workers\n","    self.batch_size = batch_size\n","    self.n_epochs = n_epochs\n","    self.lr = coeff_lr * 10**(-log_lr)\n","\n","    self.folder = f\"{DRIVE}/{project_folder}\"\n","\n","    self.verbose = True\n","    self.verbose_step = 250\n","\n","    self.SchedulerClass = torch.optim.lr_scheduler.MultiplicativeLR\n","    self.scheduler_params = dict(\n","        lr_lambda = lambda epoch: 0.94**0.25,\n","        verbose=True\n","    )\n","global_config = TrainGlobalConfig(batch_size=BATCH_SIZE,\n","                                  n_epochs=MAX_EPOCH,\n","                                  d_size=D_SIZE,\n","                                  num_workers=NUM_WORKERS,\n","                                  project_folder=f\"effdet{D_SIZE}-{LOG_LR}-{COEFF_LR}_wd4-5_512x640_ap\",\n","                                  log_lr=LOG_LR,\n","                                  coeff_lr=COEFF_LR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TItOs4rQ4yDv"},"source":["# Load and Transform Data"]},{"cell_type":"markdown","metadata":{"id":"-6dURsyYIXjn"},"source":["Splits the data into training and validation sets so that the valiation set has 20% of the videos and 20% of the images."]},{"cell_type":"code","metadata":{"id":"KIvkSrhQCtM_"},"source":["def get_train_and_val(rand=6189):\n","  annotation_list = []\n","  seq_list = []\n","  col_names = [\"frame_id\", \"object_id\", \"x\", \"y\", \"width\", \"height\", \"object_class\",\n","               \"species\", \"occluded\", \"noisy_frame\"]\n","  for annot, image_dir in zip(ANNOTATION_FILES, IMAGE_DIRS):\n","    if \".csv\" in annot:\n","      annotation_list.append(pd.read_csv(annot, header=None,\n","                         names=col_names))\n","      annotation_list[-1]['csv'] = annot.split(\"/\")[-1]\n","      images = [f\"{image_dir}/{d}\" for d in os.listdir(image_dir)]\n","      img_shape = cv2.imread(images[0]).shape[:2]\n","      frames = [int(img.split(\"_\")[-1].split(\".\")[0]) for img in images]\n","      frame_df = pd.DataFrame({\"frame_id\": frames, \"file\": images})\n","      frame_df[\"csv\"] = f\"{image_dir.split('/')[-1]}.csv\"\n","      frame_df[\"img_height\"] = img_shape[0]\n","      frame_df[\"img_width\"] = img_shape[1]\n","      seq_list.append(frame_df)\n","  frame_df = pd.concat(seq_list).reset_index(drop=True)\n","  annotations = pd.concat(annotation_list).merge(frame_df).reset_index(drop=True)\n","  \n","  while True:\n","    train_files, val_files = train_test_split(frame_df[['csv']].drop_duplicates(),\n","                                              test_size=0.2,\n","                                              random_state=rand)\n","    val_df = frame_df[frame_df['csv'].isin(val_files['csv'])].reset_index(drop=True)\n","    train_df = frame_df[frame_df['csv'].isin(train_files['csv'])].reset_index(drop=True)\n","    val_frac = val_df['file'].nunique()/frame_df['file'].nunique()\n","    if val_frac < 0.22 and val_frac > 0.18:\n","        break\n","    rand += 1\n","  train_annotations = annotations[annotations['csv'].isin(train_files['csv'])].reset_index(drop=True)\n","  val_annotations = annotations[annotations['csv'].isin(val_files['csv'])].reset_index(drop=True)\n","  return train_df, val_df, train_annotations, val_annotations\n","train_df, val_df, train_annotations, val_annotations = get_train_and_val()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfCFnGgMP0Y6"},"source":["**Transformations**\n","\n","Training transforms consist of crop and resize followed bu flips.\n","\n","Validation transform consists of rescaling and, if necessary, padding."]},{"cell_type":"code","metadata":{"id":"SmMDYlFTY8xg"},"source":["BBOX = A.BboxParams(\n","             format='pascal_voc',\n","             min_area=0, \n","             min_visibility=0,\n","             label_fields=['labels'])\n","\n","def get_train_transforms(img_size):\n","  \"\"\"Returns a function to perform the standard sequence of preprocessing steps\n","     for training data.\n","  \"\"\"\n","  return A.Compose([A.RandomResizedCrop(height=img_size[0], width=img_size[1],\n","                                        scale=(0.1, 1.0),\n","                                        ratio=(3/4, 4/3),\n","                                        p=1.0),\n","                    A.HorizontalFlip(p=0.5),\n","                    A.VerticalFlip(p=0.5),\n","                    A.RandomRotate90(p=1.0),\n","                    A.Transpose(p=0.5),\n","                    ToTensorV2(p=1.0)],\n","                   bbox_params=BBOX, \n","                   p=1.0)\n","\n","def get_val_transform(img_size):\n","  \"\"\"Returns a function to perform the standard sequence of preprocessing steps\n","     for validation data.\n","  \"\"\"\n","  max_size = max(img_size)\n","  return A.Compose([A.LongestMaxSize(max_size=640,\n","                                     p=1.0),\n","                    A.PadIfNeeded(min_height=512,\n","                                  min_width=640,\n","                                  border_mode=0,\n","                                  p=1.0),\n","                    ToTensorV2(p=1.0)],\n","                   bbox_params=BBOX, \n","                   p=1.0)\n","  \n","def get_default_transform(img_size):\n","  \"\"\"Returns a function to perform the default transform if the training\n","     transform fails.\n","  \"\"\"\n","  return A.Compose([A.Resize(height=img_size[0],\n","                             width=img_size[1], p=1.0),\n","                    ToTensorV2(p=1.0)], \n","                   bbox_params=BBOX,\n","                   p=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0suMvS0mP_J0"},"source":["**Generic UAV Dataset**"]},{"cell_type":"code","metadata":{"id":"wMP1ag-u-nfe"},"source":["class UAVDataset(Dataset):\n","  \n","  def __init__(self, meta_data,\n","               boxes,\n","               transform=None,\n","               image_size=None,\n","               train=False):\n","    super(UAVDataset).__init__()\n","    \n","    self.meta_data = meta_data\n","    self.boxes = boxes\n","    if type(image_size) == int:\n","      self.image_size = (image_size, image_size)\n","    else:\n","      self.image_size = image_size\n","    self.transform = transform(self.image_size) if transform else None\n","    self.train = train\n","    \n","  def _box_to_tensor(self, sample, target):\n","    \"\"\"Convert boundind box array to tensor\"\"\"\n","    if len(sample[\"bboxes\"]) > 0:\n","      target[\"bboxes\"] = torch.tensor(sample[\"bboxes\"])\n","    else:\n","      target[\"bboxes\"] = torch.zeros((0,4))\n","      # Convert bounded box to yxyx format\n","    if self.train:\n","      target[\"bboxes\"][:,[0,1,2,3]] = target[\"bboxes\"][:,[1,0,3,2]]\n","    return target\n","  \n","  def __len__(self) -> int:\n","    \"\"\"Returns the number of images.\"\"\"\n","    return self.meta_data.shape[0]\n","\n","  def load_image_and_boxes(self, image_meta, image_boxes):\n","    \"\"\"Loads image corresponding to image_meta row.\n","       Converts bounding boxes to x_min, y_min, x_max, y_max format.\n","    \"\"\"\n","    image = cv2.imread(image_meta[\"file\"]).astype(np.float32)/ 255.0\n","    bboxes = image_boxes[[\"x\", \"y\", \"width\", \"height\"]].values\n","    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n","    bboxes[bboxes[:, 2] > image_meta[\"img_width\"], 2] = image_meta[\"img_width\"]\n","    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n","    bboxes[bboxes[:, 3] > image_meta[\"img_height\"], 3] = image_meta[\"img_height\"]\n","    return image, bboxes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQ13DKCdQRIs"},"source":["**Training Data**"]},{"cell_type":"code","metadata":{"id":"hwFj1r7R_mld"},"source":["class TrainDataset(UAVDataset):\n","\n","  def __init__(self, meta_data,\n","               boxes,\n","               image_size=None,\n","               transform=None,\n","               default_transform=None,\n","               max_iter=30):\n","    super(TrainDataset, self).__init__(meta_data, boxes, transform, image_size, train=True)\n","    self.default_transform = default_transform(self.image_size) if default_transform else None\n","    self.max_iter = max_iter\n","\n","  def __getitem__(self, index: int):\n","    \"\"\"Retrieves the image and boxes with the specified index.\"\"\"\n","    image_meta = self.meta_data.loc[index]\n","    image_boxes = self.boxes[self.boxes[\"file\"] == image_meta[\"file\"]]\n","    image, bboxes = self.load_image_and_boxes(image_meta, image_boxes)\n","    labels = torch.ones((bboxes.shape[0]), dtype=torch.int64)\n","    target = {\"bboxes\": torch.tensor(bboxes),\n","              \"labels\": labels}\n","    if self.transform and target[\"bboxes\"].shape[0] == 0:\n","      sample = self.transform(image=image,\n","                              bboxes=target[\"bboxes\"],\n","                              labels=target[\"labels\"])\n","      image, target = sample[\"image\"], self._box_to_tensor(sample, target)  \n","      return image, target\n","    elif self.transform:\n","      for i in range(self.max_iter):\n","        sample = self.transform(image=image,\n","                                bboxes=target[\"bboxes\"],\n","                                labels=target[\"labels\"])\n","        if len(sample[\"bboxes\"]) > 0:\n","          image, target= sample[\"image\"], self._box_to_tensor(sample, target)\n","          target[\"labels\"] = torch.stack(sample[\"labels\"])\n","          return image, target\n","    if self.default_transform and image.shape[2] == 3:\n","      sample = self.default_transform(image=image,\n","                                      bboxes=target[\"bboxes\"],\n","                                      labels=target[\"labels\"])\n","      image, target = sample[\"image\"], self._box_to_tensor(sample, target)\n","    return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lk_ltBmQVwu"},"source":["**Validation Data**"]},{"cell_type":"code","metadata":{"id":"IQlIgEavS_8d"},"source":["class ValDataset(UAVDataset):\n","\n","  def __init__(self, meta_data,\n","               boxes,\n","               image_size,\n","               transform=None,\n","               train=False):\n","    super(ValDataset, self).__init__(meta_data, boxes, transform, image_size, train)\n","\n","  def __getitem__(self, index: int):\n","    \"\"\"Retrieves the image and boxes with the specified index.\"\"\"\n","    image_meta = self.meta_data.loc[index]\n","    image_boxes = self.boxes[self.boxes[\"file\"] == image_meta[\"file\"]]\n","    image, bboxes = self.load_image_and_boxes(image_meta, image_boxes)\n","    labels = torch.ones(bboxes.shape[0], dtype=torch.int64)\n","    target = {\"bboxes\": bboxes,\n","              \"labels\": labels}\n","    \n","    if self.transform:\n","      sample = self.transform(image=image,\n","                              bboxes=target[\"bboxes\"],\n","                              labels=target[\"labels\"])\n","      image, target = sample['image'], self._box_to_tensor(sample, target)\n","    return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dpul2XtLGdJ6"},"source":["Constructs a PyTorch dataloader from the datasets."]},{"cell_type":"code","metadata":{"id":"O0wR2uH0nQxP"},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch)) \n","\n","train_dataset = TrainDataset(meta_data=train_df,\n","                             boxes=train_annotations,\n","                             image_size=TRAIN_SIZE,\n","                             transform=get_train_transforms,\n","                             default_transform=get_default_transform)\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=global_config.batch_size,\n","    num_workers=global_config.num_workers,\n","    sampler=RandomSampler(train_dataset),\n","    pin_memory=False,\n","    drop_last=True,\n","    collate_fn=collate_fn)\n","\n","\n","val_loaders = {}\n","for csv in val_df[\"csv\"].unique():\n","  val = val_df[val_df[\"csv\"] == csv].reset_index(drop=True)\n","  img_height, img_width = val[\"img_height\"].iloc[0], val[\"img_width\"].iloc[0]\n","  annot = val_annotations[val_annotations[\"csv\"] == csv]\n","  val_dataset = ValDataset(meta_data=val,\n","                           boxes=annot,\n","                           image_size = (img_height, img_width),\n","                           transform=get_val_transform,\n","                           train=True)\n","  val_loader = torch.utils.data.DataLoader(\n","      val_dataset, \n","      batch_size=global_config.batch_size,\n","      num_workers=global_config.num_workers,\n","      sampler=SequentialSampler(val_dataset),\n","      shuffle=False,\n","      pin_memory=False,\n","      collate_fn=collate_fn)\n","  val_loaders[csv] = val_loader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dwxXPBUrGiyn"},"source":["Class to track training and validation metrics."]},{"cell_type":"code","metadata":{"id":"cHSyxJ6HjeNs"},"source":["class AverageMeter:\n","  \"\"\"Computes and stores the average and current value\"\"\"\n","  def __init__(self):\n","    self.reset()\n","\n","  def reset(self):\n","    self.current = 0\n","    self.avg = 0\n","    self.sum = 0\n","    self.count = 0\n","\n","  def update(self, val, n=1, avg=False):\n","    self.current = val\n","    self.sum += val * n if avg else val\n","    self.count += n\n","    self.avg = self.sum / self.count\n","    \n","  def concat(self, other_meter):\n","    self.current += other_meter.current\n","    self.sum += other_meter.sum\n","    self.count += other_meter.count\n","    self.avg = self.sum / self.count\n","        \n","class LossMeter():\n","  def __init__(self, loss=None, class_loss=None, box_loss=None):\n","    self.loss = loss if loss is not None else AverageMeter()\n","    self.class_loss = class_loss if class_loss is not None else AverageMeter()\n","    self.box_loss = box_loss if box_loss is not None else AverageMeter()\n","\n","  def update(self, output, n=1, avg=False):\n","    self.loss.update(output['loss'].detach().item(), n, avg)\n","    self.class_loss.update(output['class_loss'].detach().item(), n, avg)\n","    self.box_loss.update(output['box_loss'].detach().item(), n, avg)\n","\n","  def concat(self, other_meter):\n","    self.loss.concat(other_meter.loss)\n","    self.class_loss.concat(other_meter.class_loss)\n","    self.box_loss.concat(other_meter.box_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPJTIAc_GnQn"},"source":["Trains the model."]},{"cell_type":"code","metadata":{"id":"BJ-ycoVFeWqf"},"source":["class Fitter:\n","    \n","    def __init__(self, model, val_model, device, config, start_epoch=0):\n","      self.config = config\n","      self.start_epoch = start_epoch\n","      \n","      self.base_dir = config.folder\n","      if not os.path.exists(self.base_dir):\n","        os.makedirs(self.base_dir)\n","        \n","      self.log_path = f\"{self.base_dir}/log.txt\"\n","      self.best_summary_loss = None\n","\n","      self.model = model\n","      self.val_model = val_model\n","      self.device = device\n","\n","      self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr,\n","                                         weight_decay=4e-5)\n","      self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n","      self.log(f\"Fitter prepared. Device is {self.device}\")\n","\n","    def _print_line(self, summary_loss, step, total_steps, stage, t):\n","      print(\n","          f\"{stage} Step {step}/{total_steps}, \" + \\\n","          f\"summary_loss: {summary_loss.loss.avg:.5f}, \" + \\\n","          f\"class_loss: {summary_loss.class_loss.avg:.5f}, \" + \\\n","          f\"box_loss: {summary_loss.box_loss.avg:.5f}, \" + \\\n","          f\"time: {(time.time() - t):.5f}\")\n","      \n","    def _log_line(self, summary_loss, epoch, stage, t):\n","      return f\"{stage} Epoch: {epoch}, \" + \\\n","             f\"summary loss: {summary_loss.loss.avg:.5f}, \" +\\\n","             f\"class loss: {summary_loss.class_loss.avg:.5f}, \" + \\\n","             f\"box_loss: {summary_loss.box_loss.avg:.5f}, \" + \\\n","             f\"time: {(time.time() - t):.5f}\"\n","\n","    def _avg_loss(self, loss_list):\n","      loss, class_loss, box_loss = AverageMeter(), AverageMeter(), AverageMeter()\n","      for l in loss_list:\n","        loss.update(l.loss.avg)\n","        class_loss.update(l.class_loss.avg)\n","        box_loss.update(l.box_loss.avg)\n","        \n","      return LossMeter(loss, class_loss, box_loss)\n","\n","    \n","\n","    def fit(self, train_loader, validation_loaders):\n","      if self.start_epoch > 0 and not self.best_summary_loss:\n","        self.best_summary_loss = self.validation(validation_loader)\n","\n","      for epoch in range(self.start_epoch, self.config.n_epochs):\n","        if self.config.verbose:\n","          lr = self.optimizer.param_groups[0][\"lr\"]\n","          timestamp = datetime.utcnow().isoformat()\n","          self.log(f\"\\n{timestamp}\\nLR: {lr}\")\n","\n","        t = time.time()\n","        summary_loss = self.train_one_epoch(train_loader)\n","\n","        self.log(self._log_line(summary_loss, epoch, \"Train\", t))\n","        self.save(f\"{self.base_dir}/last-checkpoint.bin\", epoch)\n","\n","        t = time.time()\n","        self.val_model.model.load_state_dict(self.model.model.state_dict())\n","        ## Each validation video is processed separately\n","        summary_losses = {k: self.validation(k, vl) for k, vl in validation_loaders.items()}\n","\n","        for k, sl in summary_losses.items():\n","          self.log(self._log_line(sl, epoch, f\"Val {k}\", t))\n","        total_summary_loss = self._avg_loss(summary_losses.values())\n","        self.log(self._log_line(total_summary_loss, epoch, \"Val\", t))\n","       \n","        if not self.best_summary_loss or total_summary_loss.loss.avg < self.best_summary_loss:\n","          self.best_summary_loss = total_summary_loss.loss.avg\n","          self.model.eval()\n","          self.save(f\"{self.base_dir}/best-checkpoint-{str(epoch).zfill(3)}epoch.bin\", epoch)\n","          for path in sorted(glob(f\"{self.base_dir}/best-checkpoint-*epoch.bin\"))[:-3]:\n","            os.remove(path)\n","\n","        self.scheduler.step()\n","\n","    def train_one_epoch(self, train_loader):\n","      self.model.train()\n","      summary_loss = LossMeter()\n","      t = time.time()\n","      for step, (images, targets) in enumerate(train_loader):\n","        if self.config.verbose and step % self.config.verbose_step == 0:\n","          self._print_line(summary_loss, step, len(train_loader), \"Train\", t)\n","        images = torch.stack(images)\n","        images = images.to(self.device).float()\n","        batch_size = images.shape[0]\n","        bboxes = [target[\"bboxes\"].to(self.device).float() for target in targets]\n","        labels = [target[\"labels\"].to(self.device).float() for target in targets]\n","\n","        self.optimizer.zero_grad()\n","        \n","        output = self.model(images, {\"bbox\": bboxes, \"cls\": labels})\n","            \n","        output[\"loss\"].backward()\n","\n","        summary_loss.update(output, batch_size, avg=True)\n","\n","        self.optimizer.step()\n","        \n","      return summary_loss\n","\n","    def validation(self, csv, val_loader):\n","      self.val_model.eval()\n","      summary_loss = LossMeter()\n","      t = time.time()\n","      for step, (images, targets) in enumerate(val_loader):\n","        with torch.no_grad():\n","          images = torch.stack(images)\n","          batch_size = images.shape[0]\n","          images = images.to(self.device).float()\n","          bboxes = [target[\"bboxes\"].to(self.device).float() for target in targets]\n","          labels = [target[\"labels\"].to(self.device).float() for target in targets]\n","\n","          output = self.val_model(images, {\"bbox\": bboxes, \"cls\": labels})\n","          summary_loss.update(output, batch_size, avg=True)\n","      return summary_loss\n","\n","\n","    \n","    def save(self, path, epoch):\n","      self.model.eval()\n","      torch.save({\n","          \"model_state_dict\": self.model.model.state_dict(),\n","          \"optimizer_state_dict\": self.optimizer.state_dict(),\n","          \"scheduler_state_dict\": self.scheduler.state_dict(),\n","          \"best_summary_loss\": self.best_summary_loss,\n","          \"epoch\": epoch,\n","          }, path)\n","\n","    def load(self, path):\n","      checkpoint = torch.load(path)\n","      self.model.model.load_state_dict(checkpoint[\"model_state_dict\"])\n","      self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","      self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","      self.best_summary_loss = checkpoint[\"best_summary_loss\"]\n","      self.start_epoch = checkpoint[\"epoch\"] + 1\n","        \n","    def log(self, message):\n","      if self.config.verbose:\n","        print(message)\n","      with open(self.log_path, \"a+\") as logger:\n","        logger.write(f\"{message}\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U2e_-oNkGpue"},"source":["Extension of the benchmark training class. Needed to correct an unfixed bug in the repository."]},{"cell_type":"code","metadata":{"id":"BUeyp53_1PA6"},"source":["class ExtendDetBenchTrain(DetBenchTrain):\n","\n","  def __init__(self, model, config):\n","    super(ExtendDetBenchTrain, self).__init__(model, config)\n","\n","  def forward(self, x, target):\n","    class_out, box_out = self.model(x)\n","    cls_targets, box_targets, num_positives = self.anchor_labeler.batch_label_anchors(\n","        target['bbox'], target['cls'])\n","    loss, class_loss, box_loss = self.loss_fn(class_out, box_out, cls_targets, box_targets, num_positives)\n","    output = dict(loss=loss, class_loss=class_loss, box_loss=box_loss)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3P7O2zl9G1PX"},"source":["Loads EfficientDet.\n","\n","A lot of config copying because the original config gets frozen when teh model is created.\n","\n","Creates separate valiation models for each validation dataset since they each need their own input dimensions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gw09ZLe9DYvo","executionInfo":{"status":"ok","timestamp":1622546933608,"user_tz":240,"elapsed":7899,"user":{"displayName":"Daniel Morton","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxdX_X4qG11j5MBUgqAy-1QxhxaucRZ1oLJFwguw=s64","userId":"10030415829092908707"}},"outputId":"7d53f8b9-7f1d-4d51-b602-174e94faa5d3"},"source":["def get_net(epoch=EPOCH,\n","            global_config=global_config,\n","            num_classes=NUM_CLASSES,\n","            image_size=TRAIN_SIZE,\n","            d_size=D_SIZE):\n","  device = torch.device('cuda:0')\n","  enet_config = get_efficientdet_config(f'tf_efficientdet_d{d_size}_ap')\n","  val_config = enet_config.copy()\n","  enet_config.image_size = [image_size, image_size]\n","  val_config.image_size = [512, 640]\n","  copy_config = enet_config.copy()\n","  copy_config.num_classes = num_classes\n","  net = EfficientDet(enet_config, pretrained_backbone=False)\n","  val_net = EfficientDet(val_config, pretrained_backbone=False)\n","  val_net.reset_head(num_classes=copy_config.num_classes)\n","  val_net = ExtendDetBenchTrain(val_net, copy_config)\n","  val_net.to(device)\n","  if epoch == 0:\n","    checkpoint = torch.load(f\"{DRIVE}/{EDETS[d_size]}\")\n","    net.load_state_dict(checkpoint)\n","    net.reset_head(num_classes=copy_config.num_classes)\n","    net = ExtendDetBenchTrain(net, copy_config)\n","    net.to(device)\n","    fitter = Fitter(model=net,\n","                    val_model=val_net,\n","                    device=device, config=global_config, start_epoch=0)\n","  else:\n","    net.reset_head(num_classes=copy_config.num_classes)\n","    net = DetBenchTrain(net, copy_config)\n","    net.to(device)\n","    fitter = Fitter(model=net,\n","                    val_model=val_net,\n","                    device=device, config=global_config, start_epoch=0)\n","    fitter.load(f\"{global_config.folder}/best-checkpoint-{str(epoch).zfill(3)}epoch.bin\")\n","    \n","\n","  return fitter\n","\n","fitter = get_net()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Adjusting learning rate of group 0 to 2.0000e-04.\n","Fitter prepared. Device is cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lx0DzkB3HPcy"},"source":["Trains the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chx863oGjuyP","outputId":"dc4d262a-d4fe-4ba6-aee5-d554c8714f9c"},"source":["fitter.fit(train_loader, val_loaders)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","2021-06-01T11:28:53.263745\n","LR: 0.0002\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.93653\n","Train Step 250/4006, summary_loss: 1.31258, class_loss: 0.97609, box_loss: 0.00673, time: 73.90527\n","Train Step 500/4006, summary_loss: 1.14124, class_loss: 0.82658, box_loss: 0.00629, time: 146.35868\n","Train Step 750/4006, summary_loss: 1.08365, class_loss: 0.77856, box_loss: 0.00610, time: 219.30628\n","Train Step 1000/4006, summary_loss: 1.02438, class_loss: 0.72440, box_loss: 0.00600, time: 292.20523\n","Train Step 1250/4006, summary_loss: 0.99188, class_loss: 0.69935, box_loss: 0.00585, time: 364.74505\n","Train Step 1500/4006, summary_loss: 0.94573, class_loss: 0.66085, box_loss: 0.00570, time: 437.21034\n","Train Step 1750/4006, summary_loss: 0.91497, class_loss: 0.63478, box_loss: 0.00560, time: 509.96182\n","Train Step 2000/4006, summary_loss: 0.89428, class_loss: 0.61661, box_loss: 0.00555, time: 582.93568\n","Train Step 2250/4006, summary_loss: 0.86929, class_loss: 0.59499, box_loss: 0.00549, time: 655.89065\n","Train Step 2500/4006, summary_loss: 0.85210, class_loss: 0.58014, box_loss: 0.00544, time: 728.82787\n","Train Step 2750/4006, summary_loss: 0.83332, class_loss: 0.56368, box_loss: 0.00539, time: 801.97785\n","Train Step 3000/4006, summary_loss: 0.81615, class_loss: 0.54941, box_loss: 0.00533, time: 874.69924\n","Train Step 3250/4006, summary_loss: 0.80040, class_loss: 0.53606, box_loss: 0.00529, time: 947.60872\n","Train Step 3500/4006, summary_loss: 0.78791, class_loss: 0.52519, box_loss: 0.00525, time: 1020.59772\n","Train Step 3750/4006, summary_loss: 0.78129, class_loss: 0.52023, box_loss: 0.00522, time: 1093.67042\n","Train Step 4000/4006, summary_loss: 0.77297, class_loss: 0.51407, box_loss: 0.00518, time: 1166.25274\n","Train Epoch: 0, summary loss: 0.77255, class loss: 0.51379, box_loss: 0.00518, time: 1167.89367\n","Val 0000000067_0000000019.csv Epoch: 0, summary loss: 0.70016, class loss: 0.40985, box_loss: 0.00581, time: 130.92057\n","Val 0000000067_0000000046.csv Epoch: 0, summary loss: 0.76013, class loss: 0.53387, box_loss: 0.00453, time: 130.92367\n","Val 0000000067_0000000050.csv Epoch: 0, summary loss: 1.24712, class loss: 0.63580, box_loss: 0.01223, time: 130.95854\n","Val 0000000067_0000000052.csv Epoch: 0, summary loss: 2.46905, class loss: 1.52539, box_loss: 0.01887, time: 130.96076\n","Val 0000000354_0000000000.csv Epoch: 0, summary loss: 3.06276, class loss: 2.37132, box_loss: 0.01383, time: 130.96291\n","Val 0000000359_0000000000.csv Epoch: 0, summary loss: 0.87599, class loss: 0.54050, box_loss: 0.00671, time: 130.96496\n","Val 0000000363_0000000000.csv Epoch: 0, summary loss: 1.45598, class loss: 0.88629, box_loss: 0.01139, time: 130.96719\n","Val Epoch: 0, summary loss: 1.51017, class loss: 0.98615, box_loss: 0.01048, time: 130.96928\n","Adjusting learning rate of group 0 to 1.9693e-04.\n","\n","2021-06-01T11:50:32.701300\n","LR: 0.00019693003544236372\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.66613\n","Train Step 250/4006, summary_loss: 0.57546, class_loss: 0.34810, box_loss: 0.00455, time: 74.06728\n","Train Step 500/4006, summary_loss: 0.59385, class_loss: 0.36133, box_loss: 0.00465, time: 146.57958\n","Train Step 750/4006, summary_loss: 0.58714, class_loss: 0.35825, box_loss: 0.00458, time: 219.17958\n","Train Step 1000/4006, summary_loss: 0.59218, class_loss: 0.36430, box_loss: 0.00456, time: 291.77886\n","Train Step 1250/4006, summary_loss: 0.59407, class_loss: 0.36548, box_loss: 0.00457, time: 364.62650\n","Train Step 1500/4006, summary_loss: 0.59166, class_loss: 0.36338, box_loss: 0.00457, time: 437.91379\n","Train Step 1750/4006, summary_loss: 0.58883, class_loss: 0.36113, box_loss: 0.00455, time: 511.18386\n","Train Step 2000/4006, summary_loss: 0.58101, class_loss: 0.35619, box_loss: 0.00450, time: 584.06076\n","Train Step 2250/4006, summary_loss: 0.57903, class_loss: 0.35393, box_loss: 0.00450, time: 657.17454\n","Train Step 2500/4006, summary_loss: 0.58621, class_loss: 0.36091, box_loss: 0.00451, time: 730.42691\n","Train Step 2750/4006, summary_loss: 0.58502, class_loss: 0.36009, box_loss: 0.00450, time: 803.31893\n","Train Step 3000/4006, summary_loss: 0.58383, class_loss: 0.35896, box_loss: 0.00450, time: 876.27810\n","Train Step 3250/4006, summary_loss: 0.58334, class_loss: 0.35869, box_loss: 0.00449, time: 948.85740\n","Train Step 3500/4006, summary_loss: 0.58280, class_loss: 0.35873, box_loss: 0.00448, time: 1021.73675\n","Train Step 3750/4006, summary_loss: 0.58136, class_loss: 0.35730, box_loss: 0.00448, time: 1094.29249\n","Train Step 4000/4006, summary_loss: 0.57844, class_loss: 0.35494, box_loss: 0.00447, time: 1167.57963\n","Train Epoch: 1, summary loss: 0.57839, class loss: 0.35499, box_loss: 0.00447, time: 1169.22212\n","Val 0000000067_0000000019.csv Epoch: 1, summary loss: 0.68251, class loss: 0.40733, box_loss: 0.00550, time: 130.62698\n","Val 0000000067_0000000046.csv Epoch: 1, summary loss: 0.70699, class loss: 0.48828, box_loss: 0.00437, time: 130.63018\n","Val 0000000067_0000000050.csv Epoch: 1, summary loss: 1.09665, class loss: 0.54331, box_loss: 0.01107, time: 130.63252\n","Val 0000000067_0000000052.csv Epoch: 1, summary loss: 1.74804, class loss: 0.67391, box_loss: 0.02148, time: 130.63446\n","Val 0000000354_0000000000.csv Epoch: 1, summary loss: 1.68231, class loss: 1.16866, box_loss: 0.01027, time: 130.63656\n","Val 0000000359_0000000000.csv Epoch: 1, summary loss: 0.92932, class loss: 0.59416, box_loss: 0.00670, time: 130.63865\n","Val 0000000363_0000000000.csv Epoch: 1, summary loss: 1.22219, class loss: 0.82812, box_loss: 0.00788, time: 130.64096\n","Val Epoch: 1, summary loss: 1.15257, class loss: 0.67197, box_loss: 0.00961, time: 130.64316\n","Adjusting learning rate of group 0 to 1.9391e-04.\n","\n","2021-06-01T12:12:13.150863\n","LR: 0.00019390719429665315\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.76355\n","Train Step 250/4006, summary_loss: 0.54869, class_loss: 0.33774, box_loss: 0.00422, time: 73.81579\n","Train Step 500/4006, summary_loss: 0.54883, class_loss: 0.33939, box_loss: 0.00419, time: 146.60264\n","Train Step 750/4006, summary_loss: 0.53663, class_loss: 0.32952, box_loss: 0.00414, time: 219.80402\n","Train Step 1000/4006, summary_loss: 0.53615, class_loss: 0.32838, box_loss: 0.00416, time: 292.51026\n","Train Step 1250/4006, summary_loss: 0.54061, class_loss: 0.33095, box_loss: 0.00419, time: 365.61600\n","Train Step 1500/4006, summary_loss: 0.54340, class_loss: 0.33219, box_loss: 0.00422, time: 438.68217\n","Train Step 1750/4006, summary_loss: 0.54081, class_loss: 0.33022, box_loss: 0.00421, time: 512.01314\n","Train Step 2000/4006, summary_loss: 0.54169, class_loss: 0.33144, box_loss: 0.00420, time: 585.10591\n","Train Step 2250/4006, summary_loss: 0.54194, class_loss: 0.33156, box_loss: 0.00421, time: 657.58210\n","Train Step 2500/4006, summary_loss: 0.53977, class_loss: 0.32982, box_loss: 0.00420, time: 730.89728\n","Train Step 2750/4006, summary_loss: 0.54337, class_loss: 0.33290, box_loss: 0.00421, time: 803.98624\n","Train Step 3000/4006, summary_loss: 0.54174, class_loss: 0.33156, box_loss: 0.00420, time: 877.04718\n","Train Step 3250/4006, summary_loss: 0.54045, class_loss: 0.33023, box_loss: 0.00420, time: 949.64758\n","Train Step 3500/4006, summary_loss: 0.53639, class_loss: 0.32745, box_loss: 0.00418, time: 1022.62347\n","Train Step 3750/4006, summary_loss: 0.53479, class_loss: 0.32614, box_loss: 0.00417, time: 1095.32382\n","Train Step 4000/4006, summary_loss: 0.53266, class_loss: 0.32438, box_loss: 0.00417, time: 1167.86835\n","Train Epoch: 2, summary loss: 0.53272, class loss: 0.32445, box_loss: 0.00417, time: 1169.54231\n","Val 0000000067_0000000019.csv Epoch: 2, summary loss: 0.69946, class loss: 0.33757, box_loss: 0.00724, time: 131.35768\n","Val 0000000067_0000000046.csv Epoch: 2, summary loss: 0.52771, class loss: 0.32555, box_loss: 0.00404, time: 131.36069\n","Val 0000000067_0000000050.csv Epoch: 2, summary loss: 1.01857, class loss: 0.52716, box_loss: 0.00983, time: 131.36317\n","Val 0000000067_0000000052.csv Epoch: 2, summary loss: 1.55637, class loss: 0.84923, box_loss: 0.01414, time: 131.36509\n","Val 0000000354_0000000000.csv Epoch: 2, summary loss: 3.65513, class loss: 2.81789, box_loss: 0.01674, time: 131.36695\n","Val 0000000359_0000000000.csv Epoch: 2, summary loss: 0.97551, class loss: 0.51014, box_loss: 0.00931, time: 131.36881\n","Val 0000000363_0000000000.csv Epoch: 2, summary loss: 1.27533, class loss: 0.83314, box_loss: 0.00884, time: 131.37070\n","Val Epoch: 2, summary loss: 1.38687, class loss: 0.88581, box_loss: 0.01002, time: 131.37301\n","Adjusting learning rate of group 0 to 1.9093e-04.\n","\n","2021-06-01T12:33:54.368478\n","LR: 0.00019093075322684607\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.78464\n","Train Step 250/4006, summary_loss: 0.50105, class_loss: 0.30369, box_loss: 0.00395, time: 74.39371\n","Train Step 500/4006, summary_loss: 0.50695, class_loss: 0.30565, box_loss: 0.00403, time: 147.68716\n","Train Step 750/4006, summary_loss: 0.52073, class_loss: 0.31599, box_loss: 0.00409, time: 220.90408\n","Train Step 1000/4006, summary_loss: 0.52010, class_loss: 0.31512, box_loss: 0.00410, time: 293.59879\n","Train Step 1250/4006, summary_loss: 0.52620, class_loss: 0.31895, box_loss: 0.00415, time: 366.64331\n","Train Step 1500/4006, summary_loss: 0.52528, class_loss: 0.31872, box_loss: 0.00413, time: 439.76847\n","Train Step 1750/4006, summary_loss: 0.52203, class_loss: 0.31654, box_loss: 0.00411, time: 512.93607\n","Train Step 2000/4006, summary_loss: 0.51906, class_loss: 0.31445, box_loss: 0.00409, time: 585.99993\n","Train Step 2250/4006, summary_loss: 0.51450, class_loss: 0.31077, box_loss: 0.00407, time: 659.52183\n","Train Step 2500/4006, summary_loss: 0.51952, class_loss: 0.31608, box_loss: 0.00407, time: 732.45058\n","Train Step 2750/4006, summary_loss: 0.52107, class_loss: 0.31714, box_loss: 0.00408, time: 805.67397\n","Train Step 3000/4006, summary_loss: 0.51937, class_loss: 0.31586, box_loss: 0.00407, time: 878.67537\n","Train Step 3250/4006, summary_loss: 0.51781, class_loss: 0.31471, box_loss: 0.00406, time: 951.45451\n","Train Step 3500/4006, summary_loss: 0.51962, class_loss: 0.31625, box_loss: 0.00407, time: 1024.73949\n","Train Step 3750/4006, summary_loss: 0.51740, class_loss: 0.31417, box_loss: 0.00406, time: 1097.63866\n","Train Step 4000/4006, summary_loss: 0.51572, class_loss: 0.31259, box_loss: 0.00406, time: 1170.74222\n","Train Epoch: 3, summary loss: 0.51573, class loss: 0.31258, box_loss: 0.00406, time: 1172.39526\n","Val 0000000067_0000000019.csv Epoch: 3, summary loss: 0.57531, class loss: 0.33442, box_loss: 0.00482, time: 131.16837\n","Val 0000000067_0000000046.csv Epoch: 3, summary loss: 0.50854, class loss: 0.32978, box_loss: 0.00358, time: 131.17140\n","Val 0000000067_0000000050.csv Epoch: 3, summary loss: 0.83866, class loss: 0.45487, box_loss: 0.00768, time: 131.17377\n","Val 0000000067_0000000052.csv Epoch: 3, summary loss: 0.94581, class loss: 0.51460, box_loss: 0.00862, time: 131.17570\n","Val 0000000354_0000000000.csv Epoch: 3, summary loss: 2.72519, class loss: 2.12413, box_loss: 0.01202, time: 131.17775\n","Val 0000000359_0000000000.csv Epoch: 3, summary loss: 0.79224, class loss: 0.46726, box_loss: 0.00650, time: 131.17986\n","Val 0000000363_0000000000.csv Epoch: 3, summary loss: 1.23809, class loss: 0.87558, box_loss: 0.00725, time: 131.18194\n","Val Epoch: 3, summary loss: 1.08912, class loss: 0.72866, box_loss: 0.00721, time: 131.18407\n","Adjusting learning rate of group 0 to 1.8800e-04.\n","\n","2021-06-01T12:55:38.512437\n","LR: 0.000188\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.69434\n","Train Step 250/4006, summary_loss: 0.48060, class_loss: 0.28937, box_loss: 0.00382, time: 74.04893\n","Train Step 500/4006, summary_loss: 0.49116, class_loss: 0.29749, box_loss: 0.00387, time: 146.61530\n","Train Step 750/4006, summary_loss: 0.49050, class_loss: 0.29493, box_loss: 0.00391, time: 219.16919\n","Train Step 1000/4006, summary_loss: 0.49844, class_loss: 0.30088, box_loss: 0.00395, time: 291.49173\n","Train Step 1250/4006, summary_loss: 0.49601, class_loss: 0.29902, box_loss: 0.00394, time: 364.50643\n","Train Step 1500/4006, summary_loss: 0.49757, class_loss: 0.30090, box_loss: 0.00393, time: 437.05112\n","Train Step 1750/4006, summary_loss: 0.49842, class_loss: 0.30162, box_loss: 0.00394, time: 509.80237\n","Train Step 2000/4006, summary_loss: 0.49667, class_loss: 0.30071, box_loss: 0.00392, time: 582.34683\n","Train Step 2250/4006, summary_loss: 0.49527, class_loss: 0.29962, box_loss: 0.00391, time: 655.21737\n","Train Step 2500/4006, summary_loss: 0.49495, class_loss: 0.29917, box_loss: 0.00392, time: 728.11903\n","Train Step 2750/4006, summary_loss: 0.49690, class_loss: 0.30081, box_loss: 0.00392, time: 801.15456\n","Train Step 3000/4006, summary_loss: 0.49669, class_loss: 0.30056, box_loss: 0.00392, time: 873.77355\n","Train Step 3250/4006, summary_loss: 0.49580, class_loss: 0.30018, box_loss: 0.00391, time: 946.73900\n","Train Step 3500/4006, summary_loss: 0.49523, class_loss: 0.29943, box_loss: 0.00392, time: 1019.71945\n","Train Step 3750/4006, summary_loss: 0.49467, class_loss: 0.29869, box_loss: 0.00392, time: 1093.06620\n","Train Step 4000/4006, summary_loss: 0.49431, class_loss: 0.29807, box_loss: 0.00392, time: 1165.90078\n","Train Epoch: 4, summary loss: 0.49419, class loss: 0.29798, box_loss: 0.00392, time: 1167.56663\n","Val 0000000067_0000000019.csv Epoch: 4, summary loss: 0.59351, class loss: 0.30209, box_loss: 0.00583, time: 131.43572\n","Val 0000000067_0000000046.csv Epoch: 4, summary loss: 0.54545, class loss: 0.35213, box_loss: 0.00387, time: 131.43848\n","Val 0000000067_0000000050.csv Epoch: 4, summary loss: 0.90570, class loss: 0.49904, box_loss: 0.00813, time: 131.44100\n","Val 0000000067_0000000052.csv Epoch: 4, summary loss: 1.69711, class loss: 0.91173, box_loss: 0.01571, time: 131.44325\n","Val 0000000354_0000000000.csv Epoch: 4, summary loss: 3.17525, class loss: 2.45181, box_loss: 0.01447, time: 131.44528\n","Val 0000000359_0000000000.csv Epoch: 4, summary loss: 1.07052, class loss: 0.65705, box_loss: 0.00827, time: 131.44729\n","Val 0000000363_0000000000.csv Epoch: 4, summary loss: 1.29423, class loss: 0.87560, box_loss: 0.00837, time: 131.44932\n","Val Epoch: 4, summary loss: 1.32597, class loss: 0.86421, box_loss: 0.00924, time: 131.45130\n","Adjusting learning rate of group 0 to 1.8511e-04.\n","\n","2021-06-01T13:17:17.830336\n","LR: 0.00018511423331582188\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.85849\n","Train Step 250/4006, summary_loss: 0.49168, class_loss: 0.29462, box_loss: 0.00394, time: 74.46181\n","Train Step 500/4006, summary_loss: 0.48750, class_loss: 0.29167, box_loss: 0.00392, time: 147.25882\n","Train Step 750/4006, summary_loss: 0.48147, class_loss: 0.28758, box_loss: 0.00388, time: 220.86174\n","Train Step 1000/4006, summary_loss: 0.48473, class_loss: 0.29046, box_loss: 0.00389, time: 293.64771\n","Train Step 1250/4006, summary_loss: 0.48581, class_loss: 0.29211, box_loss: 0.00387, time: 366.63987\n","Train Step 1500/4006, summary_loss: 0.49001, class_loss: 0.29526, box_loss: 0.00390, time: 439.47217\n","Train Step 1750/4006, summary_loss: 0.48784, class_loss: 0.29375, box_loss: 0.00388, time: 512.55355\n","Train Step 2000/4006, summary_loss: 0.48470, class_loss: 0.29167, box_loss: 0.00386, time: 585.84959\n","Train Step 2250/4006, summary_loss: 0.48263, class_loss: 0.29024, box_loss: 0.00385, time: 658.72869\n","Train Step 2500/4006, summary_loss: 0.48210, class_loss: 0.29000, box_loss: 0.00384, time: 731.73848\n","Train Step 2750/4006, summary_loss: 0.47942, class_loss: 0.28864, box_loss: 0.00382, time: 804.58756\n","Train Step 3000/4006, summary_loss: 0.47808, class_loss: 0.28840, box_loss: 0.00379, time: 877.75481\n","Train Step 3250/4006, summary_loss: 0.47880, class_loss: 0.28896, box_loss: 0.00380, time: 950.98493\n","Train Step 3500/4006, summary_loss: 0.47660, class_loss: 0.28775, box_loss: 0.00378, time: 1024.43224\n","Train Step 3750/4006, summary_loss: 0.47589, class_loss: 0.28730, box_loss: 0.00377, time: 1098.05956\n","Train Step 4000/4006, summary_loss: 0.47612, class_loss: 0.28787, box_loss: 0.00377, time: 1170.99239\n","Train Epoch: 5, summary loss: 0.47608, class loss: 0.28784, box_loss: 0.00376, time: 1172.65620\n","Val 0000000067_0000000019.csv Epoch: 5, summary loss: 0.54872, class loss: 0.31501, box_loss: 0.00467, time: 131.73002\n","Val 0000000067_0000000046.csv Epoch: 5, summary loss: 0.48965, class loss: 0.30745, box_loss: 0.00364, time: 131.73325\n","Val 0000000067_0000000050.csv Epoch: 5, summary loss: 0.88791, class loss: 0.47342, box_loss: 0.00829, time: 131.73557\n","Val 0000000067_0000000052.csv Epoch: 5, summary loss: 1.39800, class loss: 0.71552, box_loss: 0.01365, time: 131.73745\n","Val 0000000354_0000000000.csv Epoch: 5, summary loss: 3.25708, class loss: 2.59090, box_loss: 0.01332, time: 131.73943\n","Val 0000000359_0000000000.csv Epoch: 5, summary loss: 0.99342, class loss: 0.71480, box_loss: 0.00557, time: 131.74161\n","Val 0000000363_0000000000.csv Epoch: 5, summary loss: 1.57502, class loss: 1.15290, box_loss: 0.00844, time: 131.74375\n","Val Epoch: 5, summary loss: 1.30712, class loss: 0.89571, box_loss: 0.00823, time: 131.75780\n","Adjusting learning rate of group 0 to 1.8227e-04.\n","\n","2021-06-01T13:39:02.537006\n","LR: 0.00018227276263885396\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.56095\n","Train Step 250/4006, summary_loss: 0.45377, class_loss: 0.26828, box_loss: 0.00371, time: 74.06797\n","Train Step 500/4006, summary_loss: 0.45635, class_loss: 0.27364, box_loss: 0.00365, time: 146.83080\n","Train Step 750/4006, summary_loss: 0.45654, class_loss: 0.27628, box_loss: 0.00361, time: 219.67267\n","Train Step 1000/4006, summary_loss: 0.46267, class_loss: 0.28074, box_loss: 0.00364, time: 292.90521\n","Train Step 1250/4006, summary_loss: 0.46370, class_loss: 0.28056, box_loss: 0.00366, time: 366.01996\n","Train Step 1500/4006, summary_loss: 0.46459, class_loss: 0.28000, box_loss: 0.00369, time: 438.93192\n","Train Step 1750/4006, summary_loss: 0.46367, class_loss: 0.27973, box_loss: 0.00368, time: 511.77818\n","Train Step 2000/4006, summary_loss: 0.46680, class_loss: 0.28187, box_loss: 0.00370, time: 584.85311\n","Train Step 2250/4006, summary_loss: 0.46625, class_loss: 0.28117, box_loss: 0.00370, time: 658.08093\n","Train Step 2500/4006, summary_loss: 0.46478, class_loss: 0.28077, box_loss: 0.00368, time: 731.40242\n","Train Step 2750/4006, summary_loss: 0.46332, class_loss: 0.27982, box_loss: 0.00367, time: 805.06113\n","Train Step 3000/4006, summary_loss: 0.46232, class_loss: 0.27898, box_loss: 0.00367, time: 878.26497\n","Train Step 3250/4006, summary_loss: 0.46116, class_loss: 0.27789, box_loss: 0.00367, time: 951.11768\n","Train Step 3500/4006, summary_loss: 0.46146, class_loss: 0.27798, box_loss: 0.00367, time: 1024.37702\n","Train Step 3750/4006, summary_loss: 0.46350, class_loss: 0.28005, box_loss: 0.00367, time: 1097.35001\n","Train Step 4000/4006, summary_loss: 0.46407, class_loss: 0.28056, box_loss: 0.00367, time: 1170.51867\n","Train Epoch: 6, summary loss: 0.46406, class loss: 0.28057, box_loss: 0.00367, time: 1172.19234\n","Val 0000000067_0000000019.csv Epoch: 6, summary loss: 0.63024, class loss: 0.27218, box_loss: 0.00716, time: 131.08014\n","Val 0000000067_0000000046.csv Epoch: 6, summary loss: 0.49299, class loss: 0.27531, box_loss: 0.00435, time: 131.08313\n","Val 0000000067_0000000050.csv Epoch: 6, summary loss: 0.90138, class loss: 0.48058, box_loss: 0.00842, time: 131.08548\n","Val 0000000067_0000000052.csv Epoch: 6, summary loss: 1.09572, class loss: 0.69011, box_loss: 0.00811, time: 131.08752\n","Val 0000000354_0000000000.csv Epoch: 6, summary loss: 3.90300, class loss: 2.83425, box_loss: 0.02137, time: 131.08966\n","Val 0000000359_0000000000.csv Epoch: 6, summary loss: 0.96523, class loss: 0.52102, box_loss: 0.00888, time: 131.09168\n","Val 0000000363_0000000000.csv Epoch: 6, summary loss: 1.59409, class loss: 1.17260, box_loss: 0.00843, time: 131.09373\n","Val Epoch: 6, summary loss: 1.36895, class loss: 0.89229, box_loss: 0.00953, time: 131.09583\n","Adjusting learning rate of group 0 to 1.7947e-04.\n","\n","2021-06-01T14:00:46.121877\n","LR: 0.0001794749080332353\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.80926\n","Train Step 250/4006, summary_loss: 0.44702, class_loss: 0.26637, box_loss: 0.00361, time: 74.46696\n","Train Step 500/4006, summary_loss: 0.44704, class_loss: 0.26679, box_loss: 0.00361, time: 147.26589\n","Train Step 750/4006, summary_loss: 0.45011, class_loss: 0.27034, box_loss: 0.00360, time: 220.53776\n","Train Step 1000/4006, summary_loss: 0.46203, class_loss: 0.28077, box_loss: 0.00363, time: 293.61922\n","Train Step 1250/4006, summary_loss: 0.46912, class_loss: 0.28715, box_loss: 0.00364, time: 366.64349\n","Train Step 1500/4006, summary_loss: 0.47012, class_loss: 0.28777, box_loss: 0.00365, time: 439.73689\n","Train Step 1750/4006, summary_loss: 0.46963, class_loss: 0.28686, box_loss: 0.00366, time: 512.83512\n","Train Step 2000/4006, summary_loss: 0.46591, class_loss: 0.28397, box_loss: 0.00364, time: 585.56317\n","Train Step 2250/4006, summary_loss: 0.46293, class_loss: 0.28147, box_loss: 0.00363, time: 658.53195\n","Train Step 2500/4006, summary_loss: 0.46151, class_loss: 0.27985, box_loss: 0.00363, time: 731.98972\n","Train Step 2750/4006, summary_loss: 0.45908, class_loss: 0.27780, box_loss: 0.00363, time: 804.67310\n","Train Step 3000/4006, summary_loss: 0.45693, class_loss: 0.27643, box_loss: 0.00361, time: 877.76494\n","Train Step 3250/4006, summary_loss: 0.45482, class_loss: 0.27534, box_loss: 0.00359, time: 951.04335\n","Train Step 3500/4006, summary_loss: 0.45382, class_loss: 0.27447, box_loss: 0.00359, time: 1023.96722\n","Train Step 3750/4006, summary_loss: 0.45412, class_loss: 0.27468, box_loss: 0.00359, time: 1097.36412\n","Train Step 4000/4006, summary_loss: 0.45277, class_loss: 0.27394, box_loss: 0.00358, time: 1170.56901\n","Train Epoch: 7, summary loss: 0.45274, class loss: 0.27392, box_loss: 0.00358, time: 1172.24404\n","Val 0000000067_0000000019.csv Epoch: 7, summary loss: 0.47522, class loss: 0.25774, box_loss: 0.00435, time: 131.62978\n","Val 0000000067_0000000046.csv Epoch: 7, summary loss: 0.49997, class loss: 0.31078, box_loss: 0.00378, time: 131.63247\n","Val 0000000067_0000000050.csv Epoch: 7, summary loss: 0.82521, class loss: 0.46282, box_loss: 0.00725, time: 131.63499\n","Val 0000000067_0000000052.csv Epoch: 7, summary loss: 1.11331, class loss: 0.66242, box_loss: 0.00902, time: 131.63691\n","Val 0000000354_0000000000.csv Epoch: 7, summary loss: 2.73436, class loss: 2.16470, box_loss: 0.01139, time: 131.63903\n","Val 0000000359_0000000000.csv Epoch: 7, summary loss: 0.95268, class loss: 0.67086, box_loss: 0.00564, time: 131.64109\n","Val 0000000363_0000000000.csv Epoch: 7, summary loss: 2.25067, class loss: 1.82459, box_loss: 0.00852, time: 131.64310\n","Val Epoch: 7, summary loss: 1.26449, class loss: 0.90770, box_loss: 0.00714, time: 131.64515\n","Adjusting learning rate of group 0 to 1.7672e-04.\n","\n","2021-06-01T14:22:30.309069\n","LR: 0.00017671999999999997\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.66233\n","Train Step 250/4006, summary_loss: 0.43615, class_loss: 0.25837, box_loss: 0.00356, time: 74.30626\n","Train Step 500/4006, summary_loss: 0.43183, class_loss: 0.25859, box_loss: 0.00346, time: 148.22325\n","Train Step 750/4006, summary_loss: 0.43819, class_loss: 0.26422, box_loss: 0.00348, time: 221.74900\n","Train Step 1000/4006, summary_loss: 0.43720, class_loss: 0.26378, box_loss: 0.00347, time: 295.21283\n","Train Step 1250/4006, summary_loss: 0.43651, class_loss: 0.26339, box_loss: 0.00346, time: 368.27880\n","Train Step 1500/4006, summary_loss: 0.43874, class_loss: 0.26435, box_loss: 0.00349, time: 441.60489\n","Train Step 1750/4006, summary_loss: 0.44007, class_loss: 0.26563, box_loss: 0.00349, time: 515.13382\n","Train Step 2000/4006, summary_loss: 0.44015, class_loss: 0.26581, box_loss: 0.00349, time: 588.38800\n","Train Step 2250/4006, summary_loss: 0.43914, class_loss: 0.26490, box_loss: 0.00348, time: 661.65571\n","Train Step 2500/4006, summary_loss: 0.43872, class_loss: 0.26458, box_loss: 0.00348, time: 734.70200\n","Train Step 2750/4006, summary_loss: 0.44219, class_loss: 0.26747, box_loss: 0.00349, time: 808.36775\n","Train Step 3000/4006, summary_loss: 0.44188, class_loss: 0.26690, box_loss: 0.00350, time: 881.74795\n","Train Step 3250/4006, summary_loss: 0.44071, class_loss: 0.26581, box_loss: 0.00350, time: 954.97491\n","Train Step 3500/4006, summary_loss: 0.44024, class_loss: 0.26568, box_loss: 0.00349, time: 1028.03793\n","Train Step 3750/4006, summary_loss: 0.43869, class_loss: 0.26458, box_loss: 0.00348, time: 1101.02112\n","Train Step 4000/4006, summary_loss: 0.43897, class_loss: 0.26490, box_loss: 0.00348, time: 1174.10385\n","Train Epoch: 8, summary loss: 0.43894, class loss: 0.26488, box_loss: 0.00348, time: 1175.76258\n","Val 0000000067_0000000019.csv Epoch: 8, summary loss: 0.58197, class loss: 0.34073, box_loss: 0.00482, time: 131.51127\n","Val 0000000067_0000000046.csv Epoch: 8, summary loss: 0.56490, class loss: 0.36218, box_loss: 0.00405, time: 131.51401\n","Val 0000000067_0000000050.csv Epoch: 8, summary loss: 0.83357, class loss: 0.42430, box_loss: 0.00819, time: 131.51659\n","Val 0000000067_0000000052.csv Epoch: 8, summary loss: 1.15706, class loss: 0.59249, box_loss: 0.01129, time: 131.51873\n","Val 0000000354_0000000000.csv Epoch: 8, summary loss: 3.28532, class loss: 2.58817, box_loss: 0.01394, time: 131.52098\n","Val 0000000359_0000000000.csv Epoch: 8, summary loss: 0.99877, class loss: 0.65152, box_loss: 0.00695, time: 131.52309\n","Val 0000000363_0000000000.csv Epoch: 8, summary loss: 1.42812, class loss: 1.05923, box_loss: 0.00738, time: 131.52518\n","Val Epoch: 8, summary loss: 1.26424, class loss: 0.85980, box_loss: 0.00809, time: 131.52739\n","Adjusting learning rate of group 0 to 1.7401e-04.\n","\n","2021-06-01T14:44:17.887447\n","LR: 0.00017400737931687257\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 1.14042\n","Train Step 250/4006, summary_loss: 0.44097, class_loss: 0.26467, box_loss: 0.00353, time: 74.35651\n","Train Step 500/4006, summary_loss: 0.44188, class_loss: 0.26796, box_loss: 0.00348, time: 147.12573\n","Train Step 750/4006, summary_loss: 0.44296, class_loss: 0.26833, box_loss: 0.00349, time: 220.85337\n","Train Step 1000/4006, summary_loss: 0.43973, class_loss: 0.26688, box_loss: 0.00346, time: 293.80671\n","Train Step 1250/4006, summary_loss: 0.44145, class_loss: 0.26766, box_loss: 0.00348, time: 366.39943\n","Train Step 1500/4006, summary_loss: 0.43777, class_loss: 0.26532, box_loss: 0.00345, time: 439.78513\n","Train Step 1750/4006, summary_loss: 0.43575, class_loss: 0.26461, box_loss: 0.00342, time: 512.52332\n","Train Step 2000/4006, summary_loss: 0.43325, class_loss: 0.26279, box_loss: 0.00341, time: 585.77144\n","Train Step 2250/4006, summary_loss: 0.43144, class_loss: 0.26153, box_loss: 0.00340, time: 658.82341\n","Train Step 2500/4006, summary_loss: 0.43172, class_loss: 0.26196, box_loss: 0.00340, time: 731.71918\n","Train Step 2750/4006, summary_loss: 0.43264, class_loss: 0.26243, box_loss: 0.00340, time: 805.04142\n","Train Step 3000/4006, summary_loss: 0.43741, class_loss: 0.26622, box_loss: 0.00342, time: 877.94948\n","Train Step 3250/4006, summary_loss: 0.43784, class_loss: 0.26630, box_loss: 0.00343, time: 951.07509\n","Train Step 3500/4006, summary_loss: 0.43677, class_loss: 0.26545, box_loss: 0.00343, time: 1024.11350\n","Train Step 3750/4006, summary_loss: 0.43671, class_loss: 0.26536, box_loss: 0.00343, time: 1096.97326\n","Train Step 4000/4006, summary_loss: 0.43755, class_loss: 0.26598, box_loss: 0.00343, time: 1170.08544\n","Train Epoch: 9, summary loss: 0.43748, class loss: 0.26594, box_loss: 0.00343, time: 1171.74104\n","Val 0000000067_0000000019.csv Epoch: 9, summary loss: 0.50618, class loss: 0.26764, box_loss: 0.00477, time: 131.29825\n","Val 0000000067_0000000046.csv Epoch: 9, summary loss: 0.47390, class loss: 0.27457, box_loss: 0.00399, time: 131.31019\n","Val 0000000067_0000000050.csv Epoch: 9, summary loss: 0.82342, class loss: 0.47212, box_loss: 0.00703, time: 131.31249\n","Val 0000000067_0000000052.csv Epoch: 9, summary loss: 1.14884, class loss: 0.70604, box_loss: 0.00886, time: 131.31467\n","Val 0000000354_0000000000.csv Epoch: 9, summary loss: 3.09358, class loss: 2.27530, box_loss: 0.01637, time: 131.31678\n","Val 0000000359_0000000000.csv Epoch: 9, summary loss: 0.95626, class loss: 0.55213, box_loss: 0.00808, time: 131.31904\n","Val 0000000363_0000000000.csv Epoch: 9, summary loss: 1.61945, class loss: 1.16348, box_loss: 0.00912, time: 131.32918\n","Val Epoch: 9, summary loss: 1.23166, class loss: 0.81590, box_loss: 0.00832, time: 131.33135\n","Adjusting learning rate of group 0 to 1.7134e-04.\n","\n","2021-06-01T15:06:01.252754\n","LR: 0.0001713363968805227\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.75068\n","Train Step 250/4006, summary_loss: 0.42907, class_loss: 0.26290, box_loss: 0.00332, time: 74.10637\n","Train Step 500/4006, summary_loss: 0.42235, class_loss: 0.25659, box_loss: 0.00332, time: 146.91808\n","Train Step 750/4006, summary_loss: 0.42547, class_loss: 0.25769, box_loss: 0.00336, time: 220.29266\n","Train Step 1000/4006, summary_loss: 0.42767, class_loss: 0.25784, box_loss: 0.00340, time: 293.18340\n","Train Step 1250/4006, summary_loss: 0.43152, class_loss: 0.26308, box_loss: 0.00337, time: 366.37494\n","Train Step 1500/4006, summary_loss: 0.42948, class_loss: 0.26280, box_loss: 0.00333, time: 439.25812\n","Train Step 1750/4006, summary_loss: 0.42666, class_loss: 0.26083, box_loss: 0.00332, time: 512.51495\n","Train Step 2000/4006, summary_loss: 0.42614, class_loss: 0.26071, box_loss: 0.00331, time: 585.17597\n","Train Step 2250/4006, summary_loss: 0.42560, class_loss: 0.25981, box_loss: 0.00332, time: 658.43121\n","Train Step 2500/4006, summary_loss: 0.42416, class_loss: 0.25894, box_loss: 0.00330, time: 731.18076\n","Train Step 2750/4006, summary_loss: 0.42445, class_loss: 0.25874, box_loss: 0.00331, time: 804.04336\n","Train Step 3000/4006, summary_loss: 0.42490, class_loss: 0.25899, box_loss: 0.00332, time: 877.35160\n","Train Step 3250/4006, summary_loss: 0.42377, class_loss: 0.25805, box_loss: 0.00331, time: 950.67820\n","Train Step 3500/4006, summary_loss: 0.42329, class_loss: 0.25760, box_loss: 0.00331, time: 1023.95817\n","Train Step 3750/4006, summary_loss: 0.42262, class_loss: 0.25698, box_loss: 0.00331, time: 1097.02235\n","Train Step 4000/4006, summary_loss: 0.42144, class_loss: 0.25627, box_loss: 0.00330, time: 1170.02570\n","Train Epoch: 10, summary loss: 0.42147, class loss: 0.25631, box_loss: 0.00330, time: 1171.69192\n","Val 0000000067_0000000019.csv Epoch: 10, summary loss: 0.52594, class loss: 0.27896, box_loss: 0.00494, time: 132.00752\n","Val 0000000067_0000000046.csv Epoch: 10, summary loss: 0.50683, class loss: 0.30215, box_loss: 0.00409, time: 132.01097\n","Val 0000000067_0000000050.csv Epoch: 10, summary loss: 0.89989, class loss: 0.47608, box_loss: 0.00848, time: 132.01337\n","Val 0000000067_0000000052.csv Epoch: 10, summary loss: 2.16378, class loss: 1.25463, box_loss: 0.01818, time: 132.01555\n","Val 0000000354_0000000000.csv Epoch: 10, summary loss: 5.55254, class loss: 4.72181, box_loss: 0.01661, time: 132.01763\n","Val 0000000359_0000000000.csv Epoch: 10, summary loss: 0.94277, class loss: 0.52806, box_loss: 0.00829, time: 132.01969\n","Val 0000000363_0000000000.csv Epoch: 10, summary loss: 1.30158, class loss: 0.88487, box_loss: 0.00833, time: 132.02176\n","Val Epoch: 10, summary loss: 1.69905, class loss: 1.20665, box_loss: 0.00985, time: 132.02397\n","Adjusting learning rate of group 0 to 1.6871e-04.\n","\n","2021-06-01T15:27:45.348880\n","LR: 0.00016870641355124118\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.89147\n","Train Step 250/4006, summary_loss: 0.44845, class_loss: 0.28303, box_loss: 0.00331, time: 74.39209\n","Train Step 500/4006, summary_loss: 0.43118, class_loss: 0.26647, box_loss: 0.00329, time: 147.84645\n","Train Step 750/4006, summary_loss: 0.42751, class_loss: 0.26107, box_loss: 0.00333, time: 221.24081\n","Train Step 1000/4006, summary_loss: 0.42089, class_loss: 0.25611, box_loss: 0.00330, time: 294.78327\n","Train Step 1250/4006, summary_loss: 0.42185, class_loss: 0.25682, box_loss: 0.00330, time: 367.97505\n","Train Step 1500/4006, summary_loss: 0.42251, class_loss: 0.25646, box_loss: 0.00332, time: 441.99482\n","Train Step 1750/4006, summary_loss: 0.42323, class_loss: 0.25531, box_loss: 0.00336, time: 515.48252\n","Train Step 2000/4006, summary_loss: 0.42428, class_loss: 0.25574, box_loss: 0.00337, time: 589.03532\n","Train Step 2250/4006, summary_loss: 0.42635, class_loss: 0.25679, box_loss: 0.00339, time: 662.65408\n","Train Step 2500/4006, summary_loss: 0.42315, class_loss: 0.25485, box_loss: 0.00337, time: 736.06404\n","Train Step 2750/4006, summary_loss: 0.42325, class_loss: 0.25518, box_loss: 0.00336, time: 809.64893\n","Train Step 3000/4006, summary_loss: 0.42299, class_loss: 0.25501, box_loss: 0.00336, time: 882.98697\n","Train Step 3250/4006, summary_loss: 0.42229, class_loss: 0.25433, box_loss: 0.00336, time: 956.14349\n","Train Step 3500/4006, summary_loss: 0.42124, class_loss: 0.25362, box_loss: 0.00335, time: 1029.34400\n","Train Step 3750/4006, summary_loss: 0.42158, class_loss: 0.25443, box_loss: 0.00334, time: 1102.64032\n","Train Step 4000/4006, summary_loss: 0.42261, class_loss: 0.25553, box_loss: 0.00334, time: 1175.78535\n","Train Epoch: 11, summary loss: 0.42258, class loss: 0.25552, box_loss: 0.00334, time: 1177.46747\n","Val 0000000067_0000000019.csv Epoch: 11, summary loss: 0.56739, class loss: 0.29179, box_loss: 0.00551, time: 131.59169\n","Val 0000000067_0000000046.csv Epoch: 11, summary loss: 0.48891, class loss: 0.29473, box_loss: 0.00388, time: 131.59472\n","Val 0000000067_0000000050.csv Epoch: 11, summary loss: 0.83875, class loss: 0.47135, box_loss: 0.00735, time: 131.59713\n","Val 0000000067_0000000052.csv Epoch: 11, summary loss: 1.32284, class loss: 0.77542, box_loss: 0.01095, time: 131.59907\n","Val 0000000354_0000000000.csv Epoch: 11, summary loss: 2.51256, class loss: 1.75136, box_loss: 0.01522, time: 131.60095\n","Val 0000000359_0000000000.csv Epoch: 11, summary loss: 0.80443, class loss: 0.52272, box_loss: 0.00563, time: 131.60283\n","Val 0000000363_0000000000.csv Epoch: 11, summary loss: 1.51127, class loss: 1.06451, box_loss: 0.00894, time: 131.60469\n","Val Epoch: 11, summary loss: 1.14945, class loss: 0.73884, box_loss: 0.00821, time: 131.60662\n","Adjusting learning rate of group 0 to 1.6612e-04.\n","\n","2021-06-01T15:49:34.734288\n","LR: 0.00016611679999999998\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.71261\n","Train Step 250/4006, summary_loss: 0.42269, class_loss: 0.25265, box_loss: 0.00340, time: 74.75567\n","Train Step 500/4006, summary_loss: 0.42239, class_loss: 0.25288, box_loss: 0.00339, time: 148.05929\n","Train Step 750/4006, summary_loss: 0.41862, class_loss: 0.24986, box_loss: 0.00338, time: 221.14844\n","Train Step 1000/4006, summary_loss: 0.41522, class_loss: 0.24853, box_loss: 0.00333, time: 294.29766\n","Train Step 1250/4006, summary_loss: 0.41262, class_loss: 0.24712, box_loss: 0.00331, time: 367.12176\n","Train Step 1500/4006, summary_loss: 0.41012, class_loss: 0.24584, box_loss: 0.00329, time: 440.10984\n","Train Step 1750/4006, summary_loss: 0.41170, class_loss: 0.24789, box_loss: 0.00328, time: 513.17178\n","Train Step 2000/4006, summary_loss: 0.41071, class_loss: 0.24751, box_loss: 0.00326, time: 586.43158\n","Train Step 2250/4006, summary_loss: 0.41325, class_loss: 0.24948, box_loss: 0.00328, time: 659.26766\n","Train Step 2500/4006, summary_loss: 0.41498, class_loss: 0.25142, box_loss: 0.00327, time: 732.53395\n","Train Step 2750/4006, summary_loss: 0.41452, class_loss: 0.25118, box_loss: 0.00327, time: 806.18955\n","Train Step 3000/4006, summary_loss: 0.41283, class_loss: 0.24997, box_loss: 0.00326, time: 879.56548\n","Train Step 3250/4006, summary_loss: 0.41230, class_loss: 0.24976, box_loss: 0.00325, time: 953.18320\n","Train Step 3500/4006, summary_loss: 0.41195, class_loss: 0.24940, box_loss: 0.00325, time: 1026.65521\n","Train Step 3750/4006, summary_loss: 0.41101, class_loss: 0.24857, box_loss: 0.00325, time: 1099.67945\n","Train Step 4000/4006, summary_loss: 0.41125, class_loss: 0.24866, box_loss: 0.00325, time: 1173.34999\n","Train Epoch: 12, summary loss: 0.41114, class loss: 0.24860, box_loss: 0.00325, time: 1174.99583\n","Val 0000000067_0000000019.csv Epoch: 12, summary loss: 0.54628, class loss: 0.30457, box_loss: 0.00483, time: 131.87742\n","Val 0000000067_0000000046.csv Epoch: 12, summary loss: 0.57872, class loss: 0.36237, box_loss: 0.00433, time: 131.88094\n","Val 0000000067_0000000050.csv Epoch: 12, summary loss: 0.84085, class loss: 0.44291, box_loss: 0.00796, time: 131.88335\n","Val 0000000067_0000000052.csv Epoch: 12, summary loss: 1.34525, class loss: 0.72179, box_loss: 0.01247, time: 131.88551\n","Val 0000000354_0000000000.csv Epoch: 12, summary loss: 2.30450, class loss: 1.42918, box_loss: 0.01751, time: 131.88769\n","Val 0000000359_0000000000.csv Epoch: 12, summary loss: 0.98893, class loss: 0.54871, box_loss: 0.00880, time: 131.88988\n","Val 0000000363_0000000000.csv Epoch: 12, summary loss: 1.41883, class loss: 0.97247, box_loss: 0.00893, time: 131.89204\n","Val Epoch: 12, summary loss: 1.14619, class loss: 0.68314, box_loss: 0.00926, time: 131.89427\n","Adjusting learning rate of group 0 to 1.6357e-04.\n","\n","2021-06-01T16:11:21.920482\n","LR: 0.0001635669365578602\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.82985\n","Train Step 250/4006, summary_loss: 0.38781, class_loss: 0.23466, box_loss: 0.00306, time: 74.67289\n","Train Step 500/4006, summary_loss: 0.39121, class_loss: 0.23585, box_loss: 0.00311, time: 147.59318\n","Train Step 750/4006, summary_loss: 0.39650, class_loss: 0.23839, box_loss: 0.00316, time: 221.01923\n","Train Step 1000/4006, summary_loss: 0.39987, class_loss: 0.24063, box_loss: 0.00318, time: 294.14745\n","Train Step 1250/4006, summary_loss: 0.40949, class_loss: 0.24830, box_loss: 0.00322, time: 367.44279\n","Train Step 1500/4006, summary_loss: 0.41307, class_loss: 0.25044, box_loss: 0.00325, time: 440.00015\n","Train Step 1750/4006, summary_loss: 0.41247, class_loss: 0.25078, box_loss: 0.00323, time: 514.00637\n","Train Step 2000/4006, summary_loss: 0.41273, class_loss: 0.25074, box_loss: 0.00324, time: 587.57218\n","Train Step 2250/4006, summary_loss: 0.41168, class_loss: 0.25046, box_loss: 0.00322, time: 660.95653\n","Train Step 2500/4006, summary_loss: 0.41183, class_loss: 0.25088, box_loss: 0.00322, time: 734.66530\n","Train Step 2750/4006, summary_loss: 0.41324, class_loss: 0.25230, box_loss: 0.00322, time: 808.00058\n","Train Step 3000/4006, summary_loss: 0.41255, class_loss: 0.25161, box_loss: 0.00322, time: 881.25918\n","Train Step 3250/4006, summary_loss: 0.41365, class_loss: 0.25226, box_loss: 0.00323, time: 954.51200\n","Train Step 3500/4006, summary_loss: 0.41159, class_loss: 0.25092, box_loss: 0.00321, time: 1027.59822\n","Train Step 3750/4006, summary_loss: 0.41132, class_loss: 0.25065, box_loss: 0.00321, time: 1101.32215\n","Train Step 4000/4006, summary_loss: 0.41047, class_loss: 0.25012, box_loss: 0.00321, time: 1174.44012\n","Train Epoch: 13, summary loss: 0.41049, class loss: 0.25011, box_loss: 0.00321, time: 1176.11932\n","Val 0000000067_0000000019.csv Epoch: 13, summary loss: 0.58195, class loss: 0.29775, box_loss: 0.00568, time: 132.20557\n","Val 0000000067_0000000046.csv Epoch: 13, summary loss: 0.55090, class loss: 0.33159, box_loss: 0.00439, time: 132.20868\n","Val 0000000067_0000000050.csv Epoch: 13, summary loss: 0.84686, class loss: 0.42527, box_loss: 0.00843, time: 132.21119\n","Val 0000000067_0000000052.csv Epoch: 13, summary loss: 1.21528, class loss: 0.53720, box_loss: 0.01356, time: 132.21349\n","Val 0000000354_0000000000.csv Epoch: 13, summary loss: 1.87527, class loss: 1.13401, box_loss: 0.01483, time: 132.21559\n","Val 0000000359_0000000000.csv Epoch: 13, summary loss: 0.95658, class loss: 0.52102, box_loss: 0.00871, time: 132.21768\n","Val 0000000363_0000000000.csv Epoch: 13, summary loss: 1.59943, class loss: 1.08473, box_loss: 0.01029, time: 132.21975\n","Val Epoch: 13, summary loss: 1.08947, class loss: 0.61880, box_loss: 0.00941, time: 132.22192\n","Adjusting learning rate of group 0 to 1.6106e-04.\n","\n","2021-06-01T16:33:10.546833\n","LR: 0.00016105621306769134\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.63765\n","Train Step 250/4006, summary_loss: 0.38841, class_loss: 0.23486, box_loss: 0.00307, time: 75.19699\n","Train Step 500/4006, summary_loss: 0.39046, class_loss: 0.23682, box_loss: 0.00307, time: 148.28803\n","Train Step 750/4006, summary_loss: 0.39129, class_loss: 0.23626, box_loss: 0.00310, time: 221.16341\n","Train Step 1000/4006, summary_loss: 0.39358, class_loss: 0.23833, box_loss: 0.00310, time: 294.32732\n","Train Step 1250/4006, summary_loss: 0.39442, class_loss: 0.23904, box_loss: 0.00311, time: 367.37721\n","Train Step 1500/4006, summary_loss: 0.39426, class_loss: 0.23921, box_loss: 0.00310, time: 440.42397\n","Train Step 1750/4006, summary_loss: 0.39322, class_loss: 0.23864, box_loss: 0.00309, time: 513.73700\n","Train Step 2000/4006, summary_loss: 0.39325, class_loss: 0.23887, box_loss: 0.00309, time: 586.88905\n","Train Step 2250/4006, summary_loss: 0.39318, class_loss: 0.23912, box_loss: 0.00308, time: 660.31147\n","Train Step 2500/4006, summary_loss: 0.39249, class_loss: 0.23905, box_loss: 0.00307, time: 733.65046\n","Train Step 2750/4006, summary_loss: 0.39471, class_loss: 0.24031, box_loss: 0.00309, time: 806.69526\n","Train Step 3000/4006, summary_loss: 0.39666, class_loss: 0.24120, box_loss: 0.00311, time: 879.54505\n","Train Step 3250/4006, summary_loss: 0.39663, class_loss: 0.24101, box_loss: 0.00311, time: 952.91501\n","Train Step 3500/4006, summary_loss: 0.39670, class_loss: 0.24091, box_loss: 0.00312, time: 1026.04667\n","Train Step 3750/4006, summary_loss: 0.39827, class_loss: 0.24141, box_loss: 0.00314, time: 1099.18189\n","Train Step 4000/4006, summary_loss: 0.39790, class_loss: 0.24120, box_loss: 0.00313, time: 1172.50496\n","Train Epoch: 14, summary loss: 0.39799, class loss: 0.24125, box_loss: 0.00313, time: 1174.19077\n","Val 0000000067_0000000019.csv Epoch: 14, summary loss: 0.66870, class loss: 0.38523, box_loss: 0.00567, time: 131.88945\n","Val 0000000067_0000000046.csv Epoch: 14, summary loss: 0.47517, class loss: 0.30000, box_loss: 0.00350, time: 131.89270\n","Val 0000000067_0000000050.csv Epoch: 14, summary loss: 0.82358, class loss: 0.45046, box_loss: 0.00746, time: 131.89522\n","Val 0000000067_0000000052.csv Epoch: 14, summary loss: 1.27477, class loss: 0.66140, box_loss: 0.01227, time: 131.89739\n","Val 0000000354_0000000000.csv Epoch: 14, summary loss: 3.50517, class loss: 2.81868, box_loss: 0.01373, time: 131.89968\n","Val 0000000359_0000000000.csv Epoch: 14, summary loss: 0.96442, class loss: 0.58911, box_loss: 0.00751, time: 131.90176\n","Val 0000000363_0000000000.csv Epoch: 14, summary loss: 1.44272, class loss: 0.92016, box_loss: 0.01045, time: 131.90379\n","Val Epoch: 14, summary loss: 1.30779, class loss: 0.87500, box_loss: 0.00866, time: 131.90583\n","Adjusting learning rate of group 0 to 1.5858e-04.\n","\n","2021-06-01T16:54:56.939639\n","LR: 0.00015858402873816668\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.66864\n","Train Step 250/4006, summary_loss: 0.39428, class_loss: 0.24138, box_loss: 0.00306, time: 74.62854\n","Train Step 500/4006, summary_loss: 0.39960, class_loss: 0.24327, box_loss: 0.00313, time: 148.31675\n","Train Step 750/4006, summary_loss: 0.40167, class_loss: 0.24276, box_loss: 0.00318, time: 221.54375\n","Train Step 1000/4006, summary_loss: 0.39787, class_loss: 0.24162, box_loss: 0.00312, time: 294.83230\n","Train Step 1250/4006, summary_loss: 0.40079, class_loss: 0.24339, box_loss: 0.00315, time: 368.15862\n","Train Step 1500/4006, summary_loss: 0.40060, class_loss: 0.24391, box_loss: 0.00313, time: 441.22285\n","Train Step 1750/4006, summary_loss: 0.40000, class_loss: 0.24357, box_loss: 0.00313, time: 514.19275\n","Train Step 2000/4006, summary_loss: 0.39923, class_loss: 0.24282, box_loss: 0.00313, time: 587.15936\n","Train Step 2250/4006, summary_loss: 0.39848, class_loss: 0.24226, box_loss: 0.00312, time: 660.86313\n","Train Step 2500/4006, summary_loss: 0.40208, class_loss: 0.24495, box_loss: 0.00314, time: 733.92777\n","Train Step 2750/4006, summary_loss: 0.40169, class_loss: 0.24455, box_loss: 0.00314, time: 806.94550\n","Train Step 3000/4006, summary_loss: 0.40113, class_loss: 0.24436, box_loss: 0.00314, time: 880.08336\n","Train Step 3250/4006, summary_loss: 0.40424, class_loss: 0.24650, box_loss: 0.00315, time: 953.22311\n","Train Step 3500/4006, summary_loss: 0.40384, class_loss: 0.24620, box_loss: 0.00315, time: 1026.66074\n","Train Step 3750/4006, summary_loss: 0.40347, class_loss: 0.24582, box_loss: 0.00315, time: 1100.02413\n","Train Step 4000/4006, summary_loss: 0.40213, class_loss: 0.24521, box_loss: 0.00314, time: 1173.68604\n","Train Epoch: 15, summary loss: 0.40213, class loss: 0.24520, box_loss: 0.00314, time: 1175.36015\n","Val 0000000067_0000000019.csv Epoch: 15, summary loss: 0.53708, class loss: 0.29985, box_loss: 0.00474, time: 132.06547\n","Val 0000000067_0000000046.csv Epoch: 15, summary loss: 0.48313, class loss: 0.29171, box_loss: 0.00383, time: 132.06839\n","Val 0000000067_0000000050.csv Epoch: 15, summary loss: 0.78629, class loss: 0.43275, box_loss: 0.00707, time: 132.07080\n","Val 0000000067_0000000052.csv Epoch: 15, summary loss: 1.20845, class loss: 0.67612, box_loss: 0.01065, time: 132.07334\n","Val 0000000354_0000000000.csv Epoch: 15, summary loss: 2.05315, class loss: 1.40634, box_loss: 0.01294, time: 132.07540\n","Val 0000000359_0000000000.csv Epoch: 15, summary loss: 0.81945, class loss: 0.44101, box_loss: 0.00757, time: 132.07766\n","Val 0000000363_0000000000.csv Epoch: 15, summary loss: 1.23770, class loss: 0.82888, box_loss: 0.00818, time: 132.07976\n","Val Epoch: 15, summary loss: 1.01789, class loss: 0.62524, box_loss: 0.00785, time: 132.08189\n","Adjusting learning rate of group 0 to 1.5615e-04.\n","\n","2021-06-01T17:16:44.983835\n","LR: 0.00015614979199999996\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.69912\n","Train Step 250/4006, summary_loss: 0.38701, class_loss: 0.23764, box_loss: 0.00299, time: 74.78301\n","Train Step 500/4006, summary_loss: 0.38707, class_loss: 0.23771, box_loss: 0.00299, time: 148.16101\n","Train Step 750/4006, summary_loss: 0.38893, class_loss: 0.23842, box_loss: 0.00301, time: 222.13503\n","Train Step 1000/4006, summary_loss: 0.38812, class_loss: 0.23834, box_loss: 0.00300, time: 295.44210\n","Train Step 1250/4006, summary_loss: 0.38812, class_loss: 0.23757, box_loss: 0.00301, time: 368.59486\n","Train Step 1500/4006, summary_loss: 0.38911, class_loss: 0.23807, box_loss: 0.00302, time: 441.68982\n","Train Step 1750/4006, summary_loss: 0.39055, class_loss: 0.23814, box_loss: 0.00305, time: 515.33574\n","Train Step 2000/4006, summary_loss: 0.39019, class_loss: 0.23820, box_loss: 0.00304, time: 588.52387\n","Train Step 2250/4006, summary_loss: 0.39078, class_loss: 0.23858, box_loss: 0.00304, time: 661.87488\n","Train Step 2500/4006, summary_loss: 0.39221, class_loss: 0.23986, box_loss: 0.00305, time: 735.09985\n","Train Step 2750/4006, summary_loss: 0.39206, class_loss: 0.23988, box_loss: 0.00304, time: 808.81866\n","Train Step 3000/4006, summary_loss: 0.39170, class_loss: 0.23933, box_loss: 0.00305, time: 881.92651\n","Train Step 3250/4006, summary_loss: 0.39318, class_loss: 0.24022, box_loss: 0.00306, time: 955.09196\n","Train Step 3500/4006, summary_loss: 0.39180, class_loss: 0.23930, box_loss: 0.00305, time: 1027.94469\n","Train Step 3750/4006, summary_loss: 0.39236, class_loss: 0.23945, box_loss: 0.00306, time: 1100.98953\n","Train Step 4000/4006, summary_loss: 0.39220, class_loss: 0.23924, box_loss: 0.00306, time: 1173.77700\n","Train Epoch: 16, summary loss: 0.39208, class loss: 0.23918, box_loss: 0.00306, time: 1175.45472\n","Val 0000000067_0000000019.csv Epoch: 16, summary loss: 0.54364, class loss: 0.27957, box_loss: 0.00528, time: 131.96426\n","Val 0000000067_0000000046.csv Epoch: 16, summary loss: 0.46024, class loss: 0.26148, box_loss: 0.00398, time: 131.96760\n","Val 0000000067_0000000050.csv Epoch: 16, summary loss: 0.78454, class loss: 0.42595, box_loss: 0.00717, time: 131.97014\n","Val 0000000067_0000000052.csv Epoch: 16, summary loss: 1.06463, class loss: 0.60923, box_loss: 0.00911, time: 131.97203\n","Val 0000000354_0000000000.csv Epoch: 16, summary loss: 1.69984, class loss: 1.04060, box_loss: 0.01318, time: 131.97389\n","Val 0000000359_0000000000.csv Epoch: 16, summary loss: 1.04957, class loss: 0.64379, box_loss: 0.00812, time: 131.97573\n","Val 0000000363_0000000000.csv Epoch: 16, summary loss: 1.29998, class loss: 0.94144, box_loss: 0.00717, time: 131.97755\n","Val Epoch: 16, summary loss: 0.98606, class loss: 0.60029, box_loss: 0.00772, time: 131.97944\n","Adjusting learning rate of group 0 to 1.5375e-04.\n","\n","2021-06-01T17:38:32.993435\n","LR: 0.00015375292036438857\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.89165\n","Train Step 250/4006, summary_loss: 0.42785, class_loss: 0.26537, box_loss: 0.00325, time: 74.33503\n","Train Step 500/4006, summary_loss: 0.40759, class_loss: 0.24820, box_loss: 0.00319, time: 147.71953\n","Train Step 750/4006, summary_loss: 0.39890, class_loss: 0.24346, box_loss: 0.00311, time: 220.97772\n","Train Step 1000/4006, summary_loss: 0.39205, class_loss: 0.23921, box_loss: 0.00306, time: 294.38529\n","Train Step 1250/4006, summary_loss: 0.39489, class_loss: 0.24112, box_loss: 0.00308, time: 367.53134\n","Train Step 1500/4006, summary_loss: 0.39495, class_loss: 0.24091, box_loss: 0.00308, time: 441.03631\n","Train Step 1750/4006, summary_loss: 0.39529, class_loss: 0.24058, box_loss: 0.00309, time: 514.02789\n","Train Step 2000/4006, summary_loss: 0.39724, class_loss: 0.24197, box_loss: 0.00311, time: 587.52610\n","Train Step 2250/4006, summary_loss: 0.39342, class_loss: 0.23983, box_loss: 0.00307, time: 661.04801\n","Train Step 2500/4006, summary_loss: 0.39488, class_loss: 0.24073, box_loss: 0.00308, time: 734.31737\n","Train Step 2750/4006, summary_loss: 0.39412, class_loss: 0.24001, box_loss: 0.00308, time: 807.69353\n","Train Step 3000/4006, summary_loss: 0.39259, class_loss: 0.23902, box_loss: 0.00307, time: 880.77446\n","Train Step 3250/4006, summary_loss: 0.39226, class_loss: 0.23884, box_loss: 0.00307, time: 954.09475\n","Train Step 3500/4006, summary_loss: 0.39175, class_loss: 0.23871, box_loss: 0.00306, time: 1027.49363\n","Train Step 3750/4006, summary_loss: 0.39138, class_loss: 0.23853, box_loss: 0.00306, time: 1100.30233\n","Train Step 4000/4006, summary_loss: 0.39049, class_loss: 0.23801, box_loss: 0.00305, time: 1173.87633\n","Train Epoch: 17, summary loss: 0.39056, class loss: 0.23804, box_loss: 0.00305, time: 1175.57717\n","Val 0000000067_0000000019.csv Epoch: 17, summary loss: 0.55831, class loss: 0.30382, box_loss: 0.00509, time: 132.87220\n","Val 0000000067_0000000046.csv Epoch: 17, summary loss: 0.49149, class loss: 0.29196, box_loss: 0.00399, time: 132.87687\n","Val 0000000067_0000000050.csv Epoch: 17, summary loss: 0.75973, class loss: 0.43368, box_loss: 0.00652, time: 132.87841\n","Val 0000000067_0000000052.csv Epoch: 17, summary loss: 0.93120, class loss: 0.54664, box_loss: 0.00769, time: 132.88046\n","Val 0000000354_0000000000.csv Epoch: 17, summary loss: 1.87910, class loss: 1.19524, box_loss: 0.01368, time: 132.88259\n","Val 0000000359_0000000000.csv Epoch: 17, summary loss: 0.91469, class loss: 0.51633, box_loss: 0.00797, time: 132.88461\n","Val 0000000363_0000000000.csv Epoch: 17, summary loss: 1.24740, class loss: 0.81080, box_loss: 0.00873, time: 132.88668\n","Val Epoch: 17, summary loss: 0.96885, class loss: 0.58549, box_loss: 0.00767, time: 132.88872\n","Adjusting learning rate of group 0 to 1.5139e-04.\n","\n","2021-06-01T18:00:22.030695\n","LR: 0.00015139284028362984\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.77521\n","Train Step 250/4006, summary_loss: 0.37536, class_loss: 0.23032, box_loss: 0.00290, time: 74.95068\n","Train Step 500/4006, summary_loss: 0.37320, class_loss: 0.22795, box_loss: 0.00290, time: 148.15190\n","Train Step 750/4006, summary_loss: 0.37896, class_loss: 0.23167, box_loss: 0.00295, time: 221.01751\n","Train Step 1000/4006, summary_loss: 0.38296, class_loss: 0.23536, box_loss: 0.00295, time: 293.96584\n","Train Step 1250/4006, summary_loss: 0.38395, class_loss: 0.23644, box_loss: 0.00295, time: 367.26028\n","Train Step 1500/4006, summary_loss: 0.38198, class_loss: 0.23479, box_loss: 0.00294, time: 440.54632\n","Train Step 1750/4006, summary_loss: 0.38305, class_loss: 0.23516, box_loss: 0.00296, time: 513.48548\n","Train Step 2000/4006, summary_loss: 0.38417, class_loss: 0.23589, box_loss: 0.00297, time: 586.58569\n","Train Step 2250/4006, summary_loss: 0.38495, class_loss: 0.23608, box_loss: 0.00298, time: 659.90393\n","Train Step 2500/4006, summary_loss: 0.38473, class_loss: 0.23572, box_loss: 0.00298, time: 733.43508\n","Train Step 2750/4006, summary_loss: 0.38585, class_loss: 0.23565, box_loss: 0.00300, time: 806.57958\n","Train Step 3000/4006, summary_loss: 0.38522, class_loss: 0.23520, box_loss: 0.00300, time: 879.44055\n","Train Step 3250/4006, summary_loss: 0.38567, class_loss: 0.23586, box_loss: 0.00300, time: 952.86975\n","Train Step 3500/4006, summary_loss: 0.38590, class_loss: 0.23597, box_loss: 0.00300, time: 1025.75385\n","Train Step 3750/4006, summary_loss: 0.38474, class_loss: 0.23515, box_loss: 0.00299, time: 1099.03851\n","Train Step 4000/4006, summary_loss: 0.38456, class_loss: 0.23507, box_loss: 0.00299, time: 1172.54547\n","Train Epoch: 18, summary loss: 0.38452, class loss: 0.23503, box_loss: 0.00299, time: 1174.20213\n","Val 0000000067_0000000019.csv Epoch: 18, summary loss: 0.55298, class loss: 0.27715, box_loss: 0.00552, time: 132.66952\n","Val 0000000067_0000000046.csv Epoch: 18, summary loss: 0.50527, class loss: 0.30868, box_loss: 0.00393, time: 132.67330\n","Val 0000000067_0000000050.csv Epoch: 18, summary loss: 0.75552, class loss: 0.43366, box_loss: 0.00644, time: 132.67619\n","Val 0000000067_0000000052.csv Epoch: 18, summary loss: 0.99759, class loss: 0.51660, box_loss: 0.00962, time: 132.67859\n","Val 0000000354_0000000000.csv Epoch: 18, summary loss: 1.53817, class loss: 0.87621, box_loss: 0.01324, time: 132.68095\n","Val 0000000359_0000000000.csv Epoch: 18, summary loss: 0.79165, class loss: 0.43506, box_loss: 0.00713, time: 132.68340\n","Val 0000000363_0000000000.csv Epoch: 18, summary loss: 1.41158, class loss: 0.99482, box_loss: 0.00834, time: 132.68581\n","Val Epoch: 18, summary loss: 0.93611, class loss: 0.54888, box_loss: 0.00774, time: 132.68823\n","Adjusting learning rate of group 0 to 1.4907e-04.\n","\n","2021-06-01T18:22:09.660553\n","LR: 0.00014906898701387667\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.81380\n","Train Step 250/4006, summary_loss: 0.36576, class_loss: 0.22406, box_loss: 0.00283, time: 75.29837\n","Train Step 500/4006, summary_loss: 0.36872, class_loss: 0.22637, box_loss: 0.00285, time: 148.80692\n","Train Step 750/4006, summary_loss: 0.37633, class_loss: 0.23029, box_loss: 0.00292, time: 221.99642\n","Train Step 1000/4006, summary_loss: 0.37705, class_loss: 0.22974, box_loss: 0.00295, time: 295.35421\n","Train Step 1250/4006, summary_loss: 0.37764, class_loss: 0.23067, box_loss: 0.00294, time: 368.61393\n","Train Step 1500/4006, summary_loss: 0.38006, class_loss: 0.23252, box_loss: 0.00295, time: 441.64053\n","Train Step 1750/4006, summary_loss: 0.37956, class_loss: 0.23179, box_loss: 0.00296, time: 514.66427\n","Train Step 2000/4006, summary_loss: 0.38033, class_loss: 0.23180, box_loss: 0.00297, time: 587.75860\n","Train Step 2250/4006, summary_loss: 0.38141, class_loss: 0.23254, box_loss: 0.00298, time: 661.08643\n","Train Step 2500/4006, summary_loss: 0.38140, class_loss: 0.23312, box_loss: 0.00297, time: 734.63678\n","Train Step 2750/4006, summary_loss: 0.38084, class_loss: 0.23261, box_loss: 0.00296, time: 807.90659\n","Train Step 3000/4006, summary_loss: 0.38110, class_loss: 0.23252, box_loss: 0.00297, time: 881.76361\n","Train Step 3250/4006, summary_loss: 0.38074, class_loss: 0.23217, box_loss: 0.00297, time: 955.20522\n","Train Step 3500/4006, summary_loss: 0.38067, class_loss: 0.23220, box_loss: 0.00297, time: 1028.58825\n","Train Step 3750/4006, summary_loss: 0.38127, class_loss: 0.23228, box_loss: 0.00298, time: 1101.73871\n","Train Step 4000/4006, summary_loss: 0.38160, class_loss: 0.23255, box_loss: 0.00298, time: 1174.93112\n","Train Epoch: 19, summary loss: 0.38174, class loss: 0.23265, box_loss: 0.00298, time: 1176.60360\n","Val 0000000067_0000000019.csv Epoch: 19, summary loss: 0.49073, class loss: 0.27083, box_loss: 0.00440, time: 132.74316\n","Val 0000000067_0000000046.csv Epoch: 19, summary loss: 0.47314, class loss: 0.26287, box_loss: 0.00421, time: 132.74650\n","Val 0000000067_0000000050.csv Epoch: 19, summary loss: 0.87317, class loss: 0.43227, box_loss: 0.00882, time: 132.74885\n","Val 0000000067_0000000052.csv Epoch: 19, summary loss: 1.10552, class loss: 0.56602, box_loss: 0.01079, time: 132.75090\n","Val 0000000354_0000000000.csv Epoch: 19, summary loss: 1.71352, class loss: 0.99119, box_loss: 0.01445, time: 132.76349\n","Val 0000000359_0000000000.csv Epoch: 19, summary loss: 1.15475, class loss: 0.69130, box_loss: 0.00927, time: 132.76566\n","Val 0000000363_0000000000.csv Epoch: 19, summary loss: 2.21474, class loss: 1.36632, box_loss: 0.01697, time: 132.76777\n","Val Epoch: 19, summary loss: 1.14651, class loss: 0.65440, box_loss: 0.00984, time: 132.76993\n","Adjusting learning rate of group 0 to 1.4678e-04.\n","\n","2021-06-01T18:43:59.331387\n","LR: 0.00014678080447999995\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.71646\n","Train Step 250/4006, summary_loss: 0.37723, class_loss: 0.23188, box_loss: 0.00291, time: 75.06053\n","Train Step 500/4006, summary_loss: 0.37675, class_loss: 0.23125, box_loss: 0.00291, time: 148.23546\n","Train Step 750/4006, summary_loss: 0.37684, class_loss: 0.23085, box_loss: 0.00292, time: 221.50561\n","Train Step 1000/4006, summary_loss: 0.37999, class_loss: 0.23216, box_loss: 0.00296, time: 294.38671\n","Train Step 1250/4006, summary_loss: 0.38284, class_loss: 0.23379, box_loss: 0.00298, time: 367.67788\n","Train Step 1500/4006, summary_loss: 0.38038, class_loss: 0.23267, box_loss: 0.00295, time: 441.21922\n","Train Step 1750/4006, summary_loss: 0.37948, class_loss: 0.23172, box_loss: 0.00296, time: 514.75292\n","Train Step 2000/4006, summary_loss: 0.37904, class_loss: 0.23142, box_loss: 0.00295, time: 588.13310\n","Train Step 2250/4006, summary_loss: 0.37745, class_loss: 0.23044, box_loss: 0.00294, time: 661.39517\n","Train Step 2500/4006, summary_loss: 0.37817, class_loss: 0.23061, box_loss: 0.00295, time: 735.36435\n","Train Step 2750/4006, summary_loss: 0.37903, class_loss: 0.23076, box_loss: 0.00297, time: 808.78748\n","Train Step 3000/4006, summary_loss: 0.37788, class_loss: 0.22997, box_loss: 0.00296, time: 882.36478\n","Train Step 3250/4006, summary_loss: 0.37752, class_loss: 0.22954, box_loss: 0.00296, time: 955.81241\n","Train Step 3500/4006, summary_loss: 0.37702, class_loss: 0.22952, box_loss: 0.00295, time: 1029.14261\n","Train Step 3750/4006, summary_loss: 0.37670, class_loss: 0.22966, box_loss: 0.00294, time: 1102.25953\n","Train Step 4000/4006, summary_loss: 0.37575, class_loss: 0.22923, box_loss: 0.00293, time: 1175.93830\n","Train Epoch: 20, summary loss: 0.37568, class loss: 0.22921, box_loss: 0.00293, time: 1177.59930\n","Val 0000000067_0000000019.csv Epoch: 20, summary loss: 0.47749, class loss: 0.25029, box_loss: 0.00454, time: 132.11089\n","Val 0000000067_0000000046.csv Epoch: 20, summary loss: 0.42552, class loss: 0.23702, box_loss: 0.00377, time: 132.11402\n","Val 0000000067_0000000050.csv Epoch: 20, summary loss: 0.94951, class loss: 0.47128, box_loss: 0.00956, time: 132.11650\n","Val 0000000067_0000000052.csv Epoch: 20, summary loss: 1.33719, class loss: 0.74447, box_loss: 0.01185, time: 132.11864\n","Val 0000000354_0000000000.csv Epoch: 20, summary loss: 1.67150, class loss: 0.97770, box_loss: 0.01388, time: 132.12075\n","Val 0000000359_0000000000.csv Epoch: 20, summary loss: 0.89854, class loss: 0.46693, box_loss: 0.00863, time: 132.12292\n","Val 0000000363_0000000000.csv Epoch: 20, summary loss: 1.42621, class loss: 0.95677, box_loss: 0.00939, time: 132.12501\n","Val Epoch: 20, summary loss: 1.02657, class loss: 0.58635, box_loss: 0.00880, time: 132.12722\n","Adjusting learning rate of group 0 to 1.4453e-04.\n","\n","2021-06-01T19:05:49.374919\n","LR: 0.00014452774514252525\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.78301\n","Train Step 250/4006, summary_loss: 0.37287, class_loss: 0.23014, box_loss: 0.00285, time: 74.82895\n","Train Step 500/4006, summary_loss: 0.37222, class_loss: 0.22946, box_loss: 0.00286, time: 148.71349\n","Train Step 750/4006, summary_loss: 0.37543, class_loss: 0.22996, box_loss: 0.00291, time: 222.10113\n","Train Step 1000/4006, summary_loss: 0.37264, class_loss: 0.22846, box_loss: 0.00288, time: 295.36790\n","Train Step 1250/4006, summary_loss: 0.37527, class_loss: 0.23020, box_loss: 0.00290, time: 368.35201\n","Train Step 1500/4006, summary_loss: 0.37720, class_loss: 0.23113, box_loss: 0.00292, time: 441.61962\n","Train Step 1750/4006, summary_loss: 0.37800, class_loss: 0.23135, box_loss: 0.00293, time: 514.61303\n","Train Step 2000/4006, summary_loss: 0.37829, class_loss: 0.23135, box_loss: 0.00294, time: 587.81925\n","Train Step 2250/4006, summary_loss: 0.37857, class_loss: 0.23143, box_loss: 0.00294, time: 661.26815\n","Train Step 2500/4006, summary_loss: 0.37678, class_loss: 0.23028, box_loss: 0.00293, time: 734.52966\n","Train Step 2750/4006, summary_loss: 0.37587, class_loss: 0.22976, box_loss: 0.00292, time: 808.03791\n","Train Step 3000/4006, summary_loss: 0.37405, class_loss: 0.22871, box_loss: 0.00291, time: 881.47403\n","Train Step 3250/4006, summary_loss: 0.37404, class_loss: 0.22924, box_loss: 0.00290, time: 954.80234\n","Train Step 3500/4006, summary_loss: 0.37403, class_loss: 0.22907, box_loss: 0.00290, time: 1028.77612\n","Train Step 3750/4006, summary_loss: 0.37344, class_loss: 0.22859, box_loss: 0.00290, time: 1102.33711\n","Train Step 4000/4006, summary_loss: 0.37467, class_loss: 0.22926, box_loss: 0.00291, time: 1175.63571\n","Train Epoch: 21, summary loss: 0.37468, class loss: 0.22923, box_loss: 0.00291, time: 1177.29703\n","Val 0000000067_0000000019.csv Epoch: 21, summary loss: 0.48080, class loss: 0.25487, box_loss: 0.00452, time: 132.72707\n","Val 0000000067_0000000046.csv Epoch: 21, summary loss: 0.44192, class loss: 0.24474, box_loss: 0.00394, time: 132.73014\n","Val 0000000067_0000000050.csv Epoch: 21, summary loss: 0.79323, class loss: 0.43942, box_loss: 0.00708, time: 132.73274\n","Val 0000000067_0000000052.csv Epoch: 21, summary loss: 1.14810, class loss: 0.65557, box_loss: 0.00985, time: 132.73493\n","Val 0000000354_0000000000.csv Epoch: 21, summary loss: 1.83657, class loss: 1.03667, box_loss: 0.01600, time: 132.73709\n","Val 0000000359_0000000000.csv Epoch: 21, summary loss: 0.86187, class loss: 0.48831, box_loss: 0.00747, time: 132.73924\n","Val 0000000363_0000000000.csv Epoch: 21, summary loss: 1.19696, class loss: 0.84955, box_loss: 0.00695, time: 132.74137\n","Val Epoch: 21, summary loss: 0.96564, class loss: 0.56702, box_loss: 0.00797, time: 132.74350\n","Adjusting learning rate of group 0 to 1.4231e-04.\n","\n","2021-06-01T19:27:39.702661\n","LR: 0.00014230926986661204\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.80511\n","Train Step 250/4006, summary_loss: 0.37228, class_loss: 0.22536, box_loss: 0.00294, time: 74.32061\n","Train Step 500/4006, summary_loss: 0.37782, class_loss: 0.22886, box_loss: 0.00298, time: 148.10286\n","Train Step 750/4006, summary_loss: 0.37913, class_loss: 0.22897, box_loss: 0.00300, time: 221.64730\n","Train Step 1000/4006, summary_loss: 0.37567, class_loss: 0.22687, box_loss: 0.00298, time: 295.27462\n","Train Step 1250/4006, summary_loss: 0.37329, class_loss: 0.22590, box_loss: 0.00295, time: 369.06135\n","Train Step 1500/4006, summary_loss: 0.37466, class_loss: 0.22855, box_loss: 0.00292, time: 442.49716\n","Train Step 1750/4006, summary_loss: 0.37466, class_loss: 0.22868, box_loss: 0.00292, time: 515.64890\n","Train Step 2000/4006, summary_loss: 0.37507, class_loss: 0.22845, box_loss: 0.00293, time: 589.32381\n","Train Step 2250/4006, summary_loss: 0.37476, class_loss: 0.22911, box_loss: 0.00291, time: 662.54251\n","Train Step 2500/4006, summary_loss: 0.37324, class_loss: 0.22815, box_loss: 0.00290, time: 736.16668\n","Train Step 2750/4006, summary_loss: 0.37422, class_loss: 0.22891, box_loss: 0.00291, time: 809.25676\n","Train Step 3000/4006, summary_loss: 0.37356, class_loss: 0.22868, box_loss: 0.00290, time: 882.54235\n","Train Step 3250/4006, summary_loss: 0.37211, class_loss: 0.22791, box_loss: 0.00288, time: 956.47445\n","Train Step 3500/4006, summary_loss: 0.37200, class_loss: 0.22779, box_loss: 0.00288, time: 1029.93374\n","Train Step 3750/4006, summary_loss: 0.37146, class_loss: 0.22755, box_loss: 0.00288, time: 1103.47770\n","Train Step 4000/4006, summary_loss: 0.37154, class_loss: 0.22783, box_loss: 0.00287, time: 1177.07947\n","Train Epoch: 22, summary loss: 0.37143, class loss: 0.22777, box_loss: 0.00287, time: 1178.74671\n","Val 0000000067_0000000019.csv Epoch: 22, summary loss: 0.55780, class loss: 0.26878, box_loss: 0.00578, time: 132.38585\n","Val 0000000067_0000000046.csv Epoch: 22, summary loss: 0.46655, class loss: 0.25773, box_loss: 0.00418, time: 132.38937\n","Val 0000000067_0000000050.csv Epoch: 22, summary loss: 0.78098, class loss: 0.42525, box_loss: 0.00711, time: 132.39161\n","Val 0000000067_0000000052.csv Epoch: 22, summary loss: 1.24391, class loss: 0.63526, box_loss: 0.01217, time: 132.39369\n","Val 0000000354_0000000000.csv Epoch: 22, summary loss: 1.76630, class loss: 0.97224, box_loss: 0.01588, time: 132.39575\n","Val 0000000359_0000000000.csv Epoch: 22, summary loss: 1.14976, class loss: 0.69285, box_loss: 0.00914, time: 132.39781\n","Val 0000000363_0000000000.csv Epoch: 22, summary loss: 1.46857, class loss: 0.90635, box_loss: 0.01124, time: 132.39985\n","Val Epoch: 22, summary loss: 1.06198, class loss: 0.59407, box_loss: 0.00936, time: 132.40196\n","Adjusting learning rate of group 0 to 1.4012e-04.\n","\n","2021-06-01T19:49:31.154274\n","LR: 0.00014012484779304406\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.86265\n","Train Step 250/4006, summary_loss: 0.37968, class_loss: 0.23550, box_loss: 0.00288, time: 74.73677\n","Train Step 500/4006, summary_loss: 0.36744, class_loss: 0.22758, box_loss: 0.00280, time: 148.29274\n","Train Step 750/4006, summary_loss: 0.36604, class_loss: 0.22646, box_loss: 0.00279, time: 221.55379\n","Train Step 1000/4006, summary_loss: 0.36678, class_loss: 0.22563, box_loss: 0.00282, time: 295.18369\n","Train Step 1250/4006, summary_loss: 0.36739, class_loss: 0.22573, box_loss: 0.00283, time: 368.90806\n","Train Step 1500/4006, summary_loss: 0.36907, class_loss: 0.22767, box_loss: 0.00283, time: 442.27108\n","Train Step 1750/4006, summary_loss: 0.36948, class_loss: 0.22764, box_loss: 0.00284, time: 515.88446\n","Train Step 2000/4006, summary_loss: 0.36855, class_loss: 0.22719, box_loss: 0.00283, time: 589.17732\n","Train Step 2250/4006, summary_loss: 0.37061, class_loss: 0.22854, box_loss: 0.00284, time: 663.11021\n","Train Step 2500/4006, summary_loss: 0.37085, class_loss: 0.22856, box_loss: 0.00285, time: 736.69865\n","Train Step 2750/4006, summary_loss: 0.37166, class_loss: 0.22890, box_loss: 0.00286, time: 809.97782\n","Train Step 3000/4006, summary_loss: 0.37089, class_loss: 0.22799, box_loss: 0.00286, time: 883.68865\n","Train Step 3250/4006, summary_loss: 0.37002, class_loss: 0.22761, box_loss: 0.00285, time: 957.21858\n","Train Step 3500/4006, summary_loss: 0.37079, class_loss: 0.22818, box_loss: 0.00285, time: 1030.88748\n","Train Step 3750/4006, summary_loss: 0.37028, class_loss: 0.22781, box_loss: 0.00285, time: 1104.28249\n","Train Step 4000/4006, summary_loss: 0.36992, class_loss: 0.22746, box_loss: 0.00285, time: 1177.33259\n","Train Epoch: 23, summary loss: 0.36992, class loss: 0.22744, box_loss: 0.00285, time: 1179.00939\n","Val 0000000067_0000000019.csv Epoch: 23, summary loss: 0.54240, class loss: 0.25980, box_loss: 0.00565, time: 132.64239\n","Val 0000000067_0000000046.csv Epoch: 23, summary loss: 0.48659, class loss: 0.28476, box_loss: 0.00404, time: 132.64552\n","Val 0000000067_0000000050.csv Epoch: 23, summary loss: 0.79626, class loss: 0.42862, box_loss: 0.00735, time: 132.64815\n","Val 0000000067_0000000052.csv Epoch: 23, summary loss: 1.34094, class loss: 0.70530, box_loss: 0.01271, time: 132.65011\n","Val 0000000354_0000000000.csv Epoch: 23, summary loss: 1.95008, class loss: 1.02431, box_loss: 0.01852, time: 132.65199\n","Val 0000000359_0000000000.csv Epoch: 23, summary loss: 1.02546, class loss: 0.50973, box_loss: 0.01031, time: 132.65389\n","Val 0000000363_0000000000.csv Epoch: 23, summary loss: 1.41858, class loss: 1.01610, box_loss: 0.00805, time: 132.65596\n","Val Epoch: 23, summary loss: 1.08004, class loss: 0.60409, box_loss: 0.00952, time: 132.65808\n","Adjusting learning rate of group 0 to 1.3797e-04.\n","\n","2021-06-01T20:11:23.108172\n","LR: 0.00013797395621119993\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.95524\n","Train Step 250/4006, summary_loss: 0.36452, class_loss: 0.22212, box_loss: 0.00285, time: 74.37082\n","Train Step 500/4006, summary_loss: 0.36258, class_loss: 0.22268, box_loss: 0.00280, time: 148.37118\n","Train Step 750/4006, summary_loss: 0.36647, class_loss: 0.22482, box_loss: 0.00283, time: 222.27573\n","Train Step 1000/4006, summary_loss: 0.36596, class_loss: 0.22476, box_loss: 0.00282, time: 295.56895\n","Train Step 1250/4006, summary_loss: 0.36526, class_loss: 0.22397, box_loss: 0.00283, time: 369.34095\n","Train Step 1500/4006, summary_loss: 0.36118, class_loss: 0.22142, box_loss: 0.00280, time: 442.60629\n","Train Step 1750/4006, summary_loss: 0.35824, class_loss: 0.21971, box_loss: 0.00277, time: 516.01297\n","Train Step 2000/4006, summary_loss: 0.35893, class_loss: 0.21981, box_loss: 0.00278, time: 589.45083\n","Train Step 2250/4006, summary_loss: 0.36220, class_loss: 0.22135, box_loss: 0.00282, time: 663.37780\n","Train Step 2500/4006, summary_loss: 0.36308, class_loss: 0.22237, box_loss: 0.00281, time: 737.34806\n","Train Step 2750/4006, summary_loss: 0.36322, class_loss: 0.22232, box_loss: 0.00282, time: 810.65232\n","Train Step 3000/4006, summary_loss: 0.36153, class_loss: 0.22125, box_loss: 0.00281, time: 883.97082\n","Train Step 3250/4006, summary_loss: 0.36219, class_loss: 0.22200, box_loss: 0.00280, time: 957.25741\n","Train Step 3500/4006, summary_loss: 0.36196, class_loss: 0.22188, box_loss: 0.00280, time: 1030.68285\n","Train Step 3750/4006, summary_loss: 0.36103, class_loss: 0.22143, box_loss: 0.00279, time: 1104.12115\n","Train Step 4000/4006, summary_loss: 0.36113, class_loss: 0.22147, box_loss: 0.00279, time: 1177.49020\n","Train Epoch: 24, summary loss: 0.36114, class loss: 0.22149, box_loss: 0.00279, time: 1179.15878\n","Val 0000000067_0000000019.csv Epoch: 24, summary loss: 0.57810, class loss: 0.32508, box_loss: 0.00506, time: 132.84084\n","Val 0000000067_0000000046.csv Epoch: 24, summary loss: 0.49477, class loss: 0.29760, box_loss: 0.00394, time: 132.84416\n","Val 0000000067_0000000050.csv Epoch: 24, summary loss: 0.79883, class loss: 0.42700, box_loss: 0.00744, time: 132.84672\n","Val 0000000067_0000000052.csv Epoch: 24, summary loss: 1.03535, class loss: 0.58389, box_loss: 0.00903, time: 132.84894\n","Val 0000000354_0000000000.csv Epoch: 24, summary loss: 2.00850, class loss: 1.34672, box_loss: 0.01324, time: 132.85114\n","Val 0000000359_0000000000.csv Epoch: 24, summary loss: 0.96436, class loss: 0.49440, box_loss: 0.00940, time: 132.85327\n","Val 0000000363_0000000000.csv Epoch: 24, summary loss: 1.23650, class loss: 0.85862, box_loss: 0.00756, time: 132.85545\n","Val Epoch: 24, summary loss: 1.01663, class loss: 0.61904, box_loss: 0.00795, time: 132.85759\n","Adjusting learning rate of group 0 to 1.3586e-04.\n","\n","2021-06-01T20:33:15.430285\n","LR: 0.0001358560804339737\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.77146\n","Train Step 250/4006, summary_loss: 0.37841, class_loss: 0.22998, box_loss: 0.00297, time: 74.78507\n","Train Step 500/4006, summary_loss: 0.37354, class_loss: 0.22780, box_loss: 0.00291, time: 147.96127\n","Train Step 750/4006, summary_loss: 0.37603, class_loss: 0.23261, box_loss: 0.00287, time: 221.56820\n","Train Step 1000/4006, summary_loss: 0.37807, class_loss: 0.23327, box_loss: 0.00290, time: 295.35656\n","Train Step 1250/4006, summary_loss: 0.37017, class_loss: 0.22890, box_loss: 0.00283, time: 368.71297\n","Train Step 1500/4006, summary_loss: 0.36962, class_loss: 0.22765, box_loss: 0.00284, time: 442.23394\n","Train Step 1750/4006, summary_loss: 0.36876, class_loss: 0.22752, box_loss: 0.00282, time: 515.46712\n","Train Step 2000/4006, summary_loss: 0.36716, class_loss: 0.22763, box_loss: 0.00279, time: 589.22339\n","Train Step 2250/4006, summary_loss: 0.36680, class_loss: 0.22737, box_loss: 0.00279, time: 662.51003\n","Train Step 2500/4006, summary_loss: 0.36844, class_loss: 0.22805, box_loss: 0.00281, time: 736.09530\n","Train Step 2750/4006, summary_loss: 0.36791, class_loss: 0.22799, box_loss: 0.00280, time: 809.34891\n","Train Step 3000/4006, summary_loss: 0.36732, class_loss: 0.22728, box_loss: 0.00280, time: 882.94330\n","Train Step 3250/4006, summary_loss: 0.36623, class_loss: 0.22649, box_loss: 0.00279, time: 956.25850\n","Train Step 3500/4006, summary_loss: 0.36602, class_loss: 0.22608, box_loss: 0.00280, time: 1029.86797\n","Train Step 3750/4006, summary_loss: 0.36614, class_loss: 0.22620, box_loss: 0.00280, time: 1103.12795\n","Train Step 4000/4006, summary_loss: 0.36502, class_loss: 0.22548, box_loss: 0.00279, time: 1176.55223\n","Train Epoch: 25, summary loss: 0.36515, class loss: 0.22560, box_loss: 0.00279, time: 1178.23555\n","Val 0000000067_0000000019.csv Epoch: 25, summary loss: 0.51717, class loss: 0.26649, box_loss: 0.00501, time: 132.81084\n","Val 0000000067_0000000046.csv Epoch: 25, summary loss: 0.49144, class loss: 0.28213, box_loss: 0.00419, time: 132.81409\n","Val 0000000067_0000000050.csv Epoch: 25, summary loss: 0.90270, class loss: 0.45013, box_loss: 0.00905, time: 132.81668\n","Val 0000000067_0000000052.csv Epoch: 25, summary loss: 1.11913, class loss: 0.59902, box_loss: 0.01040, time: 132.81882\n","Val 0000000354_0000000000.csv Epoch: 25, summary loss: 1.86158, class loss: 1.13512, box_loss: 0.01453, time: 132.83254\n","Val 0000000359_0000000000.csv Epoch: 25, summary loss: 1.27491, class loss: 0.58684, box_loss: 0.01376, time: 132.83460\n","Val 0000000363_0000000000.csv Epoch: 25, summary loss: 1.95881, class loss: 1.39360, box_loss: 0.01130, time: 132.83659\n","Val Epoch: 25, summary loss: 1.16082, class loss: 0.67333, box_loss: 0.00975, time: 132.83869\n","Adjusting learning rate of group 0 to 1.3377e-04.\n","\n","2021-06-01T20:55:06.794089\n","LR: 0.00013377071367461527\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.83077\n","Train Step 250/4006, summary_loss: 0.34291, class_loss: 0.21026, box_loss: 0.00265, time: 74.70405\n","Train Step 500/4006, summary_loss: 0.35222, class_loss: 0.21667, box_loss: 0.00271, time: 148.16755\n","Train Step 750/4006, summary_loss: 0.35031, class_loss: 0.21622, box_loss: 0.00268, time: 221.49387\n","Train Step 1000/4006, summary_loss: 0.35088, class_loss: 0.21675, box_loss: 0.00268, time: 295.32750\n","Train Step 1250/4006, summary_loss: 0.35580, class_loss: 0.22031, box_loss: 0.00271, time: 368.76659\n","Train Step 1500/4006, summary_loss: 0.35742, class_loss: 0.22125, box_loss: 0.00272, time: 442.46171\n","Train Step 1750/4006, summary_loss: 0.35690, class_loss: 0.22052, box_loss: 0.00273, time: 515.96492\n","Train Step 2000/4006, summary_loss: 0.35628, class_loss: 0.21975, box_loss: 0.00273, time: 588.97313\n","Train Step 2250/4006, summary_loss: 0.35475, class_loss: 0.21859, box_loss: 0.00272, time: 662.04063\n","Train Step 2500/4006, summary_loss: 0.35408, class_loss: 0.21793, box_loss: 0.00272, time: 735.17208\n","Train Step 2750/4006, summary_loss: 0.35514, class_loss: 0.21842, box_loss: 0.00273, time: 808.27601\n","Train Step 3000/4006, summary_loss: 0.35541, class_loss: 0.21876, box_loss: 0.00273, time: 881.68502\n","Train Step 3250/4006, summary_loss: 0.35437, class_loss: 0.21823, box_loss: 0.00272, time: 955.06189\n","Train Step 3500/4006, summary_loss: 0.35491, class_loss: 0.21861, box_loss: 0.00273, time: 1028.21532\n","Train Step 3750/4006, summary_loss: 0.35573, class_loss: 0.21905, box_loss: 0.00273, time: 1101.58102\n","Train Step 4000/4006, summary_loss: 0.35666, class_loss: 0.21960, box_loss: 0.00274, time: 1174.40078\n","Train Epoch: 26, summary loss: 0.35661, class loss: 0.21959, box_loss: 0.00274, time: 1176.07063\n","Val 0000000067_0000000019.csv Epoch: 26, summary loss: 0.49821, class loss: 0.25339, box_loss: 0.00490, time: 132.76835\n","Val 0000000067_0000000046.csv Epoch: 26, summary loss: 0.48547, class loss: 0.28785, box_loss: 0.00395, time: 132.77126\n","Val 0000000067_0000000050.csv Epoch: 26, summary loss: 0.86227, class loss: 0.43446, box_loss: 0.00856, time: 132.77393\n","Val 0000000067_0000000052.csv Epoch: 26, summary loss: 1.10020, class loss: 0.61890, box_loss: 0.00963, time: 132.77615\n","Val 0000000354_0000000000.csv Epoch: 26, summary loss: 1.74877, class loss: 0.95122, box_loss: 0.01595, time: 132.77823\n","Val 0000000359_0000000000.csv Epoch: 26, summary loss: 1.58599, class loss: 0.75883, box_loss: 0.01654, time: 132.78041\n","Val 0000000363_0000000000.csv Epoch: 26, summary loss: 2.44713, class loss: 1.55015, box_loss: 0.01794, time: 132.78274\n","Val Epoch: 26, summary loss: 1.24686, class loss: 0.69354, box_loss: 0.01107, time: 132.80295\n","Adjusting learning rate of group 0 to 1.3172e-04.\n","\n","2021-06-01T21:16:55.964217\n","LR: 0.00013171735692546138\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.79632\n","Train Step 250/4006, summary_loss: 0.35265, class_loss: 0.21346, box_loss: 0.00278, time: 74.93234\n","Train Step 500/4006, summary_loss: 0.35517, class_loss: 0.21697, box_loss: 0.00276, time: 148.69693\n","Train Step 750/4006, summary_loss: 0.35141, class_loss: 0.21470, box_loss: 0.00273, time: 222.11252\n","Train Step 1000/4006, summary_loss: 0.34957, class_loss: 0.21502, box_loss: 0.00269, time: 295.58909\n","Train Step 1250/4006, summary_loss: 0.35194, class_loss: 0.21687, box_loss: 0.00270, time: 369.36403\n","Train Step 1500/4006, summary_loss: 0.35134, class_loss: 0.21646, box_loss: 0.00270, time: 443.23166\n","Train Step 1750/4006, summary_loss: 0.35345, class_loss: 0.21727, box_loss: 0.00272, time: 516.34554\n","Train Step 2000/4006, summary_loss: 0.35163, class_loss: 0.21597, box_loss: 0.00271, time: 589.77979\n","Train Step 2250/4006, summary_loss: 0.35243, class_loss: 0.21629, box_loss: 0.00272, time: 663.54899\n","Train Step 2500/4006, summary_loss: 0.35362, class_loss: 0.21747, box_loss: 0.00272, time: 737.29367\n","Train Step 2750/4006, summary_loss: 0.35383, class_loss: 0.21766, box_loss: 0.00272, time: 810.92232\n","Train Step 3000/4006, summary_loss: 0.35299, class_loss: 0.21727, box_loss: 0.00271, time: 884.30361\n","Train Step 3250/4006, summary_loss: 0.35294, class_loss: 0.21727, box_loss: 0.00271, time: 958.36295\n","Train Step 3500/4006, summary_loss: 0.35288, class_loss: 0.21782, box_loss: 0.00270, time: 1031.92448\n","Train Step 3750/4006, summary_loss: 0.35347, class_loss: 0.21860, box_loss: 0.00270, time: 1105.66796\n","Train Step 4000/4006, summary_loss: 0.35403, class_loss: 0.21900, box_loss: 0.00270, time: 1179.34815\n","Train Epoch: 27, summary loss: 0.35419, class loss: 0.21910, box_loss: 0.00270, time: 1181.01480\n","Val 0000000067_0000000019.csv Epoch: 27, summary loss: 0.51583, class loss: 0.25579, box_loss: 0.00520, time: 132.93876\n","Val 0000000067_0000000046.csv Epoch: 27, summary loss: 0.54269, class loss: 0.32670, box_loss: 0.00432, time: 132.94215\n","Val 0000000067_0000000050.csv Epoch: 27, summary loss: 0.78778, class loss: 0.41953, box_loss: 0.00736, time: 132.94464\n","Val 0000000067_0000000052.csv Epoch: 27, summary loss: 1.03120, class loss: 0.59623, box_loss: 0.00870, time: 132.94669\n","Val 0000000354_0000000000.csv Epoch: 27, summary loss: 1.70106, class loss: 0.86514, box_loss: 0.01672, time: 132.94872\n","Val 0000000359_0000000000.csv Epoch: 27, summary loss: 1.22301, class loss: 0.77141, box_loss: 0.00903, time: 132.95092\n","Val 0000000363_0000000000.csv Epoch: 27, summary loss: 1.87034, class loss: 1.40371, box_loss: 0.00933, time: 132.95297\n","Val Epoch: 27, summary loss: 1.09598, class loss: 0.66264, box_loss: 0.00867, time: 132.95507\n","Adjusting learning rate of group 0 to 1.2970e-04.\n","\n","2021-06-01T21:38:50.225270\n","LR: 0.0001296955188385279\n","Train Step 0/4006, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.80440\n","Train Step 250/4006, summary_loss: 0.36233, class_loss: 0.22615, box_loss: 0.00272, time: 75.01141\n","Train Step 500/4006, summary_loss: 0.35438, class_loss: 0.21970, box_loss: 0.00269, time: 148.06989\n","Train Step 750/4006, summary_loss: 0.35201, class_loss: 0.21796, box_loss: 0.00268, time: 221.68348\n","Train Step 1000/4006, summary_loss: 0.34985, class_loss: 0.21600, box_loss: 0.00268, time: 295.94870\n","Train Step 1250/4006, summary_loss: 0.35193, class_loss: 0.21644, box_loss: 0.00271, time: 369.64038\n","Train Step 1500/4006, summary_loss: 0.34849, class_loss: 0.21457, box_loss: 0.00268, time: 443.46272\n","Train Step 1750/4006, summary_loss: 0.34996, class_loss: 0.21613, box_loss: 0.00268, time: 517.07255\n","Train Step 2000/4006, summary_loss: 0.34894, class_loss: 0.21568, box_loss: 0.00267, time: 590.74787\n","Train Step 2250/4006, summary_loss: 0.34806, class_loss: 0.21500, box_loss: 0.00266, time: 664.63982\n","Train Step 2500/4006, summary_loss: 0.34654, class_loss: 0.21419, box_loss: 0.00265, time: 738.00074\n","Train Step 2750/4006, summary_loss: 0.34581, class_loss: 0.21359, box_loss: 0.00264, time: 811.45677\n","Train Step 3000/4006, summary_loss: 0.34667, class_loss: 0.21415, box_loss: 0.00265, time: 884.88841\n","Train Step 3250/4006, summary_loss: 0.34593, class_loss: 0.21395, box_loss: 0.00264, time: 958.70148\n","Train Step 3500/4006, summary_loss: 0.34728, class_loss: 0.21486, box_loss: 0.00265, time: 1032.39149\n","Train Step 3750/4006, summary_loss: 0.34644, class_loss: 0.21432, box_loss: 0.00264, time: 1106.13121\n","Train Step 4000/4006, summary_loss: 0.35076, class_loss: 0.21781, box_loss: 0.00266, time: 1179.69465\n","Train Epoch: 28, summary loss: 0.35079, class loss: 0.21785, box_loss: 0.00266, time: 1181.36543\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_GpjOuQcejxU"},"source":[""],"execution_count":null,"outputs":[]}]}