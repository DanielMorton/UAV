{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Edet UAV Time Split.ipynb","provenance":[{"file_id":"1pOgo5jc8RTZf8ADCQ_xdM9aQbfBSvYrV","timestamp":1622079027133},{"file_id":"1Eaq8QeWzHEGHE9Zi7QdbPDKU-DNNQxRb","timestamp":1621540438010},{"file_id":"1UZS4QMn7dfZr5WHN1v9rmW4N6fHvUdKJ","timestamp":1620701338792},{"file_id":"1zI8F6NUxqRJqGiupvQJwYC-aNEb8BhFG","timestamp":1619725859470}],"collapsed_sections":[],"mount_file_id":"14JnGp7v22cXI4sV8OP5CE5955tcYnPfB","authorship_tag":"ABX9TyOXk0DT2JQqaduOS1VnOND1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"q3vhLq07ISsR"},"source":["Traing code for EfficientDet models using the last 25% of each video as validation data. Very similar to Edet UAV. Comments added only at points of divergence."]},{"cell_type":"code","metadata":{"id":"s5bQ_ZWS-Rsd"},"source":["import numpy as np\n","import pandas as pd\n","\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","import time\n","import random\n","import warnings\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from datetime import datetime\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.model_selection import train_test_split\n","\n","DRIVE = \"/content/drive/MyDrive/CNN/UAV\"\n","IMAGE_ZIP = \"images.zip\"\n","IMAGES = f\"{DRIVE}/{IMAGE_ZIP}\"\n","ANNOTATION_DIR = f\"{DRIVE}/annotations\"\n","BASE_IMAGE_DIR = f\"/content/images\"\n","ANNOTATION_FILES = [f\"{ANNOTATION_DIR}/{d}\" for d in os.listdir(ANNOTATION_DIR) if \".csv\" in d]\n","EDETS = [d for d in os.listdir(DRIVE) if '.pth' in d]\n","ANNOTATION_FILES.sort()\n","EDETS.sort()\n","os.environ[\"DRIVE\"] = DRIVE\n","os.environ[\"IMAGE_ZIP\"] = IMAGE_ZIP\n","os.environ[\"IMAGES\"] = IMAGES\n","os.environ[\"ANNOTATION_DIR\"] = ANNOTATION_DIR\n","os.environ[\"BASE_IMAGE_DIR\"] = BASE_IMAGE_DIR\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tenDmG77IyUm"},"source":["The size each video must be scaled to for valdiation."]},{"cell_type":"code","metadata":{"id":"xBBsakp_MKwM"},"source":["sizes = {\"0000000067_0000000019.csv\": [512, 640],\n","             \"0000000067_0000000046.csv\": [512, 640],\n","             \"0000000067_0000000050.csv\": [512, 640],\n","             \"0000000067_0000000052.csv\": [512, 640],\n","             \"0000000354_0000000000.csv\": [512, 640],\n","             \"0000000359_0000000000.csv\": [384, 640],\n","             \"0000000363_0000000000.csv\": [384, 640],\n","         \"0000000010_0000000000.csv\": [512, 640],\n","               \"0000000054_0000000000.csv\":\t[512, 640],\n","               \"0000000056_0000000000.csv\":\t[512, 640],\n","               \"0000000062_0000000000.csv\":\t[512, 640],\n","               \"0000000067_0000000005.csv\":\t[512, 640],\n","               \"0000000067_0000000012.csv\":\t[512, 640],\n","               \"0000000067_0000000014.csv\":\t[512, 640],\n","               \"0000000067_0000000015.csv\":\t[512, 640],\n","               \"0000000067_0000000024.csv\":\t[512, 640],\n","               \"0000000067_0000000025.csv\": [512, 640],\n","               \"0000000067_0000000026.csv\":\t[512, 640],\n","               \"0000000067_0000000027.csv\":\t[512, 640],\n","               \"0000000067_0000000028.csv\":\t[512, 640],\n","               \"0000000067_0000000029.csv\":\t[512, 640],\n","               \"0000000067_0000000031.csv\":\t[512,\t640],\n","               \"0000000067_0000000032.csv\":\t[512, 640],\n","               \"0000000067_0000000040.csv\":\t[512,\t640],\n","               \"0000000067_0000000041.csv\":\t[512,\t640],\n","               \"0000000067_0000000045.csv\":\t[512,\t640],\n","               \"0000000067_0000000055.csv\":\t[512,\t640],\n","               \"0000000067_0000000058.csv\":\t[512,\t640],\n","               \"0000000067_0000000059.csv\": [512, 640],\n","               \"0000000351_0000000000.csv\":\t[512,\t896],\n","               \"0000000364_0000000000.csv\":\t[384,\t768],\n","               \"0000000367_0000000000.csv\":\t[384, 768]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBgksHlA33K4"},"source":["%%bash\n","\n","cp $IMAGES /content\n","unzip -q $IMAGE_ZIP\n","rm $IMAGE_ZIP\n","\n","pip install -U -q albumentations\n","pip install -q omegaconf\n","pip install -q timm\n","pip install -q effdet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6i3XIPcSX4dT"},"source":["IMAGE_DIRS = [f\"{BASE_IMAGE_DIR}/{d}\" for d in os.listdir(BASE_IMAGE_DIR) if d != '.DS_Store']\n","IMAGE_DIRS.sort()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDZaZ-dWKPWp"},"source":["import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n","from effdet.efficientdet import HeadNet\n","\n","TRAIN_SIZE = 512\n","VAL_SIZE = 512\n","NUM_CLASSES = 1\n","BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","D_SIZE = 0\n","\n","EPOCH = 0\n","MAX_EPOCH = 100\n","\n","LOG_LR = 4\n","COEFF_LR = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ypr_Eh1T-9nh"},"source":["class TrainGlobalConfig:\n","  def __init__(self, d_size,\n","               num_workers,\n","               project_folder,\n","               batch_size, n_epochs,\n","               log_lr, coeff_lr):\n","\n","    self.num_workers = num_workers\n","    self.batch_size = batch_size\n","    self.n_epochs = n_epochs\n","    self.lr = coeff_lr * 10**(-log_lr)\n","\n","    self.folder = f\"{DRIVE}/{project_folder}\"\n","\n","    self.verbose = True\n","    self.verbose_step = 250\n","\n","    self.SchedulerClass = torch.optim.lr_scheduler.MultiplicativeLR\n","    self.scheduler_params = dict(\n","        lr_lambda = lambda epoch: 0.94**0.25,\n","        verbose=True\n","    )\n","global_config = TrainGlobalConfig(batch_size=BATCH_SIZE,\n","                                  n_epochs=MAX_EPOCH,\n","                                  d_size=D_SIZE,\n","                                  num_workers=NUM_WORKERS,\n","                                  project_folder=f\"effdet{D_SIZE}-{LOG_LR}-{COEFF_LR}_wd4-5_512x640_time_split_ap\",\n","                                  log_lr=LOG_LR,\n","                                  coeff_lr=COEFF_LR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TItOs4rQ4yDv"},"source":["# Load and Transform Data"]},{"cell_type":"markdown","metadata":{"id":"NvO0xBc2J53k"},"source":["Splits the videos into training an validation. In order to get the last 25% of bounding boxes in validation it is neccessary to take the last 30% of each video. (Bounding boxes are more likely at the beginning of a video.)"]},{"cell_type":"code","metadata":{"id":"KIvkSrhQCtM_"},"source":["def get_train_and_val(rand=6189):\n","  annotation_list = []\n","  seq_list = []\n","  col_names = [\"frame_id\", \"object_id\", \"x\", \"y\", \"width\", \"height\", \"object_class\",\n","               \"species\", \"occluded\", \"noisy_frame\"]\n","  for annot, image_dir in zip(ANNOTATION_FILES, IMAGE_DIRS):\n","    if \".csv\" in annot:\n","      annotation_list.append(pd.read_csv(annot, header=None,\n","                         names=col_names))\n","      annotation_list[-1]['csv'] = annot.split(\"/\")[-1]\n","      images = [f\"{image_dir}/{d}\" for d in os.listdir(image_dir)]\n","      img_shape = cv2.imread(images[0]).shape[:2]\n","      frames = [int(img.split(\"_\")[-1].split(\".\")[0]) for img in images]\n","      frame_df = pd.DataFrame({\"frame_id\": frames, \"file\": images})\n","      frame_df[\"csv\"] = f\"{image_dir.split('/')[-1]}.csv\"\n","      frame_df[\"img_height\"] = img_shape[0]\n","      frame_df[\"img_width\"] = img_shape[1]\n","      seq_list.append(frame_df)\n","  frame_df = pd.concat(seq_list).reset_index(drop=True)\n","  annotations = pd.concat(annotation_list).merge(frame_df).reset_index(drop=True)\n","  \n","  df = frame_df.copy()\n","  df = df.merge((df.groupby('csv')['frame_id'].max() * 0.75).to_frame().copy().rename(columns={'frame_id': 'boundary'}),\n","        left_on='csv', right_index=True)\n","  train_df, val_df = df[df['frame_id'] < df['boundary']], df[df['frame_id'] >= df['boundary']]\n","  train_annotations = annotations[annotations['file'].isin(train_df['file'])].reset_index(drop=True)\n","  val_annotations = annotations[annotations['file'].isin(val_df['file'])].reset_index(drop=True)\n","  return train_df.reset_index(drop=True), val_df.reset_index(drop=True), train_annotations, val_annotations\n","train_df, val_df, train_annotations, val_annotations = get_train_and_val()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfCFnGgMP0Y6"},"source":["**Transformations**"]},{"cell_type":"code","metadata":{"id":"SmMDYlFTY8xg"},"source":["BBOX = A.BboxParams(\n","             format='pascal_voc',\n","             min_area=0, \n","             min_visibility=0,\n","             label_fields=['labels'])\n","\n","def get_train_transforms(img_size):\n","  \"\"\"Returns a function to perform the standard sequence of preprocessing steps\n","     for training data.\n","  \"\"\"\n","  return A.Compose([A.RandomResizedCrop(height=img_size[0], width=img_size[1],\n","                                        scale=(0.1, 1.0),\n","                                        ratio=(3/4, 4/3),\n","                                        p=1.0),\n","                    A.HorizontalFlip(p=0.5),\n","                    A.VerticalFlip(p=0.5),\n","                    A.RandomRotate90(p=1.0),\n","                    A.Transpose(p=0.5),\n","                    ToTensorV2(p=1.0)],\n","                   bbox_params=BBOX, \n","                   p=1.0)\n","\n","def get_val_transform(img_size):\n","  \"\"\"Returns a function to perform the standard sequence of preprocessing steps\n","     for validation data.\n","  \"\"\"\n","  return A.Compose([A.LongestMaxSize(max_size=img_size[1],\n","                                     p=1.0),\n","                    A.PadIfNeeded(min_height=img_size[0],\n","                                  min_width=img_size[1],\n","                                  border_mode=0,\n","                                  p=1.0),\n","                    ToTensorV2(p=1.0)],\n","                   bbox_params=BBOX, \n","                   p=1.0)\n","  \n","def get_default_transform(img_size):\n","  \"\"\"Returns a function to perform the default transform if the training\n","     transform fails.\n","  \"\"\"\n","  return A.Compose([A.Resize(height=img_size[0],\n","                             width=img_size[1], p=1.0),\n","                    ToTensorV2(p=1.0)], \n","                   bbox_params=BBOX,\n","                   p=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0suMvS0mP_J0"},"source":["**Generic UAV Dataset**"]},{"cell_type":"code","metadata":{"id":"wMP1ag-u-nfe"},"source":["class UAVDataset(Dataset):\n","  \n","  def __init__(self, meta_data,\n","               boxes,\n","               transform=None,\n","               image_size=None,\n","               train=False):\n","    super(UAVDataset).__init__()\n","    \n","    self.meta_data = meta_data\n","    self.boxes = boxes\n","    if type(image_size) == int:\n","      self.image_size = (image_size, image_size)\n","    else:\n","      self.image_size = image_size\n","    self.transform = transform(self.image_size) if transform else None\n","    self.train = train\n","    \n","  def _box_to_tensor(self, sample, target):\n","    \"\"\"Convert boundind box array to tensor\"\"\"\n","    if len(sample[\"bboxes\"]) > 0:\n","      target[\"bboxes\"] = torch.tensor(sample[\"bboxes\"])\n","    else:\n","      target[\"bboxes\"] = torch.zeros((0,4))\n","      # Convert bounded box to yxyx format\n","    if self.train:\n","      target[\"bboxes\"][:,[0,1,2,3]] = target[\"bboxes\"][:,[1,0,3,2]]\n","    return target\n","  \n","  def __len__(self) -> int:\n","    \"\"\"Returns the number of images.\"\"\"\n","    return self.meta_data.shape[0]\n","\n","  def load_image_and_boxes(self, image_meta, image_boxes):\n","    \"\"\"Loads image corresponding to image_meta row.\n","       Converts bounding boxes to x_min, y_min, x_max, y_max format.\n","    \"\"\"\n","    image = cv2.imread(image_meta[\"file\"]).astype(np.float32)/ 255.0\n","    bboxes = image_boxes[[\"x\", \"y\", \"width\", \"height\"]].values\n","    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n","    bboxes[bboxes[:, 2] > image_meta[\"img_width\"], 2] = image_meta[\"img_width\"]\n","    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n","    bboxes[bboxes[:, 3] > image_meta[\"img_height\"], 3] = image_meta[\"img_height\"]\n","    return image, bboxes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQ13DKCdQRIs"},"source":["**Training Data**"]},{"cell_type":"code","metadata":{"id":"hwFj1r7R_mld"},"source":["class TrainDataset(UAVDataset):\n","\n","  def __init__(self, meta_data,\n","               boxes,\n","               image_size=None,\n","               transform=None,\n","               default_transform=None,\n","               max_iter=30):\n","    super(TrainDataset, self).__init__(meta_data, boxes, transform, image_size, train=True)\n","    self.default_transform = default_transform(self.image_size) if default_transform else None\n","    self.max_iter = max_iter\n","\n","  def __getitem__(self, index: int):\n","    \"\"\"Retrieves the image and boxes with the specified index.\"\"\"\n","    image_meta = self.meta_data.loc[index]\n","    image_boxes = self.boxes[self.boxes[\"file\"] == image_meta[\"file\"]]\n","    image, bboxes = self.load_image_and_boxes(image_meta, image_boxes)\n","    labels = torch.ones((bboxes.shape[0]), dtype=torch.int64)\n","    target = {\"bboxes\": torch.tensor(bboxes),\n","              \"labels\": labels}\n","    if self.transform and target[\"bboxes\"].shape[0] == 0:\n","      sample = self.transform(image=image,\n","                              bboxes=target[\"bboxes\"],\n","                              labels=target[\"labels\"])\n","      image, target = sample[\"image\"], self._box_to_tensor(sample, target)  \n","      return image, target\n","    elif self.transform:\n","      for i in range(self.max_iter):\n","        sample = self.transform(image=image,\n","                                bboxes=target[\"bboxes\"],\n","                                labels=target[\"labels\"])\n","        if len(sample[\"bboxes\"]) > 0:\n","          image, target= sample[\"image\"], self._box_to_tensor(sample, target)\n","          target[\"labels\"] = torch.stack(sample[\"labels\"])\n","          return image, target\n","    if self.default_transform and image.shape[2] == 3:\n","      sample = self.default_transform(image=image,\n","                                      bboxes=target[\"bboxes\"],\n","                                      labels=target[\"labels\"])\n","      image, target = sample[\"image\"], self._box_to_tensor(sample, target)\n","    return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lk_ltBmQVwu"},"source":["**Validation Data**"]},{"cell_type":"code","metadata":{"id":"IQlIgEavS_8d"},"source":["class ValDataset(UAVDataset):\n","\n","  def __init__(self, meta_data,\n","               boxes,\n","               image_size,\n","               transform=None,\n","               train=False):\n","    super(ValDataset, self).__init__(meta_data, boxes, transform, image_size, train)\n","\n","  def __getitem__(self, index: int):\n","    \"\"\"Retrieves the image and boxes with the specified index.\"\"\"\n","    image_meta = self.meta_data.loc[index]\n","    image_boxes = self.boxes[self.boxes[\"file\"] == image_meta[\"file\"]]\n","    image, bboxes = self.load_image_and_boxes(image_meta, image_boxes)\n","    labels = torch.ones(bboxes.shape[0], dtype=torch.int64)\n","    target = {\"bboxes\": bboxes,\n","              \"labels\": labels}\n","    \n","    if self.transform:\n","      sample = self.transform(image=image,\n","                              bboxes=target[\"bboxes\"],\n","                              labels=target[\"labels\"])\n","      image, target = sample['image'], self._box_to_tensor(sample, target)\n","    return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0wR2uH0nQxP"},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch)) \n","\n","train_dataset = TrainDataset(meta_data=train_df,\n","                             boxes=train_annotations,\n","                             image_size=TRAIN_SIZE,\n","                             transform=get_train_transforms,\n","                             default_transform=get_default_transform)\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=global_config.batch_size,\n","    num_workers=global_config.num_workers,\n","    sampler=RandomSampler(train_dataset),\n","    pin_memory=False,\n","    drop_last=True,\n","    collate_fn=collate_fn)\n","\n","\n","def get_val_dataset_dict(df,\n","                     annotations,\n","                     dataset,\n","                     get_transforms,\n","                     size_dict):\n","  dataset_dict = {}\n","  dataset_loaders = {}\n","  for csv in df[\"csv\"].unique():\n","    csv_df = df[df[\"csv\"] == csv].sort_values('frame_id').reset_index(drop=True)\n","    img_height, img_width = csv_df[\"img_height\"].iloc[0], csv_df[\"img_width\"].iloc[0]\n","    annot = annotations[annotations[\"csv\"] == csv]\n","    dataset_dict[csv] = dataset(meta_data=csv_df,\n","                                boxes=annot,\n","                                image_size=size_dict[csv],\n","                                transform=get_transforms,\n","                                train=True)\n","    dataset_loaders[csv] = torch.utils.data.DataLoader(dataset_dict[csv], \n","                                                       batch_size=global_config.batch_size,\n","                                                       num_workers=global_config.num_workers,\n","                                                       sampler=SequentialSampler(dataset_dict[csv]),\n","                                                       shuffle=False,\n","                                                       pin_memory=False,\n","                                                       collate_fn=collate_fn)\n","  return dataset_dict, dataset_loaders\n","val_datasets, val_loaders = get_val_dataset_dict(val_df,\n","                                                 val_annotations,\n","                                                 ValDataset,\n","                                                 get_val_transform,\n","                                                 size_dict=sizes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHSyxJ6HjeNs"},"source":["class AverageMeter:\n","  \"\"\"Computes and stores the average and current value\"\"\"\n","  def __init__(self):\n","    self.reset()\n","\n","  def reset(self):\n","    self.current = 0\n","    self.avg = 0\n","    self.sum = 0\n","    self.count = 0\n","\n","  def update(self, val, n=1, avg=False):\n","    self.current = val\n","    self.sum += val * n if avg else val\n","    self.count += n\n","    self.avg = self.sum / self.count\n","    \n","  def concat(self, other_meter):\n","    self.current += other_meter.current\n","    self.sum += other_meter.sum\n","    self.count += other_meter.count\n","    self.avg = self.sum / self.count\n","        \n","class LossMeter():\n","  def __init__(self, loss=None, class_loss=None, box_loss=None):\n","    self.loss = loss if loss is not None else AverageMeter()\n","    self.class_loss = class_loss if class_loss is not None else AverageMeter()\n","    self.box_loss = box_loss if box_loss is not None else AverageMeter()\n","\n","  def update(self, output, n=1, avg=False):\n","    self.loss.update(output['loss'].detach().item(), n, avg)\n","    self.class_loss.update(output['class_loss'].detach().item(), n, avg)\n","    self.box_loss.update(output['box_loss'].detach().item(), n, avg)\n","\n","  def concat(self, other_meter):\n","    self.loss.concat(other_meter.loss)\n","    self.class_loss.concat(other_meter.class_loss)\n","    self.box_loss.concat(other_meter.box_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHL8Xkn_OuR0"},"source":["def load_net(checkpoint_path, d_size, image_size):\n","    config = get_efficientdet_config(f'tf_efficientdet_d{d_size}_ap')\n","    config.num_classes = 1\n","    config.image_size=image_size\n","    net = EfficientDet(config, pretrained_backbone=False)\n","    net.reset_head(num_classes=copy_config.num_classes)\n","    net = ExtendDetBenchTrain(net, copy_config)\n","\n","    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    net.to(torch.device('cuda:0'))\n","    net.load_state_dict(checkpoint['model_state_dict'])\n","\n","    del checkpoint\n","    net = DetBenchPredict(net)\n","    net.eval()\n","    return net.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJ-ycoVFeWqf"},"source":["class Fitter:\n","    \n","    def __init__(self, model, val_model, device, config, d_size, start_epoch=0):\n","      self.config = config\n","      self.start_epoch = start_epoch\n","      \n","      self.base_dir = config.folder\n","      if not os.path.exists(self.base_dir):\n","        os.makedirs(self.base_dir)\n","        \n","      self.log_path = f\"{self.base_dir}/log.txt\"\n","      self.best_summary_loss = None\n","\n","      self.model = model\n","      self.val_model = val_model\n","      self.device = device\n","\n","      self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr,\n","                                         weight_decay=4e-5)\n","      self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n","      self.log(f\"Fitter prepared. Device is {self.device}\")\n","\n","    def _print_line(self, summary_loss, step, total_steps, stage, t):\n","      print(\n","          f\"{stage} Step {step}/{total_steps}, \" + \\\n","          f\"summary_loss: {summary_loss.loss.avg:.5f}, \" + \\\n","          f\"class_loss: {summary_loss.class_loss.avg:.5f}, \" + \\\n","          f\"box_loss: {summary_loss.box_loss.avg:.5f}, \" + \\\n","          f\"time: {(time.time() - t):.5f}\")\n","      \n","    def _log_line(self, summary_loss, epoch, stage, t):\n","      return f\"{stage} Epoch: {epoch}, \" + \\\n","             f\"summary loss: {summary_loss.loss.avg:.5f}, \" +\\\n","             f\"class loss: {summary_loss.class_loss.avg:.5f}, \" + \\\n","             f\"box_loss: {summary_loss.box_loss.avg:.5f}, \" + \\\n","             f\"time: {(time.time() - t):.5f}\"\n","\n","    def _avg_loss(self, loss_list):\n","      loss, class_loss, box_loss = AverageMeter(), AverageMeter(), AverageMeter()\n","      for l in loss_list:\n","        loss.update(l.loss.avg)\n","        class_loss.update(l.class_loss.avg)\n","        box_loss.update(l.box_loss.avg)\n","        \n","      return LossMeter(loss, class_loss, box_loss)\n","\n","    \n","\n","    def fit(self, train_loader, validation_loaders):\n","      if self.start_epoch > 0 and not self.best_summary_loss:\n","        self.best_summary_loss = self.validation(validation_loader)\n","\n","      for epoch in range(self.start_epoch, self.config.n_epochs):\n","        if self.config.verbose:\n","          lr = self.optimizer.param_groups[0][\"lr\"]\n","          timestamp = datetime.utcnow().isoformat()\n","          self.log(f\"\\n{timestamp}\\nLR: {lr}\")\n","\n","        t = time.time()\n","        summary_loss = self.train_one_epoch(train_loader)\n","\n","        self.log(self._log_line(summary_loss, epoch, \"Train\", t))\n","        self.save(f\"{self.base_dir}/last-checkpoint.bin\", epoch)\n","\n","        t = time.time()\n","        for k in self.val_model:\n","          self.val_model[k].model.load_state_dict(self.model.model.state_dict())\n","        ## Each validation video is processed separately\n","        summary_losses =  {k: self.validation(self.val_model[k], vl) for k, vl in validation_loaders.items()}\n","\n","        for k, sl in summary_losses.items():\n","          self.log(self._log_line(sl, epoch, f\"Val {k}\", t))\n","        total_summary_loss = self._avg_loss(summary_losses.values())\n","        self.log(self._log_line(total_summary_loss, epoch, \"Val\", t))\n","       \n","        if not self.best_summary_loss or total_summary_loss.loss.avg < self.best_summary_loss:\n","          self.best_summary_loss = total_summary_loss.loss.avg\n","          self.model.eval()\n","          self.save(f\"{self.base_dir}/best-checkpoint-{str(epoch).zfill(3)}epoch.bin\", epoch)\n","          for path in sorted(glob(f\"{self.base_dir}/best-checkpoint-*epoch.bin\"))[:-3]:\n","            os.remove(path)\n","\n","        self.scheduler.step()\n","\n","    def train_one_epoch(self, train_loader):\n","      self.model.train()\n","      summary_loss = LossMeter()\n","      t = time.time()\n","      for step, (images, targets) in enumerate(train_loader):\n","        if self.config.verbose and step % self.config.verbose_step == 0:\n","          self._print_line(summary_loss, step, len(train_loader), \"Train\", t)\n","        images = torch.stack(images)\n","        images = images.to(self.device).float()\n","        batch_size = images.shape[0]\n","        bboxes = [target[\"bboxes\"].to(self.device).float() for target in targets]\n","        labels = [target[\"labels\"].to(self.device).float() for target in targets]\n","\n","        self.optimizer.zero_grad()\n","        \n","        output = self.model(images, {\"bbox\": bboxes, \"cls\": labels})\n","            \n","        output[\"loss\"].backward()\n","\n","        summary_loss.update(output, batch_size, avg=True)\n","\n","        self.optimizer.step()\n","        \n","      return summary_loss\n","\n","    def validation(self, val_model, val_loader):\n","      val_model.eval()\n","      summary_loss = LossMeter()\n","      t = time.time()\n","      for step, (images, targets) in enumerate(val_loader):\n","        with torch.no_grad():\n","          images = torch.stack(images)\n","          batch_size = images.shape[0]\n","          images = images.to(self.device).float()\n","          bboxes = [target[\"bboxes\"].to(self.device).float() for target in targets]\n","          labels = [target[\"labels\"].to(self.device).float() for target in targets]\n","\n","          output = val_model(images, {\"bbox\": bboxes, \"cls\": labels})\n","          summary_loss.update(output, batch_size, avg=True)\n","      return summary_loss\n","\n","\n","    \n","    def save(self, path, epoch):\n","      self.model.eval()\n","      torch.save({\n","          \"model_state_dict\": self.model.model.state_dict(),\n","          \"optimizer_state_dict\": self.optimizer.state_dict(),\n","          \"scheduler_state_dict\": self.scheduler.state_dict(),\n","          \"best_summary_loss\": self.best_summary_loss,\n","          \"epoch\": epoch,\n","          }, path)\n","\n","    def load(self, path):\n","      checkpoint = torch.load(path)\n","      self.model.model.load_state_dict(checkpoint[\"model_state_dict\"])\n","      self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","      self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","      self.best_summary_loss = checkpoint[\"best_summary_loss\"]\n","      self.start_epoch = checkpoint[\"epoch\"] + 1\n","        \n","    def log(self, message):\n","      if self.config.verbose:\n","        print(message)\n","      with open(self.log_path, \"a+\") as logger:\n","        logger.write(f\"{message}\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUeyp53_1PA6"},"source":["class ExtendDetBenchTrain(DetBenchTrain):\n","\n","  def __init__(self, model, config):\n","    super(ExtendDetBenchTrain, self).__init__(model, config)\n","\n","  def forward(self, x, target):\n","    class_out, box_out = self.model(x)\n","    cls_targets, box_targets, num_positives = self.anchor_labeler.batch_label_anchors(\n","        target['bbox'], target['cls'])\n","    loss, class_loss, box_loss = self.loss_fn(class_out, box_out, cls_targets, box_targets, num_positives)\n","    output = dict(loss=loss, class_loss=class_loss, box_loss=box_loss)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gw09ZLe9DYvo","executionInfo":{"status":"ok","timestamp":1622515457795,"user_tz":240,"elapsed":15673,"user":{"displayName":"Daniel Morton","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxdX_X4qG11j5MBUgqAy-1QxhxaucRZ1oLJFwguw=s64","userId":"10030415829092908707"}},"outputId":"0eca24cc-367c-412b-ff58-21f00b75d04e"},"source":["def get_net(epoch=EPOCH,\n","            global_config=global_config,\n","            num_classes=NUM_CLASSES,\n","            image_size=TRAIN_SIZE,\n","            val_sizes=sizes,\n","            d_size=D_SIZE):\n","  device = torch.device('cuda:0')\n","  enet_config = get_efficientdet_config(f'tf_efficientdet_d{d_size}_ap')\n","  enet_config.image_size = [image_size, image_size]\n","  copy_config = enet_config.copy()\n","  copy_config.num_classes = num_classes\n","  val_net = {}\n","  for csv in sizes:\n","    val_config = get_efficientdet_config(f'tf_efficientdet_d{d_size}_ap')\n","    val_config.image_size = val_sizes[csv]\n","    val_net[csv] = EfficientDet(val_config, pretrained_backbone=False)\n","    val_net[csv].reset_head(num_classes=copy_config.num_classes)\n","    val_net[csv] = ExtendDetBenchTrain(val_net[csv], copy_config)\n","    val_net[csv].to(device)\n","  net = EfficientDet(enet_config, pretrained_backbone=False)\n","  if epoch == 0:\n","    checkpoint = torch.load(f\"{DRIVE}/{EDETS[d_size]}\")\n","    net.load_state_dict(checkpoint)\n","    net.reset_head(num_classes=copy_config.num_classes)\n","    net = ExtendDetBenchTrain(net, copy_config)\n","    net.to(device)\n","    fitter = Fitter(model=net,\n","                    val_model=val_net,\n","                    device=device, config=global_config, start_epoch=0, d_size=d_size)\n","  else:\n","    net.reset_head(num_classes=copy_config.num_classes)\n","    net = DetBenchTrain(net, copy_config)\n","    net.to(device)\n","    fitter = Fitter(model=net,\n","                    val_model=val_net,\n","                    device=device, config=global_config, start_epoch=0)\n","    fitter.load(f\"{global_config.folder}/best-checkpoint-{str(epoch).zfill(3)}epoch.bin\")\n","    \n","\n","  return fitter\n","\n","fitter = get_net()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Adjusting learning rate of group 0 to 2.0000e-04.\n","Fitter prepared. Device is cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chx863oGjuyP","outputId":"2848d594-b269-44da-e442-4d8ea4d6610c"},"source":["fitter.fit(train_loader, val_loaders)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","2021-06-01T02:44:17.699503\n","LR: 0.0002\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 1.01961\n","Train Step 250/3810, summary_loss: 1.11676, class_loss: 0.78671, box_loss: 0.00660, time: 74.07296\n","Train Step 500/3810, summary_loss: 1.00734, class_loss: 0.69261, box_loss: 0.00629, time: 147.07250\n","Train Step 750/3810, summary_loss: 0.93901, class_loss: 0.63560, box_loss: 0.00607, time: 220.22538\n","Train Step 1000/3810, summary_loss: 0.90589, class_loss: 0.60695, box_loss: 0.00598, time: 293.19583\n","Train Step 1250/3810, summary_loss: 0.87142, class_loss: 0.57889, box_loss: 0.00585, time: 365.90084\n","Train Step 1500/3810, summary_loss: 0.83781, class_loss: 0.55185, box_loss: 0.00572, time: 438.90154\n","Train Step 1750/3810, summary_loss: 0.81765, class_loss: 0.53575, box_loss: 0.00564, time: 511.44036\n","Train Step 2000/3810, summary_loss: 0.80628, class_loss: 0.52760, box_loss: 0.00557, time: 584.93818\n","Train Step 2250/3810, summary_loss: 0.79921, class_loss: 0.52267, box_loss: 0.00553, time: 657.72521\n","Train Step 2500/3810, summary_loss: 0.78540, class_loss: 0.51163, box_loss: 0.00548, time: 730.70011\n","Train Step 2750/3810, summary_loss: 0.77353, class_loss: 0.50240, box_loss: 0.00542, time: 803.80225\n","Train Step 3000/3810, summary_loss: 0.76027, class_loss: 0.49210, box_loss: 0.00536, time: 877.36135\n","Train Step 3250/3810, summary_loss: 0.75327, class_loss: 0.48663, box_loss: 0.00533, time: 950.08318\n","Train Step 3500/3810, summary_loss: 0.74219, class_loss: 0.47777, box_loss: 0.00529, time: 1023.25607\n","Train Step 3750/3810, summary_loss: 0.73352, class_loss: 0.47035, box_loss: 0.00526, time: 1095.62171\n","Train Epoch: 0, summary loss: 0.73094, class loss: 0.46832, box_loss: 0.00525, time: 1113.06005\n","Val 0000000010_0000000000.csv Epoch: 0, summary loss: 0.98299, class loss: 0.78703, box_loss: 0.00392, time: 165.83061\n","Val 0000000054_0000000000.csv Epoch: 0, summary loss: 1.20521, class loss: 0.98378, box_loss: 0.00443, time: 165.83432\n","Val 0000000056_0000000000.csv Epoch: 0, summary loss: 0.74829, class loss: 0.48393, box_loss: 0.00529, time: 165.83691\n","Val 0000000062_0000000000.csv Epoch: 0, summary loss: 1.46460, class loss: 1.07866, box_loss: 0.00772, time: 165.83905\n","Val 0000000067_0000000005.csv Epoch: 0, summary loss: 0.16707, class loss: 0.16707, box_loss: 0.00000, time: 165.88763\n","Val 0000000067_0000000012.csv Epoch: 0, summary loss: 0.18424, class loss: 0.18424, box_loss: 0.00000, time: 165.89241\n","Val 0000000067_0000000014.csv Epoch: 0, summary loss: 0.40494, class loss: 0.29104, box_loss: 0.00228, time: 165.89498\n","Val 0000000067_0000000015.csv Epoch: 0, summary loss: 0.53011, class loss: 0.31001, box_loss: 0.00440, time: 165.89707\n","Val 0000000067_0000000019.csv Epoch: 0, summary loss: 0.52544, class loss: 0.29782, box_loss: 0.00455, time: 165.89914\n","Val 0000000067_0000000024.csv Epoch: 0, summary loss: 0.52245, class loss: 0.34868, box_loss: 0.00348, time: 165.90123\n","Val 0000000067_0000000025.csv Epoch: 0, summary loss: 0.45369, class loss: 0.28935, box_loss: 0.00329, time: 165.90329\n","Val 0000000067_0000000026.csv Epoch: 0, summary loss: 0.11623, class loss: 0.11623, box_loss: 0.00000, time: 165.90533\n","Val 0000000067_0000000027.csv Epoch: 0, summary loss: 0.14471, class loss: 0.14471, box_loss: 0.00000, time: 165.90734\n","Val 0000000067_0000000028.csv Epoch: 0, summary loss: 0.17693, class loss: 0.16147, box_loss: 0.00031, time: 165.90941\n","Val 0000000067_0000000029.csv Epoch: 0, summary loss: 0.15408, class loss: 0.15408, box_loss: 0.00000, time: 165.91150\n","Val 0000000067_0000000031.csv Epoch: 0, summary loss: 0.36744, class loss: 0.24119, box_loss: 0.00252, time: 165.91356\n","Val 0000000067_0000000032.csv Epoch: 0, summary loss: 0.45167, class loss: 0.32381, box_loss: 0.00256, time: 165.91558\n","Val 0000000067_0000000040.csv Epoch: 0, summary loss: 0.29779, class loss: 0.23426, box_loss: 0.00127, time: 165.91760\n","Val 0000000067_0000000041.csv Epoch: 0, summary loss: 0.69794, class loss: 0.38242, box_loss: 0.00631, time: 165.91961\n","Val 0000000067_0000000045.csv Epoch: 0, summary loss: 0.35379, class loss: 0.28457, box_loss: 0.00138, time: 165.92161\n","Val 0000000067_0000000046.csv Epoch: 0, summary loss: 0.29743, class loss: 0.22239, box_loss: 0.00150, time: 165.92362\n","Val 0000000067_0000000050.csv Epoch: 0, summary loss: 1.09118, class loss: 0.69220, box_loss: 0.00798, time: 165.92590\n","Val 0000000067_0000000052.csv Epoch: 0, summary loss: 0.80665, class loss: 0.58485, box_loss: 0.00444, time: 165.92803\n","Val 0000000067_0000000055.csv Epoch: 0, summary loss: 0.37458, class loss: 0.31810, box_loss: 0.00113, time: 165.93038\n","Val 0000000067_0000000058.csv Epoch: 0, summary loss: 0.70332, class loss: 0.46713, box_loss: 0.00472, time: 165.93241\n","Val 0000000067_0000000059.csv Epoch: 0, summary loss: 0.35863, class loss: 0.35863, box_loss: 0.00000, time: 165.93444\n","Val 0000000351_0000000000.csv Epoch: 0, summary loss: 0.73136, class loss: 0.44103, box_loss: 0.00581, time: 165.93645\n","Val 0000000354_0000000000.csv Epoch: 0, summary loss: 3.20546, class loss: 2.59307, box_loss: 0.01225, time: 165.93846\n","Val 0000000359_0000000000.csv Epoch: 0, summary loss: 0.73617, class loss: 0.44282, box_loss: 0.00587, time: 165.94047\n","Val 0000000363_0000000000.csv Epoch: 0, summary loss: 1.00905, class loss: 1.00905, box_loss: 0.00000, time: 165.94248\n","Val 0000000364_0000000000.csv Epoch: 0, summary loss: 4.99639, class loss: 4.97571, box_loss: 0.00041, time: 165.94449\n","Val 0000000367_0000000000.csv Epoch: 0, summary loss: 1.12860, class loss: 0.93685, box_loss: 0.00384, time: 165.94649\n","Val Epoch: 0, summary loss: 0.79339, class loss: 0.63457, box_loss: 0.00318, time: 165.94868\n","Adjusting learning rate of group 0 to 1.9693e-04.\n","\n","2021-06-01T03:05:37.309176\n","LR: 0.00019693003544236372\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.82501\n","Train Step 250/3810, summary_loss: 0.62105, class_loss: 0.38129, box_loss: 0.00480, time: 74.12817\n","Train Step 500/3810, summary_loss: 0.68669, class_loss: 0.44051, box_loss: 0.00492, time: 146.92399\n","Train Step 750/3810, summary_loss: 0.68175, class_loss: 0.43558, box_loss: 0.00492, time: 220.24401\n","Train Step 1000/3810, summary_loss: 0.66905, class_loss: 0.42404, box_loss: 0.00490, time: 293.54612\n","Train Step 1250/3810, summary_loss: 0.65674, class_loss: 0.41488, box_loss: 0.00484, time: 366.32259\n","Train Step 1500/3810, summary_loss: 0.64431, class_loss: 0.40374, box_loss: 0.00481, time: 439.39830\n","Train Step 1750/3810, summary_loss: 0.63456, class_loss: 0.39689, box_loss: 0.00475, time: 512.53147\n","Train Step 2000/3810, summary_loss: 0.63012, class_loss: 0.39239, box_loss: 0.00475, time: 586.27589\n","Train Step 2250/3810, summary_loss: 0.62126, class_loss: 0.38525, box_loss: 0.00472, time: 658.97181\n","Train Step 2500/3810, summary_loss: 0.61642, class_loss: 0.38128, box_loss: 0.00470, time: 731.68596\n","Train Step 2750/3810, summary_loss: 0.61162, class_loss: 0.37780, box_loss: 0.00468, time: 804.80002\n","Train Step 3000/3810, summary_loss: 0.60741, class_loss: 0.37460, box_loss: 0.00466, time: 877.57319\n","Train Step 3250/3810, summary_loss: 0.60353, class_loss: 0.37147, box_loss: 0.00464, time: 950.78151\n","Train Step 3500/3810, summary_loss: 0.60136, class_loss: 0.37002, box_loss: 0.00463, time: 1023.33822\n","Train Step 3750/3810, summary_loss: 0.59992, class_loss: 0.36877, box_loss: 0.00462, time: 1096.66880\n","Train Epoch: 1, summary loss: 0.59977, class loss: 0.36867, box_loss: 0.00462, time: 1114.09699\n","Val 0000000010_0000000000.csv Epoch: 1, summary loss: 1.68878, class loss: 1.08166, box_loss: 0.01214, time: 166.89227\n","Val 0000000054_0000000000.csv Epoch: 1, summary loss: 4.56078, class loss: 4.17542, box_loss: 0.00771, time: 166.89570\n","Val 0000000056_0000000000.csv Epoch: 1, summary loss: 0.69258, class loss: 0.47061, box_loss: 0.00444, time: 166.89812\n","Val 0000000062_0000000000.csv Epoch: 1, summary loss: 1.46813, class loss: 1.09770, box_loss: 0.00741, time: 166.90023\n","Val 0000000067_0000000005.csv Epoch: 1, summary loss: 0.23974, class loss: 0.23974, box_loss: 0.00000, time: 166.90240\n","Val 0000000067_0000000012.csv Epoch: 1, summary loss: 0.16748, class loss: 0.16748, box_loss: 0.00000, time: 166.90453\n","Val 0000000067_0000000014.csv Epoch: 1, summary loss: 0.57129, class loss: 0.49131, box_loss: 0.00160, time: 166.90666\n","Val 0000000067_0000000015.csv Epoch: 1, summary loss: 0.67999, class loss: 0.34561, box_loss: 0.00669, time: 166.90882\n","Val 0000000067_0000000019.csv Epoch: 1, summary loss: 0.48205, class loss: 0.26647, box_loss: 0.00431, time: 166.91095\n","Val 0000000067_0000000024.csv Epoch: 1, summary loss: 0.45522, class loss: 0.28553, box_loss: 0.00339, time: 166.91309\n","Val 0000000067_0000000025.csv Epoch: 1, summary loss: 0.49632, class loss: 0.29576, box_loss: 0.00401, time: 166.91526\n","Val 0000000067_0000000026.csv Epoch: 1, summary loss: 0.23254, class loss: 0.23254, box_loss: 0.00000, time: 166.91738\n","Val 0000000067_0000000027.csv Epoch: 1, summary loss: 0.41317, class loss: 0.41317, box_loss: 0.00000, time: 166.91944\n","Val 0000000067_0000000028.csv Epoch: 1, summary loss: 0.24943, class loss: 0.23048, box_loss: 0.00038, time: 166.92154\n","Val 0000000067_0000000029.csv Epoch: 1, summary loss: 0.36547, class loss: 0.36547, box_loss: 0.00000, time: 166.92368\n","Val 0000000067_0000000031.csv Epoch: 1, summary loss: 0.37991, class loss: 0.24012, box_loss: 0.00280, time: 166.92578\n","Val 0000000067_0000000032.csv Epoch: 1, summary loss: 0.47000, class loss: 0.34327, box_loss: 0.00253, time: 166.92789\n","Val 0000000067_0000000040.csv Epoch: 1, summary loss: 0.54303, class loss: 0.46444, box_loss: 0.00157, time: 166.93004\n","Val 0000000067_0000000041.csv Epoch: 1, summary loss: 0.75826, class loss: 0.40002, box_loss: 0.00716, time: 166.93220\n","Val 0000000067_0000000045.csv Epoch: 1, summary loss: 0.38751, class loss: 0.27717, box_loss: 0.00221, time: 166.93432\n","Val 0000000067_0000000046.csv Epoch: 1, summary loss: 0.47098, class loss: 0.36667, box_loss: 0.00209, time: 166.95309\n","Val 0000000067_0000000050.csv Epoch: 1, summary loss: 1.24534, class loss: 0.82116, box_loss: 0.00848, time: 166.95530\n","Val 0000000067_0000000052.csv Epoch: 1, summary loss: 1.08678, class loss: 0.86102, box_loss: 0.00452, time: 166.95740\n","Val 0000000067_0000000055.csv Epoch: 1, summary loss: 0.59795, class loss: 0.56063, box_loss: 0.00075, time: 166.95945\n","Val 0000000067_0000000058.csv Epoch: 1, summary loss: 1.29829, class loss: 1.04179, box_loss: 0.00513, time: 166.96156\n","Val 0000000067_0000000059.csv Epoch: 1, summary loss: 0.45419, class loss: 0.45419, box_loss: 0.00000, time: 166.96366\n","Val 0000000351_0000000000.csv Epoch: 1, summary loss: 0.66625, class loss: 0.39864, box_loss: 0.00535, time: 166.96577\n","Val 0000000354_0000000000.csv Epoch: 1, summary loss: 1.70598, class loss: 1.45854, box_loss: 0.00495, time: 166.96810\n","Val 0000000359_0000000000.csv Epoch: 1, summary loss: 0.81779, class loss: 0.49129, box_loss: 0.00653, time: 166.97022\n","Val 0000000363_0000000000.csv Epoch: 1, summary loss: 0.52614, class loss: 0.52614, box_loss: 0.00000, time: 166.97230\n","Val 0000000364_0000000000.csv Epoch: 1, summary loss: 1.18086, class loss: 1.16361, box_loss: 0.00035, time: 166.97436\n","Val 0000000367_0000000000.csv Epoch: 1, summary loss: 0.77132, class loss: 0.58891, box_loss: 0.00365, time: 166.97642\n","Val Epoch: 1, summary loss: 0.81636, class loss: 0.64427, box_loss: 0.00344, time: 166.97865\n","Adjusting learning rate of group 0 to 1.9391e-04.\n","\n","2021-06-01T03:26:58.674233\n","LR: 0.00019390719429665315\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.65631\n","Train Step 250/3810, summary_loss: 0.59022, class_loss: 0.36885, box_loss: 0.00443, time: 74.43478\n","Train Step 500/3810, summary_loss: 0.58254, class_loss: 0.36066, box_loss: 0.00444, time: 147.56023\n","Train Step 750/3810, summary_loss: 0.56876, class_loss: 0.34885, box_loss: 0.00440, time: 220.90834\n","Train Step 1000/3810, summary_loss: 0.56016, class_loss: 0.34245, box_loss: 0.00435, time: 294.45237\n","Train Step 1250/3810, summary_loss: 0.55312, class_loss: 0.33623, box_loss: 0.00434, time: 367.71022\n","Train Step 1500/3810, summary_loss: 0.54862, class_loss: 0.33145, box_loss: 0.00434, time: 441.19763\n","Train Step 1750/3810, summary_loss: 0.54752, class_loss: 0.33061, box_loss: 0.00434, time: 514.28274\n","Train Step 2000/3810, summary_loss: 0.54695, class_loss: 0.32960, box_loss: 0.00435, time: 588.09329\n","Train Step 2250/3810, summary_loss: 0.54796, class_loss: 0.33065, box_loss: 0.00435, time: 661.54134\n","Train Step 2500/3810, summary_loss: 0.54919, class_loss: 0.33125, box_loss: 0.00436, time: 734.60795\n","Train Step 2750/3810, summary_loss: 0.54617, class_loss: 0.32901, box_loss: 0.00434, time: 808.25610\n","Train Step 3000/3810, summary_loss: 0.54319, class_loss: 0.32702, box_loss: 0.00432, time: 881.69408\n","Train Step 3250/3810, summary_loss: 0.54382, class_loss: 0.32728, box_loss: 0.00433, time: 954.89303\n","Train Step 3500/3810, summary_loss: 0.54155, class_loss: 0.32612, box_loss: 0.00431, time: 1027.94384\n","Train Step 3750/3810, summary_loss: 0.53945, class_loss: 0.32472, box_loss: 0.00429, time: 1101.49202\n","Train Epoch: 2, summary loss: 0.53893, class loss: 0.32422, box_loss: 0.00429, time: 1118.86685\n","Val 0000000010_0000000000.csv Epoch: 2, summary loss: 0.83337, class loss: 0.66092, box_loss: 0.00345, time: 167.08501\n","Val 0000000054_0000000000.csv Epoch: 2, summary loss: 0.67631, class loss: 0.53739, box_loss: 0.00278, time: 167.08788\n","Val 0000000056_0000000000.csv Epoch: 2, summary loss: 0.69649, class loss: 0.51998, box_loss: 0.00353, time: 167.09031\n","Val 0000000062_0000000000.csv Epoch: 2, summary loss: 1.37011, class loss: 0.93921, box_loss: 0.00862, time: 167.09268\n","Val 0000000067_0000000005.csv Epoch: 2, summary loss: 0.34961, class loss: 0.34961, box_loss: 0.00000, time: 167.09497\n","Val 0000000067_0000000012.csv Epoch: 2, summary loss: 0.07027, class loss: 0.07027, box_loss: 0.00000, time: 167.09708\n","Val 0000000067_0000000014.csv Epoch: 2, summary loss: 0.46111, class loss: 0.40402, box_loss: 0.00114, time: 167.09913\n","Val 0000000067_0000000015.csv Epoch: 2, summary loss: 0.42750, class loss: 0.26349, box_loss: 0.00328, time: 167.11490\n","Val 0000000067_0000000019.csv Epoch: 2, summary loss: 0.42488, class loss: 0.20608, box_loss: 0.00438, time: 167.11880\n","Val 0000000067_0000000024.csv Epoch: 2, summary loss: 0.36090, class loss: 0.20314, box_loss: 0.00316, time: 167.12103\n","Val 0000000067_0000000025.csv Epoch: 2, summary loss: 0.41025, class loss: 0.22447, box_loss: 0.00372, time: 167.12320\n","Val 0000000067_0000000026.csv Epoch: 2, summary loss: 0.10874, class loss: 0.10874, box_loss: 0.00000, time: 167.12525\n","Val 0000000067_0000000027.csv Epoch: 2, summary loss: 0.12574, class loss: 0.12574, box_loss: 0.00000, time: 167.12730\n","Val 0000000067_0000000028.csv Epoch: 2, summary loss: 0.12226, class loss: 0.10936, box_loss: 0.00026, time: 167.12939\n","Val 0000000067_0000000029.csv Epoch: 2, summary loss: 0.08855, class loss: 0.08855, box_loss: 0.00000, time: 167.13146\n","Val 0000000067_0000000031.csv Epoch: 2, summary loss: 0.30436, class loss: 0.19723, box_loss: 0.00214, time: 167.13354\n","Val 0000000067_0000000032.csv Epoch: 2, summary loss: 0.40142, class loss: 0.28954, box_loss: 0.00224, time: 167.13562\n","Val 0000000067_0000000040.csv Epoch: 2, summary loss: 0.34158, class loss: 0.28252, box_loss: 0.00118, time: 167.13770\n","Val 0000000067_0000000041.csv Epoch: 2, summary loss: 0.76805, class loss: 0.41500, box_loss: 0.00706, time: 167.13975\n","Val 0000000067_0000000045.csv Epoch: 2, summary loss: 0.32719, class loss: 0.25086, box_loss: 0.00153, time: 167.14181\n","Val 0000000067_0000000046.csv Epoch: 2, summary loss: 0.34278, class loss: 0.27687, box_loss: 0.00132, time: 167.14385\n","Val 0000000067_0000000050.csv Epoch: 2, summary loss: 1.04830, class loss: 0.64828, box_loss: 0.00800, time: 167.14595\n","Val 0000000067_0000000052.csv Epoch: 2, summary loss: 0.85908, class loss: 0.59953, box_loss: 0.00519, time: 167.14801\n","Val 0000000067_0000000055.csv Epoch: 2, summary loss: 0.58514, class loss: 0.55355, box_loss: 0.00063, time: 167.15009\n","Val 0000000067_0000000058.csv Epoch: 2, summary loss: 3.64134, class loss: 3.44900, box_loss: 0.00385, time: 167.15212\n","Val 0000000067_0000000059.csv Epoch: 2, summary loss: 0.59602, class loss: 0.59602, box_loss: 0.00000, time: 167.15420\n","Val 0000000351_0000000000.csv Epoch: 2, summary loss: 0.60894, class loss: 0.39410, box_loss: 0.00430, time: 167.15652\n","Val 0000000354_0000000000.csv Epoch: 2, summary loss: 1.92632, class loss: 1.66450, box_loss: 0.00524, time: 167.15867\n","Val 0000000359_0000000000.csv Epoch: 2, summary loss: 0.83537, class loss: 0.45455, box_loss: 0.00762, time: 167.16076\n","Val 0000000363_0000000000.csv Epoch: 2, summary loss: 0.63979, class loss: 0.63979, box_loss: 0.00000, time: 167.16288\n","Val 0000000364_0000000000.csv Epoch: 2, summary loss: 0.89928, class loss: 0.87770, box_loss: 0.00043, time: 167.16496\n","Val 0000000367_0000000000.csv Epoch: 2, summary loss: 0.76618, class loss: 0.55760, box_loss: 0.00417, time: 167.16707\n","Val Epoch: 2, summary loss: 0.66929, class loss: 0.52993, box_loss: 0.00279, time: 167.16931\n","Adjusting learning rate of group 0 to 1.9093e-04.\n","\n","2021-06-01T03:48:25.273990\n","LR: 0.00019093075322684607\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.69723\n","Train Step 250/3810, summary_loss: 0.52756, class_loss: 0.31800, box_loss: 0.00419, time: 74.56757\n","Train Step 500/3810, summary_loss: 0.53940, class_loss: 0.32571, box_loss: 0.00427, time: 147.64395\n","Train Step 750/3810, summary_loss: 0.53422, class_loss: 0.32140, box_loss: 0.00426, time: 220.97424\n","Train Step 1000/3810, summary_loss: 0.53485, class_loss: 0.32318, box_loss: 0.00423, time: 294.57419\n","Train Step 1250/3810, summary_loss: 0.52919, class_loss: 0.31956, box_loss: 0.00419, time: 367.49126\n","Train Step 1500/3810, summary_loss: 0.52615, class_loss: 0.31777, box_loss: 0.00417, time: 441.17637\n","Train Step 1750/3810, summary_loss: 0.52464, class_loss: 0.31796, box_loss: 0.00413, time: 514.23073\n","Train Step 2000/3810, summary_loss: 0.52368, class_loss: 0.31683, box_loss: 0.00414, time: 587.65056\n","Train Step 2250/3810, summary_loss: 0.52274, class_loss: 0.31670, box_loss: 0.00412, time: 660.45234\n","Train Step 2500/3810, summary_loss: 0.52222, class_loss: 0.31575, box_loss: 0.00413, time: 733.73146\n","Train Step 2750/3810, summary_loss: 0.52230, class_loss: 0.31573, box_loss: 0.00413, time: 806.83737\n","Train Step 3000/3810, summary_loss: 0.52125, class_loss: 0.31434, box_loss: 0.00414, time: 880.50217\n","Train Step 3250/3810, summary_loss: 0.52073, class_loss: 0.31438, box_loss: 0.00413, time: 954.01184\n","Train Step 3500/3810, summary_loss: 0.52172, class_loss: 0.31530, box_loss: 0.00413, time: 1027.04136\n","Train Step 3750/3810, summary_loss: 0.52033, class_loss: 0.31405, box_loss: 0.00413, time: 1100.61053\n","Train Epoch: 3, summary loss: 0.52001, class loss: 0.31373, box_loss: 0.00413, time: 1118.04439\n","Val 0000000010_0000000000.csv Epoch: 3, summary loss: 0.69732, class loss: 0.52364, box_loss: 0.00347, time: 167.04768\n","Val 0000000054_0000000000.csv Epoch: 3, summary loss: 0.63234, class loss: 0.45450, box_loss: 0.00356, time: 167.05068\n","Val 0000000056_0000000000.csv Epoch: 3, summary loss: 0.58244, class loss: 0.42690, box_loss: 0.00311, time: 167.05306\n","Val 0000000062_0000000000.csv Epoch: 3, summary loss: 0.86276, class loss: 0.52505, box_loss: 0.00675, time: 167.05532\n","Val 0000000067_0000000005.csv Epoch: 3, summary loss: 0.23190, class loss: 0.23190, box_loss: 0.00000, time: 167.05740\n","Val 0000000067_0000000012.csv Epoch: 3, summary loss: 0.04740, class loss: 0.04740, box_loss: 0.00000, time: 167.05943\n","Val 0000000067_0000000014.csv Epoch: 3, summary loss: 0.38018, class loss: 0.32031, box_loss: 0.00120, time: 167.06143\n","Val 0000000067_0000000015.csv Epoch: 3, summary loss: 0.43555, class loss: 0.24923, box_loss: 0.00373, time: 167.06339\n","Val 0000000067_0000000019.csv Epoch: 3, summary loss: 0.39499, class loss: 0.20026, box_loss: 0.00389, time: 167.06541\n","Val 0000000067_0000000024.csv Epoch: 3, summary loss: 0.37173, class loss: 0.18849, box_loss: 0.00366, time: 167.06744\n","Val 0000000067_0000000025.csv Epoch: 3, summary loss: 0.37488, class loss: 0.19911, box_loss: 0.00352, time: 167.06947\n","Val 0000000067_0000000026.csv Epoch: 3, summary loss: 0.04298, class loss: 0.04298, box_loss: 0.00000, time: 167.07155\n","Val 0000000067_0000000027.csv Epoch: 3, summary loss: 0.05369, class loss: 0.05369, box_loss: 0.00000, time: 167.07362\n","Val 0000000067_0000000028.csv Epoch: 3, summary loss: 0.08429, class loss: 0.06817, box_loss: 0.00032, time: 167.07588\n","Val 0000000067_0000000029.csv Epoch: 3, summary loss: 0.05756, class loss: 0.05756, box_loss: 0.00000, time: 167.07798\n","Val 0000000067_0000000031.csv Epoch: 3, summary loss: 0.30253, class loss: 0.17907, box_loss: 0.00247, time: 167.08002\n","Val 0000000067_0000000032.csv Epoch: 3, summary loss: 0.36661, class loss: 0.25865, box_loss: 0.00216, time: 167.08208\n","Val 0000000067_0000000040.csv Epoch: 3, summary loss: 0.28760, class loss: 0.20830, box_loss: 0.00159, time: 167.08413\n","Val 0000000067_0000000041.csv Epoch: 3, summary loss: 0.71768, class loss: 0.39142, box_loss: 0.00653, time: 167.08622\n","Val 0000000067_0000000045.csv Epoch: 3, summary loss: 0.25648, class loss: 0.18603, box_loss: 0.00141, time: 167.08827\n","Val 0000000067_0000000046.csv Epoch: 3, summary loss: 0.18651, class loss: 0.12658, box_loss: 0.00120, time: 167.09034\n","Val 0000000067_0000000050.csv Epoch: 3, summary loss: 1.01741, class loss: 0.62815, box_loss: 0.00779, time: 167.09242\n","Val 0000000067_0000000052.csv Epoch: 3, summary loss: 0.56482, class loss: 0.36168, box_loss: 0.00406, time: 167.09457\n","Val 0000000067_0000000055.csv Epoch: 3, summary loss: 0.45672, class loss: 0.41132, box_loss: 0.00091, time: 167.09672\n","Val 0000000067_0000000058.csv Epoch: 3, summary loss: 1.71212, class loss: 1.49598, box_loss: 0.00432, time: 167.12328\n","Val 0000000067_0000000059.csv Epoch: 3, summary loss: 0.34866, class loss: 0.34866, box_loss: 0.00000, time: 167.12553\n","Val 0000000351_0000000000.csv Epoch: 3, summary loss: 0.66171, class loss: 0.45217, box_loss: 0.00419, time: 167.12764\n","Val 0000000354_0000000000.csv Epoch: 3, summary loss: 1.50895, class loss: 1.27223, box_loss: 0.00473, time: 167.12986\n","Val 0000000359_0000000000.csv Epoch: 3, summary loss: 0.78548, class loss: 0.43647, box_loss: 0.00698, time: 167.13197\n","Val 0000000363_0000000000.csv Epoch: 3, summary loss: 0.92462, class loss: 0.92462, box_loss: 0.00000, time: 167.13418\n","Val 0000000364_0000000000.csv Epoch: 3, summary loss: 1.25046, class loss: 1.23233, box_loss: 0.00036, time: 167.13633\n","Val 0000000367_0000000000.csv Epoch: 3, summary loss: 1.58705, class loss: 1.38273, box_loss: 0.00409, time: 167.13843\n","Val Epoch: 3, summary loss: 0.56829, class loss: 0.43392, box_loss: 0.00269, time: 167.14057\n","Adjusting learning rate of group 0 to 1.8800e-04.\n","\n","2021-06-01T04:09:51.030513\n","LR: 0.000188\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.71745\n","Train Step 250/3810, summary_loss: 0.50387, class_loss: 0.29890, box_loss: 0.00410, time: 74.44224\n","Train Step 500/3810, summary_loss: 0.50017, class_loss: 0.29886, box_loss: 0.00403, time: 147.56788\n","Train Step 750/3810, summary_loss: 0.50458, class_loss: 0.30190, box_loss: 0.00405, time: 220.49892\n","Train Step 1000/3810, summary_loss: 0.50266, class_loss: 0.30049, box_loss: 0.00404, time: 293.71245\n","Train Step 1250/3810, summary_loss: 0.50014, class_loss: 0.29867, box_loss: 0.00403, time: 367.48232\n","Train Step 1500/3810, summary_loss: 0.49818, class_loss: 0.29805, box_loss: 0.00400, time: 440.58830\n","Train Step 1750/3810, summary_loss: 0.49843, class_loss: 0.29812, box_loss: 0.00401, time: 513.17386\n","Train Step 2000/3810, summary_loss: 0.49724, class_loss: 0.29747, box_loss: 0.00400, time: 586.45274\n","Train Step 2250/3810, summary_loss: 0.50066, class_loss: 0.30032, box_loss: 0.00401, time: 659.54534\n","Train Step 2500/3810, summary_loss: 0.50024, class_loss: 0.29966, box_loss: 0.00401, time: 732.98755\n","Train Step 2750/3810, summary_loss: 0.50068, class_loss: 0.29960, box_loss: 0.00402, time: 806.33810\n","Train Step 3000/3810, summary_loss: 0.50014, class_loss: 0.29906, box_loss: 0.00402, time: 879.35395\n","Train Step 3250/3810, summary_loss: 0.49805, class_loss: 0.29754, box_loss: 0.00401, time: 953.03299\n","Train Step 3500/3810, summary_loss: 0.49649, class_loss: 0.29633, box_loss: 0.00400, time: 1026.87601\n","Train Step 3750/3810, summary_loss: 0.49748, class_loss: 0.29767, box_loss: 0.00400, time: 1101.64407\n","Train Epoch: 4, summary loss: 0.49693, class loss: 0.29742, box_loss: 0.00399, time: 1119.19385\n","Val 0000000010_0000000000.csv Epoch: 4, summary loss: 0.83748, class loss: 0.60889, box_loss: 0.00457, time: 167.93071\n","Val 0000000054_0000000000.csv Epoch: 4, summary loss: 0.64805, class loss: 0.49165, box_loss: 0.00313, time: 167.93401\n","Val 0000000056_0000000000.csv Epoch: 4, summary loss: 0.61322, class loss: 0.44148, box_loss: 0.00343, time: 167.93644\n","Val 0000000062_0000000000.csv Epoch: 4, summary loss: 0.87913, class loss: 0.54371, box_loss: 0.00671, time: 167.93838\n","Val 0000000067_0000000005.csv Epoch: 4, summary loss: 0.11804, class loss: 0.11804, box_loss: 0.00000, time: 167.94052\n","Val 0000000067_0000000012.csv Epoch: 4, summary loss: 0.22892, class loss: 0.22892, box_loss: 0.00000, time: 167.94255\n","Val 0000000067_0000000014.csv Epoch: 4, summary loss: 0.90766, class loss: 0.81104, box_loss: 0.00193, time: 167.94444\n","Val 0000000067_0000000015.csv Epoch: 4, summary loss: 1.30686, class loss: 1.09945, box_loss: 0.00415, time: 167.94638\n","Val 0000000067_0000000019.csv Epoch: 4, summary loss: 0.43893, class loss: 0.24479, box_loss: 0.00388, time: 167.94828\n","Val 0000000067_0000000024.csv Epoch: 4, summary loss: 0.41710, class loss: 0.25885, box_loss: 0.00316, time: 167.95035\n","Val 0000000067_0000000025.csv Epoch: 4, summary loss: 0.42137, class loss: 0.24444, box_loss: 0.00354, time: 167.96868\n","Val 0000000067_0000000026.csv Epoch: 4, summary loss: 0.18252, class loss: 0.18252, box_loss: 0.00000, time: 167.97102\n","Val 0000000067_0000000027.csv Epoch: 4, summary loss: 0.15307, class loss: 0.15307, box_loss: 0.00000, time: 167.97328\n","Val 0000000067_0000000028.csv Epoch: 4, summary loss: 0.18706, class loss: 0.17560, box_loss: 0.00023, time: 167.97538\n","Val 0000000067_0000000029.csv Epoch: 4, summary loss: 0.05593, class loss: 0.05593, box_loss: 0.00000, time: 167.97754\n","Val 0000000067_0000000031.csv Epoch: 4, summary loss: 0.53910, class loss: 0.40304, box_loss: 0.00272, time: 167.97976\n","Val 0000000067_0000000032.csv Epoch: 4, summary loss: 0.51960, class loss: 0.42119, box_loss: 0.00197, time: 167.98196\n","Val 0000000067_0000000040.csv Epoch: 4, summary loss: 0.69420, class loss: 0.61112, box_loss: 0.00166, time: 167.98410\n","Val 0000000067_0000000041.csv Epoch: 4, summary loss: 0.72624, class loss: 0.37778, box_loss: 0.00697, time: 167.98633\n","Val 0000000067_0000000045.csv Epoch: 4, summary loss: 0.38162, class loss: 0.28204, box_loss: 0.00199, time: 167.98844\n","Val 0000000067_0000000046.csv Epoch: 4, summary loss: 2.39692, class loss: 2.33952, box_loss: 0.00115, time: 167.99057\n","Val 0000000067_0000000050.csv Epoch: 4, summary loss: 1.05787, class loss: 0.67290, box_loss: 0.00770, time: 167.99270\n","Val 0000000067_0000000052.csv Epoch: 4, summary loss: 0.81463, class loss: 0.64007, box_loss: 0.00349, time: 167.99481\n","Val 0000000067_0000000055.csv Epoch: 4, summary loss: 0.51734, class loss: 0.47213, box_loss: 0.00090, time: 167.99691\n","Val 0000000067_0000000058.csv Epoch: 4, summary loss: 0.82488, class loss: 0.62324, box_loss: 0.00403, time: 167.99906\n","Val 0000000067_0000000059.csv Epoch: 4, summary loss: 0.72050, class loss: 0.72050, box_loss: 0.00000, time: 168.00127\n","Val 0000000351_0000000000.csv Epoch: 4, summary loss: 0.53500, class loss: 0.30115, box_loss: 0.00468, time: 168.00340\n","Val 0000000354_0000000000.csv Epoch: 4, summary loss: 2.64658, class loss: 2.37861, box_loss: 0.00536, time: 168.00552\n","Val 0000000359_0000000000.csv Epoch: 4, summary loss: 0.68061, class loss: 0.38320, box_loss: 0.00595, time: 168.00763\n","Val 0000000363_0000000000.csv Epoch: 4, summary loss: 1.84892, class loss: 1.84892, box_loss: 0.00000, time: 168.00974\n","Val 0000000364_0000000000.csv Epoch: 4, summary loss: 3.56889, class loss: 3.55185, box_loss: 0.00034, time: 168.01211\n","Val 0000000367_0000000000.csv Epoch: 4, summary loss: 2.64225, class loss: 2.41698, box_loss: 0.00451, time: 168.01425\n","Val Epoch: 4, summary loss: 0.89095, class loss: 0.75321, box_loss: 0.00275, time: 168.01650\n","Adjusting learning rate of group 0 to 1.8511e-04.\n","\n","2021-06-01T04:31:18.533188\n","LR: 0.00018511423331582188\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.97666\n","Train Step 250/3810, summary_loss: 0.49089, class_loss: 0.29765, box_loss: 0.00386, time: 74.31381\n","Train Step 500/3810, summary_loss: 0.47633, class_loss: 0.28782, box_loss: 0.00377, time: 147.67257\n","Train Step 750/3810, summary_loss: 0.48428, class_loss: 0.29442, box_loss: 0.00380, time: 221.79209\n","Train Step 1000/3810, summary_loss: 0.48528, class_loss: 0.29524, box_loss: 0.00380, time: 295.46503\n","Train Step 1250/3810, summary_loss: 0.48617, class_loss: 0.29460, box_loss: 0.00383, time: 368.71104\n","Train Step 1500/3810, summary_loss: 0.48500, class_loss: 0.29325, box_loss: 0.00383, time: 442.12580\n","Train Step 1750/3810, summary_loss: 0.48348, class_loss: 0.29171, box_loss: 0.00384, time: 515.27175\n","Train Step 2000/3810, summary_loss: 0.48494, class_loss: 0.29257, box_loss: 0.00385, time: 589.10600\n","Train Step 2250/3810, summary_loss: 0.48403, class_loss: 0.29176, box_loss: 0.00385, time: 662.37570\n","Train Step 2500/3810, summary_loss: 0.48509, class_loss: 0.29249, box_loss: 0.00385, time: 736.14965\n","Train Step 2750/3810, summary_loss: 0.48407, class_loss: 0.29121, box_loss: 0.00386, time: 809.01923\n","Train Step 3000/3810, summary_loss: 0.49086, class_loss: 0.29748, box_loss: 0.00387, time: 882.53184\n","Train Step 3250/3810, summary_loss: 0.49134, class_loss: 0.29782, box_loss: 0.00387, time: 955.79724\n","Train Step 3500/3810, summary_loss: 0.49079, class_loss: 0.29700, box_loss: 0.00388, time: 1029.38928\n","Train Step 3750/3810, summary_loss: 0.48961, class_loss: 0.29605, box_loss: 0.00387, time: 1102.51222\n","Train Epoch: 5, summary loss: 0.48890, class loss: 0.29565, box_loss: 0.00387, time: 1119.73025\n","Val 0000000010_0000000000.csv Epoch: 5, summary loss: 1.06569, class loss: 0.85582, box_loss: 0.00420, time: 167.30500\n","Val 0000000054_0000000000.csv Epoch: 5, summary loss: 0.98755, class loss: 0.62038, box_loss: 0.00734, time: 167.30804\n","Val 0000000056_0000000000.csv Epoch: 5, summary loss: 0.92467, class loss: 0.53826, box_loss: 0.00773, time: 167.31178\n","Val 0000000062_0000000000.csv Epoch: 5, summary loss: 0.69993, class loss: 0.47453, box_loss: 0.00451, time: 167.31393\n","Val 0000000067_0000000005.csv Epoch: 5, summary loss: 0.12332, class loss: 0.12332, box_loss: 0.00000, time: 167.31598\n","Val 0000000067_0000000012.csv Epoch: 5, summary loss: 0.07418, class loss: 0.07418, box_loss: 0.00000, time: 167.31809\n","Val 0000000067_0000000014.csv Epoch: 5, summary loss: 0.27320, class loss: 0.21386, box_loss: 0.00119, time: 167.32019\n","Val 0000000067_0000000015.csv Epoch: 5, summary loss: 0.50188, class loss: 0.25115, box_loss: 0.00501, time: 167.32229\n","Val 0000000067_0000000019.csv Epoch: 5, summary loss: 0.39050, class loss: 0.21196, box_loss: 0.00357, time: 167.32434\n","Val 0000000067_0000000024.csv Epoch: 5, summary loss: 0.33807, class loss: 0.15822, box_loss: 0.00360, time: 167.32635\n","Val 0000000067_0000000025.csv Epoch: 5, summary loss: 0.46435, class loss: 0.21987, box_loss: 0.00489, time: 167.32828\n","Val 0000000067_0000000026.csv Epoch: 5, summary loss: 0.09951, class loss: 0.09951, box_loss: 0.00000, time: 167.33028\n","Val 0000000067_0000000027.csv Epoch: 5, summary loss: 0.14995, class loss: 0.14995, box_loss: 0.00000, time: 167.33234\n","Val 0000000067_0000000028.csv Epoch: 5, summary loss: 0.12738, class loss: 0.10501, box_loss: 0.00045, time: 167.33436\n","Val 0000000067_0000000029.csv Epoch: 5, summary loss: 0.10070, class loss: 0.10070, box_loss: 0.00000, time: 167.33641\n","Val 0000000067_0000000031.csv Epoch: 5, summary loss: 0.36060, class loss: 0.20586, box_loss: 0.00309, time: 167.33849\n","Val 0000000067_0000000032.csv Epoch: 5, summary loss: 0.42646, class loss: 0.29359, box_loss: 0.00266, time: 167.34056\n","Val 0000000067_0000000040.csv Epoch: 5, summary loss: 0.39551, class loss: 0.33066, box_loss: 0.00130, time: 167.34265\n","Val 0000000067_0000000041.csv Epoch: 5, summary loss: 0.74570, class loss: 0.38653, box_loss: 0.00718, time: 167.34468\n","Val 0000000067_0000000045.csv Epoch: 5, summary loss: 0.34833, class loss: 0.26155, box_loss: 0.00174, time: 167.34670\n","Val 0000000067_0000000046.csv Epoch: 5, summary loss: 0.20445, class loss: 0.13936, box_loss: 0.00130, time: 167.34880\n","Val 0000000067_0000000050.csv Epoch: 5, summary loss: 0.89023, class loss: 0.55524, box_loss: 0.00670, time: 167.35092\n","Val 0000000067_0000000052.csv Epoch: 5, summary loss: 0.79586, class loss: 0.58481, box_loss: 0.00422, time: 167.35302\n","Val 0000000067_0000000055.csv Epoch: 5, summary loss: 0.54849, class loss: 0.51668, box_loss: 0.00064, time: 167.35515\n","Val 0000000067_0000000058.csv Epoch: 5, summary loss: 1.02983, class loss: 0.81948, box_loss: 0.00421, time: 167.35724\n","Val 0000000067_0000000059.csv Epoch: 5, summary loss: 0.40239, class loss: 0.40239, box_loss: 0.00000, time: 167.35935\n","Val 0000000351_0000000000.csv Epoch: 5, summary loss: 0.59997, class loss: 0.38758, box_loss: 0.00425, time: 167.36145\n","Val 0000000354_0000000000.csv Epoch: 5, summary loss: 1.10035, class loss: 0.83368, box_loss: 0.00533, time: 167.38079\n","Val 0000000359_0000000000.csv Epoch: 5, summary loss: 0.74412, class loss: 0.43967, box_loss: 0.00609, time: 167.38460\n","Val 0000000363_0000000000.csv Epoch: 5, summary loss: 0.41353, class loss: 0.41353, box_loss: 0.00000, time: 167.38669\n","Val 0000000364_0000000000.csv Epoch: 5, summary loss: 1.65921, class loss: 1.64099, box_loss: 0.00036, time: 167.38872\n","Val 0000000367_0000000000.csv Epoch: 5, summary loss: 0.79789, class loss: 0.57829, box_loss: 0.00439, time: 167.39079\n","Val Epoch: 5, summary loss: 0.55574, class loss: 0.40583, box_loss: 0.00300, time: 167.39293\n","Adjusting learning rate of group 0 to 1.8227e-04.\n","\n","2021-06-01T04:52:46.260189\n","LR: 0.00018227276263885396\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.95115\n","Train Step 250/3810, summary_loss: 0.44981, class_loss: 0.26622, box_loss: 0.00367, time: 74.35793\n","Train Step 500/3810, summary_loss: 0.45294, class_loss: 0.26645, box_loss: 0.00373, time: 147.66587\n","Train Step 750/3810, summary_loss: 0.45398, class_loss: 0.26655, box_loss: 0.00375, time: 220.87706\n","Train Step 1000/3810, summary_loss: 0.45916, class_loss: 0.27312, box_loss: 0.00372, time: 294.30999\n","Train Step 1250/3810, summary_loss: 0.46093, class_loss: 0.27413, box_loss: 0.00374, time: 367.81632\n","Train Step 1500/3810, summary_loss: 0.46311, class_loss: 0.27484, box_loss: 0.00377, time: 441.66786\n","Train Step 1750/3810, summary_loss: 0.46345, class_loss: 0.27537, box_loss: 0.00376, time: 515.14220\n","Train Step 2000/3810, summary_loss: 0.46513, class_loss: 0.27630, box_loss: 0.00378, time: 588.49703\n","Train Step 2250/3810, summary_loss: 0.46869, class_loss: 0.27934, box_loss: 0.00379, time: 661.93797\n","Train Step 2500/3810, summary_loss: 0.47000, class_loss: 0.28027, box_loss: 0.00379, time: 735.30989\n","Train Step 2750/3810, summary_loss: 0.47344, class_loss: 0.28346, box_loss: 0.00380, time: 809.37384\n","Train Step 3000/3810, summary_loss: 0.47292, class_loss: 0.28331, box_loss: 0.00379, time: 882.55905\n","Train Step 3250/3810, summary_loss: 0.47015, class_loss: 0.28139, box_loss: 0.00378, time: 955.40627\n","Train Step 3500/3810, summary_loss: 0.46884, class_loss: 0.28036, box_loss: 0.00377, time: 1028.75616\n","Train Step 3750/3810, summary_loss: 0.46947, class_loss: 0.28032, box_loss: 0.00378, time: 1102.58106\n","Train Epoch: 6, summary loss: 0.46895, class loss: 0.27999, box_loss: 0.00378, time: 1120.08729\n","Val 0000000010_0000000000.csv Epoch: 6, summary loss: 0.71799, class loss: 0.54308, box_loss: 0.00350, time: 168.12829\n","Val 0000000054_0000000000.csv Epoch: 6, summary loss: 0.58564, class loss: 0.44545, box_loss: 0.00280, time: 168.13159\n","Val 0000000056_0000000000.csv Epoch: 6, summary loss: 0.58884, class loss: 0.39912, box_loss: 0.00379, time: 168.13416\n","Val 0000000062_0000000000.csv Epoch: 6, summary loss: 0.88413, class loss: 0.50703, box_loss: 0.00754, time: 168.13625\n","Val 0000000067_0000000005.csv Epoch: 6, summary loss: 0.11284, class loss: 0.11284, box_loss: 0.00000, time: 168.13833\n","Val 0000000067_0000000012.csv Epoch: 6, summary loss: 0.11880, class loss: 0.11880, box_loss: 0.00000, time: 168.14040\n","Val 0000000067_0000000014.csv Epoch: 6, summary loss: 0.33746, class loss: 0.29510, box_loss: 0.00085, time: 168.14245\n","Val 0000000067_0000000015.csv Epoch: 6, summary loss: 0.42282, class loss: 0.22766, box_loss: 0.00390, time: 168.14461\n","Val 0000000067_0000000019.csv Epoch: 6, summary loss: 0.42201, class loss: 0.24189, box_loss: 0.00360, time: 168.14677\n","Val 0000000067_0000000024.csv Epoch: 6, summary loss: 0.30492, class loss: 0.14104, box_loss: 0.00328, time: 168.14889\n","Val 0000000067_0000000025.csv Epoch: 6, summary loss: 0.38224, class loss: 0.22573, box_loss: 0.00313, time: 168.15100\n","Val 0000000067_0000000026.csv Epoch: 6, summary loss: 0.05466, class loss: 0.05466, box_loss: 0.00000, time: 168.15307\n","Val 0000000067_0000000027.csv Epoch: 6, summary loss: 0.12205, class loss: 0.12205, box_loss: 0.00000, time: 168.17365\n","Val 0000000067_0000000028.csv Epoch: 6, summary loss: 0.09345, class loss: 0.07964, box_loss: 0.00028, time: 168.17580\n","Val 0000000067_0000000029.csv Epoch: 6, summary loss: 0.09997, class loss: 0.09997, box_loss: 0.00000, time: 168.17794\n","Val 0000000067_0000000031.csv Epoch: 6, summary loss: 0.31298, class loss: 0.18183, box_loss: 0.00262, time: 168.18001\n","Val 0000000067_0000000032.csv Epoch: 6, summary loss: 0.39337, class loss: 0.29333, box_loss: 0.00200, time: 168.18209\n","Val 0000000067_0000000040.csv Epoch: 6, summary loss: 0.39082, class loss: 0.32625, box_loss: 0.00129, time: 168.18420\n","Val 0000000067_0000000041.csv Epoch: 6, summary loss: 0.69801, class loss: 0.37047, box_loss: 0.00655, time: 168.18626\n","Val 0000000067_0000000045.csv Epoch: 6, summary loss: 0.33005, class loss: 0.25486, box_loss: 0.00150, time: 168.18829\n","Val 0000000067_0000000046.csv Epoch: 6, summary loss: 0.18981, class loss: 0.13283, box_loss: 0.00114, time: 168.19037\n","Val 0000000067_0000000050.csv Epoch: 6, summary loss: 0.94115, class loss: 0.56253, box_loss: 0.00757, time: 168.19249\n","Val 0000000067_0000000052.csv Epoch: 6, summary loss: 0.87585, class loss: 0.69764, box_loss: 0.00356, time: 168.19458\n","Val 0000000067_0000000055.csv Epoch: 6, summary loss: 0.69011, class loss: 0.65463, box_loss: 0.00071, time: 168.19668\n","Val 0000000067_0000000058.csv Epoch: 6, summary loss: 0.78595, class loss: 0.58088, box_loss: 0.00410, time: 168.19883\n","Val 0000000067_0000000059.csv Epoch: 6, summary loss: 0.56436, class loss: 0.56436, box_loss: 0.00000, time: 168.20095\n","Val 0000000351_0000000000.csv Epoch: 6, summary loss: 0.48594, class loss: 0.28544, box_loss: 0.00401, time: 168.20313\n","Val 0000000354_0000000000.csv Epoch: 6, summary loss: 1.36940, class loss: 0.92466, box_loss: 0.00889, time: 168.20524\n","Val 0000000359_0000000000.csv Epoch: 6, summary loss: 0.77551, class loss: 0.42572, box_loss: 0.00700, time: 168.20736\n","Val 0000000363_0000000000.csv Epoch: 6, summary loss: 0.47883, class loss: 0.47883, box_loss: 0.00000, time: 168.20946\n","Val 0000000364_0000000000.csv Epoch: 6, summary loss: 2.01083, class loss: 1.98904, box_loss: 0.00044, time: 168.21160\n","Val 0000000367_0000000000.csv Epoch: 6, summary loss: 0.88385, class loss: 0.62277, box_loss: 0.00522, time: 168.21379\n","Val Epoch: 6, summary loss: 0.54452, class loss: 0.40500, box_loss: 0.00279, time: 168.21605\n","Adjusting learning rate of group 0 to 1.7947e-04.\n","\n","2021-06-01T05:14:15.166069\n","LR: 0.0001794749080332353\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.68281\n","Train Step 250/3810, summary_loss: 0.46252, class_loss: 0.27497, box_loss: 0.00375, time: 75.12791\n","Train Step 500/3810, summary_loss: 0.47051, class_loss: 0.28205, box_loss: 0.00377, time: 148.05632\n","Train Step 750/3810, summary_loss: 0.48072, class_loss: 0.28851, box_loss: 0.00384, time: 221.20581\n","Train Step 1000/3810, summary_loss: 0.47889, class_loss: 0.28676, box_loss: 0.00384, time: 295.22355\n","Train Step 1250/3810, summary_loss: 0.47487, class_loss: 0.28447, box_loss: 0.00381, time: 369.13423\n","Train Step 1500/3810, summary_loss: 0.47054, class_loss: 0.28112, box_loss: 0.00379, time: 442.92395\n","Train Step 1750/3810, summary_loss: 0.47256, class_loss: 0.28344, box_loss: 0.00378, time: 516.46813\n","Train Step 2000/3810, summary_loss: 0.47487, class_loss: 0.28571, box_loss: 0.00378, time: 590.41125\n","Train Step 2250/3810, summary_loss: 0.47840, class_loss: 0.28849, box_loss: 0.00380, time: 664.52225\n","Train Step 2500/3810, summary_loss: 0.47522, class_loss: 0.28664, box_loss: 0.00377, time: 739.01016\n","Train Step 2750/3810, summary_loss: 0.47276, class_loss: 0.28516, box_loss: 0.00375, time: 813.74298\n","Train Step 3000/3810, summary_loss: 0.47190, class_loss: 0.28483, box_loss: 0.00374, time: 888.16439\n","Train Step 3250/3810, summary_loss: 0.46882, class_loss: 0.28287, box_loss: 0.00372, time: 962.75341\n","Train Step 3500/3810, summary_loss: 0.46746, class_loss: 0.28147, box_loss: 0.00372, time: 1037.28922\n","Train Step 3750/3810, summary_loss: 0.46804, class_loss: 0.28213, box_loss: 0.00372, time: 1111.33905\n","Train Epoch: 7, summary loss: 0.46770, class loss: 0.28182, box_loss: 0.00372, time: 1128.99595\n","Val 0000000010_0000000000.csv Epoch: 7, summary loss: 0.68989, class loss: 0.49088, box_loss: 0.00398, time: 172.01455\n","Val 0000000054_0000000000.csv Epoch: 7, summary loss: 0.69221, class loss: 0.51580, box_loss: 0.00353, time: 172.01826\n","Val 0000000056_0000000000.csv Epoch: 7, summary loss: 1.13669, class loss: 0.80563, box_loss: 0.00662, time: 172.02094\n","Val 0000000062_0000000000.csv Epoch: 7, summary loss: 0.75542, class loss: 0.48967, box_loss: 0.00531, time: 172.02306\n","Val 0000000067_0000000005.csv Epoch: 7, summary loss: 0.15025, class loss: 0.15025, box_loss: 0.00000, time: 172.02523\n","Val 0000000067_0000000012.csv Epoch: 7, summary loss: 0.09316, class loss: 0.09316, box_loss: 0.00000, time: 172.02740\n","Val 0000000067_0000000014.csv Epoch: 7, summary loss: 0.27303, class loss: 0.21690, box_loss: 0.00112, time: 172.02959\n","Val 0000000067_0000000015.csv Epoch: 7, summary loss: 0.41941, class loss: 0.22433, box_loss: 0.00390, time: 172.03173\n","Val 0000000067_0000000019.csv Epoch: 7, summary loss: 0.41482, class loss: 0.24561, box_loss: 0.00338, time: 172.03384\n","Val 0000000067_0000000024.csv Epoch: 7, summary loss: 0.35736, class loss: 0.16333, box_loss: 0.00388, time: 172.03599\n","Val 0000000067_0000000025.csv Epoch: 7, summary loss: 0.43748, class loss: 0.22241, box_loss: 0.00430, time: 172.03812\n","Val 0000000067_0000000026.csv Epoch: 7, summary loss: 0.09225, class loss: 0.09225, box_loss: 0.00000, time: 172.04028\n","Val 0000000067_0000000027.csv Epoch: 7, summary loss: 0.15175, class loss: 0.15175, box_loss: 0.00000, time: 172.04247\n","Val 0000000067_0000000028.csv Epoch: 7, summary loss: 0.11318, class loss: 0.09008, box_loss: 0.00046, time: 172.04467\n","Val 0000000067_0000000029.csv Epoch: 7, summary loss: 0.09900, class loss: 0.09900, box_loss: 0.00000, time: 172.04701\n","Val 0000000067_0000000031.csv Epoch: 7, summary loss: 0.35037, class loss: 0.20252, box_loss: 0.00296, time: 172.04916\n","Val 0000000067_0000000032.csv Epoch: 7, summary loss: 0.38229, class loss: 0.28031, box_loss: 0.00204, time: 172.05133\n","Val 0000000067_0000000040.csv Epoch: 7, summary loss: 0.38000, class loss: 0.31170, box_loss: 0.00137, time: 172.05352\n","Val 0000000067_0000000041.csv Epoch: 7, summary loss: 0.73176, class loss: 0.36714, box_loss: 0.00729, time: 172.05574\n","Val 0000000067_0000000045.csv Epoch: 7, summary loss: 0.33862, class loss: 0.25797, box_loss: 0.00161, time: 172.05795\n","Val 0000000067_0000000046.csv Epoch: 7, summary loss: 0.18978, class loss: 0.13463, box_loss: 0.00110, time: 172.06011\n","Val 0000000067_0000000050.csv Epoch: 7, summary loss: 0.98581, class loss: 0.61552, box_loss: 0.00741, time: 172.06226\n","Val 0000000067_0000000052.csv Epoch: 7, summary loss: 0.83999, class loss: 0.64803, box_loss: 0.00384, time: 172.06442\n","Val 0000000067_0000000055.csv Epoch: 7, summary loss: 0.82200, class loss: 0.78843, box_loss: 0.00067, time: 172.06650\n","Val 0000000067_0000000058.csv Epoch: 7, summary loss: 1.00423, class loss: 0.73597, box_loss: 0.00537, time: 172.06861\n","Val 0000000067_0000000059.csv Epoch: 7, summary loss: 0.49865, class loss: 0.49865, box_loss: 0.00000, time: 172.07070\n","Val 0000000351_0000000000.csv Epoch: 7, summary loss: 0.54277, class loss: 0.31183, box_loss: 0.00462, time: 172.07277\n","Val 0000000354_0000000000.csv Epoch: 7, summary loss: 1.44087, class loss: 1.14301, box_loss: 0.00596, time: 172.09672\n","Val 0000000359_0000000000.csv Epoch: 7, summary loss: 0.68924, class loss: 0.40601, box_loss: 0.00566, time: 172.09933\n","Val 0000000363_0000000000.csv Epoch: 7, summary loss: 1.03669, class loss: 1.03669, box_loss: 0.00000, time: 172.10175\n","Val 0000000364_0000000000.csv Epoch: 7, summary loss: 2.25252, class loss: 2.23611, box_loss: 0.00033, time: 172.10400\n","Val 0000000367_0000000000.csv Epoch: 7, summary loss: 1.39189, class loss: 1.18165, box_loss: 0.00420, time: 172.10614\n","Val Epoch: 7, summary loss: 0.61729, class loss: 0.47523, box_loss: 0.00284, time: 172.10841\n","Adjusting learning rate of group 0 to 1.7672e-04.\n","\n","2021-06-01T05:35:56.584975\n","LR: 0.00017671999999999997\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 1.01476\n","Train Step 250/3810, summary_loss: 0.44536, class_loss: 0.26499, box_loss: 0.00361, time: 76.12909\n","Train Step 500/3810, summary_loss: 0.44230, class_loss: 0.26327, box_loss: 0.00358, time: 150.61011\n","Train Step 750/3810, summary_loss: 0.44781, class_loss: 0.26856, box_loss: 0.00359, time: 225.13486\n","Train Step 1000/3810, summary_loss: 0.44630, class_loss: 0.26673, box_loss: 0.00359, time: 300.04498\n","Train Step 1250/3810, summary_loss: 0.44557, class_loss: 0.26613, box_loss: 0.00359, time: 375.39917\n","Train Step 1500/3810, summary_loss: 0.44450, class_loss: 0.26523, box_loss: 0.00359, time: 450.25071\n","Train Step 1750/3810, summary_loss: 0.44191, class_loss: 0.26387, box_loss: 0.00356, time: 524.68170\n","Train Step 2000/3810, summary_loss: 0.44721, class_loss: 0.26862, box_loss: 0.00357, time: 599.18321\n","Train Step 2250/3810, summary_loss: 0.44731, class_loss: 0.26813, box_loss: 0.00358, time: 673.72341\n","Train Step 2500/3810, summary_loss: 0.44569, class_loss: 0.26684, box_loss: 0.00358, time: 747.71499\n","Train Step 2750/3810, summary_loss: 0.44598, class_loss: 0.26699, box_loss: 0.00358, time: 822.85149\n","Train Step 3000/3810, summary_loss: 0.44417, class_loss: 0.26598, box_loss: 0.00356, time: 896.91105\n","Train Step 3250/3810, summary_loss: 0.44338, class_loss: 0.26534, box_loss: 0.00356, time: 971.50632\n","Train Step 3500/3810, summary_loss: 0.44404, class_loss: 0.26603, box_loss: 0.00356, time: 1046.09816\n","Train Step 3750/3810, summary_loss: 0.44489, class_loss: 0.26688, box_loss: 0.00356, time: 1120.54209\n","Train Epoch: 8, summary loss: 0.44460, class loss: 0.26661, box_loss: 0.00356, time: 1138.30211\n","Val 0000000010_0000000000.csv Epoch: 8, summary loss: 0.62403, class loss: 0.46042, box_loss: 0.00327, time: 171.75054\n","Val 0000000054_0000000000.csv Epoch: 8, summary loss: 0.56647, class loss: 0.40500, box_loss: 0.00323, time: 171.75435\n","Val 0000000056_0000000000.csv Epoch: 8, summary loss: 0.80723, class loss: 0.64683, box_loss: 0.00321, time: 171.75665\n","Val 0000000062_0000000000.csv Epoch: 8, summary loss: 1.07140, class loss: 0.79247, box_loss: 0.00558, time: 171.75880\n","Val 0000000067_0000000005.csv Epoch: 8, summary loss: 0.07055, class loss: 0.07055, box_loss: 0.00000, time: 171.76091\n","Val 0000000067_0000000012.csv Epoch: 8, summary loss: 0.13995, class loss: 0.13995, box_loss: 0.00000, time: 171.76302\n","Val 0000000067_0000000014.csv Epoch: 8, summary loss: 0.30354, class loss: 0.26429, box_loss: 0.00078, time: 171.76508\n","Val 0000000067_0000000015.csv Epoch: 8, summary loss: 0.54620, class loss: 0.30181, box_loss: 0.00489, time: 171.76715\n","Val 0000000067_0000000019.csv Epoch: 8, summary loss: 0.38517, class loss: 0.22835, box_loss: 0.00314, time: 171.76921\n","Val 0000000067_0000000024.csv Epoch: 8, summary loss: 0.36384, class loss: 0.16560, box_loss: 0.00396, time: 171.77129\n","Val 0000000067_0000000025.csv Epoch: 8, summary loss: 0.42497, class loss: 0.21700, box_loss: 0.00416, time: 171.77334\n","Val 0000000067_0000000026.csv Epoch: 8, summary loss: 0.05183, class loss: 0.05183, box_loss: 0.00000, time: 171.77545\n","Val 0000000067_0000000027.csv Epoch: 8, summary loss: 0.09269, class loss: 0.09269, box_loss: 0.00000, time: 171.77766\n","Val 0000000067_0000000028.csv Epoch: 8, summary loss: 0.09873, class loss: 0.07970, box_loss: 0.00038, time: 171.77970\n","Val 0000000067_0000000029.csv Epoch: 8, summary loss: 0.10673, class loss: 0.10673, box_loss: 0.00000, time: 171.79827\n","Val 0000000067_0000000031.csv Epoch: 8, summary loss: 0.37388, class loss: 0.21071, box_loss: 0.00326, time: 171.80040\n","Val 0000000067_0000000032.csv Epoch: 8, summary loss: 0.39518, class loss: 0.28063, box_loss: 0.00229, time: 171.80246\n","Val 0000000067_0000000040.csv Epoch: 8, summary loss: 0.39870, class loss: 0.34228, box_loss: 0.00113, time: 171.80460\n","Val 0000000067_0000000041.csv Epoch: 8, summary loss: 0.72901, class loss: 0.38838, box_loss: 0.00681, time: 171.80668\n","Val 0000000067_0000000045.csv Epoch: 8, summary loss: 0.41642, class loss: 0.32780, box_loss: 0.00177, time: 171.80874\n","Val 0000000067_0000000046.csv Epoch: 8, summary loss: 0.18482, class loss: 0.12095, box_loss: 0.00128, time: 171.81100\n","Val 0000000067_0000000050.csv Epoch: 8, summary loss: 0.95840, class loss: 0.58432, box_loss: 0.00748, time: 171.81315\n","Val 0000000067_0000000052.csv Epoch: 8, summary loss: 1.05650, class loss: 0.80230, box_loss: 0.00508, time: 171.81522\n","Val 0000000067_0000000055.csv Epoch: 8, summary loss: 0.65377, class loss: 0.61427, box_loss: 0.00079, time: 171.81730\n","Val 0000000067_0000000058.csv Epoch: 8, summary loss: 0.66016, class loss: 0.42392, box_loss: 0.00472, time: 171.81942\n","Val 0000000067_0000000059.csv Epoch: 8, summary loss: 0.39269, class loss: 0.39269, box_loss: 0.00000, time: 171.82153\n","Val 0000000351_0000000000.csv Epoch: 8, summary loss: 0.60712, class loss: 0.39449, box_loss: 0.00425, time: 171.82360\n","Val 0000000354_0000000000.csv Epoch: 8, summary loss: 1.44036, class loss: 1.02330, box_loss: 0.00834, time: 171.82568\n","Val 0000000359_0000000000.csv Epoch: 8, summary loss: 0.74548, class loss: 0.39710, box_loss: 0.00697, time: 171.82776\n","Val 0000000363_0000000000.csv Epoch: 8, summary loss: 1.63189, class loss: 1.63189, box_loss: 0.00000, time: 171.82982\n","Val 0000000364_0000000000.csv Epoch: 8, summary loss: 0.79952, class loss: 0.77992, box_loss: 0.00039, time: 171.83193\n","Val 0000000367_0000000000.csv Epoch: 8, summary loss: 1.04518, class loss: 0.78668, box_loss: 0.00517, time: 171.83402\n","Val Epoch: 8, summary loss: 0.56695, class loss: 0.42265, box_loss: 0.00289, time: 171.83628\n","Adjusting learning rate of group 0 to 1.7401e-04.\n","\n","2021-06-01T05:57:47.028489\n","LR: 0.00017400737931687257\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.88489\n","Train Step 250/3810, summary_loss: 0.43754, class_loss: 0.26316, box_loss: 0.00349, time: 76.58608\n","Train Step 500/3810, summary_loss: 0.44014, class_loss: 0.26454, box_loss: 0.00351, time: 151.20542\n","Train Step 750/3810, summary_loss: 0.44225, class_loss: 0.26579, box_loss: 0.00353, time: 226.26186\n","Train Step 1000/3810, summary_loss: 0.44288, class_loss: 0.26626, box_loss: 0.00353, time: 301.21823\n","Train Step 1250/3810, summary_loss: 0.44340, class_loss: 0.26698, box_loss: 0.00353, time: 376.04701\n","Train Step 1500/3810, summary_loss: 0.44005, class_loss: 0.26462, box_loss: 0.00351, time: 450.84443\n","Train Step 1750/3810, summary_loss: 0.44157, class_loss: 0.26549, box_loss: 0.00352, time: 525.73088\n","Train Step 2000/3810, summary_loss: 0.43924, class_loss: 0.26422, box_loss: 0.00350, time: 600.82906\n","Train Step 2250/3810, summary_loss: 0.43804, class_loss: 0.26329, box_loss: 0.00349, time: 675.62734\n","Train Step 2500/3810, summary_loss: 0.43621, class_loss: 0.26196, box_loss: 0.00348, time: 750.12078\n","Train Step 2750/3810, summary_loss: 0.43607, class_loss: 0.26171, box_loss: 0.00349, time: 824.40455\n","Train Step 3000/3810, summary_loss: 0.43650, class_loss: 0.26197, box_loss: 0.00349, time: 898.89302\n","Train Step 3250/3810, summary_loss: 0.43679, class_loss: 0.26193, box_loss: 0.00350, time: 973.52851\n","Train Step 3500/3810, summary_loss: 0.43462, class_loss: 0.26048, box_loss: 0.00348, time: 1048.45451\n","Train Step 3750/3810, summary_loss: 0.43545, class_loss: 0.26072, box_loss: 0.00349, time: 1123.28044\n","Train Epoch: 9, summary loss: 0.43477, class loss: 0.26034, box_loss: 0.00349, time: 1141.15332\n","Val 0000000010_0000000000.csv Epoch: 9, summary loss: 1.02477, class loss: 0.69970, box_loss: 0.00650, time: 172.69236\n","Val 0000000054_0000000000.csv Epoch: 9, summary loss: 1.20129, class loss: 0.91967, box_loss: 0.00563, time: 172.69572\n","Val 0000000056_0000000000.csv Epoch: 9, summary loss: 0.44769, class loss: 0.32730, box_loss: 0.00241, time: 172.69813\n","Val 0000000062_0000000000.csv Epoch: 9, summary loss: 1.00786, class loss: 0.68416, box_loss: 0.00647, time: 172.70035\n","Val 0000000067_0000000005.csv Epoch: 9, summary loss: 0.09926, class loss: 0.09926, box_loss: 0.00000, time: 172.70249\n","Val 0000000067_0000000012.csv Epoch: 9, summary loss: 0.07997, class loss: 0.07997, box_loss: 0.00000, time: 172.70460\n","Val 0000000067_0000000014.csv Epoch: 9, summary loss: 0.32880, class loss: 0.26761, box_loss: 0.00122, time: 172.70669\n","Val 0000000067_0000000015.csv Epoch: 9, summary loss: 0.43281, class loss: 0.26925, box_loss: 0.00327, time: 172.70882\n","Val 0000000067_0000000019.csv Epoch: 9, summary loss: 0.45256, class loss: 0.22849, box_loss: 0.00448, time: 172.71090\n","Val 0000000067_0000000024.csv Epoch: 9, summary loss: 0.37521, class loss: 0.20776, box_loss: 0.00335, time: 172.71302\n","Val 0000000067_0000000025.csv Epoch: 9, summary loss: 0.40169, class loss: 0.22782, box_loss: 0.00348, time: 172.71511\n","Val 0000000067_0000000026.csv Epoch: 9, summary loss: 0.17608, class loss: 0.17608, box_loss: 0.00000, time: 172.71720\n","Val 0000000067_0000000027.csv Epoch: 9, summary loss: 0.20117, class loss: 0.20117, box_loss: 0.00000, time: 172.71927\n","Val 0000000067_0000000028.csv Epoch: 9, summary loss: 0.15816, class loss: 0.13978, box_loss: 0.00037, time: 172.72137\n","Val 0000000067_0000000029.csv Epoch: 9, summary loss: 0.12343, class loss: 0.12343, box_loss: 0.00000, time: 172.72347\n","Val 0000000067_0000000031.csv Epoch: 9, summary loss: 0.33937, class loss: 0.21717, box_loss: 0.00244, time: 172.72554\n","Val 0000000067_0000000032.csv Epoch: 9, summary loss: 0.40393, class loss: 0.29619, box_loss: 0.00215, time: 172.72760\n","Val 0000000067_0000000040.csv Epoch: 9, summary loss: 0.37634, class loss: 0.30667, box_loss: 0.00139, time: 172.72971\n","Val 0000000067_0000000041.csv Epoch: 9, summary loss: 0.74222, class loss: 0.39016, box_loss: 0.00704, time: 172.73182\n","Val 0000000067_0000000045.csv Epoch: 9, summary loss: 0.32745, class loss: 0.25704, box_loss: 0.00141, time: 172.73393\n","Val 0000000067_0000000046.csv Epoch: 9, summary loss: 0.23529, class loss: 0.15971, box_loss: 0.00151, time: 172.73604\n","Val 0000000067_0000000050.csv Epoch: 9, summary loss: 0.93049, class loss: 0.53942, box_loss: 0.00782, time: 172.73812\n","Val 0000000067_0000000052.csv Epoch: 9, summary loss: 0.74659, class loss: 0.53942, box_loss: 0.00414, time: 172.74020\n","Val 0000000067_0000000055.csv Epoch: 9, summary loss: 0.45003, class loss: 0.41004, box_loss: 0.00080, time: 172.74227\n","Val 0000000067_0000000058.csv Epoch: 9, summary loss: 0.54839, class loss: 0.41110, box_loss: 0.00275, time: 172.74437\n","Val 0000000067_0000000059.csv Epoch: 9, summary loss: 0.45218, class loss: 0.45218, box_loss: 0.00000, time: 172.74649\n","Val 0000000351_0000000000.csv Epoch: 9, summary loss: 0.56743, class loss: 0.37070, box_loss: 0.00393, time: 172.74855\n","Val 0000000354_0000000000.csv Epoch: 9, summary loss: 1.37275, class loss: 1.02413, box_loss: 0.00697, time: 172.75062\n","Val 0000000359_0000000000.csv Epoch: 9, summary loss: 0.70846, class loss: 0.36159, box_loss: 0.00694, time: 172.75274\n","Val 0000000363_0000000000.csv Epoch: 9, summary loss: 1.22366, class loss: 1.22366, box_loss: 0.00000, time: 172.75481\n","Val 0000000364_0000000000.csv Epoch: 9, summary loss: 2.05936, class loss: 2.03807, box_loss: 0.00043, time: 172.75690\n","Val 0000000367_0000000000.csv Epoch: 9, summary loss: 0.94075, class loss: 0.74386, box_loss: 0.00394, time: 172.78021\n","Val Epoch: 9, summary loss: 0.59173, class loss: 0.44977, box_loss: 0.00284, time: 172.78263\n","Adjusting learning rate of group 0 to 1.7134e-04.\n","\n","2021-06-01T06:19:41.263735\n","LR: 0.0001713363968805227\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.81414\n","Train Step 250/3810, summary_loss: 0.43182, class_loss: 0.25473, box_loss: 0.00354, time: 76.60616\n","Train Step 500/3810, summary_loss: 0.43045, class_loss: 0.25853, box_loss: 0.00344, time: 151.79278\n","Train Step 750/3810, summary_loss: 0.43100, class_loss: 0.25919, box_loss: 0.00344, time: 226.60139\n","Train Step 1000/3810, summary_loss: 0.43222, class_loss: 0.26005, box_loss: 0.00344, time: 301.46172\n","Train Step 1250/3810, summary_loss: 0.43382, class_loss: 0.26132, box_loss: 0.00345, time: 376.07463\n","Train Step 1500/3810, summary_loss: 0.43317, class_loss: 0.26068, box_loss: 0.00345, time: 450.41818\n","Train Step 1750/3810, summary_loss: 0.43534, class_loss: 0.26343, box_loss: 0.00344, time: 525.41747\n","Train Step 2000/3810, summary_loss: 0.43612, class_loss: 0.26382, box_loss: 0.00345, time: 600.06069\n","Train Step 2250/3810, summary_loss: 0.43299, class_loss: 0.26219, box_loss: 0.00342, time: 674.98360\n","Train Step 2500/3810, summary_loss: 0.43217, class_loss: 0.26096, box_loss: 0.00342, time: 749.98385\n","Train Step 2750/3810, summary_loss: 0.43059, class_loss: 0.25969, box_loss: 0.00342, time: 824.74175\n","Train Step 3000/3810, summary_loss: 0.43007, class_loss: 0.25871, box_loss: 0.00343, time: 899.60171\n","Train Step 3250/3810, summary_loss: 0.42992, class_loss: 0.25900, box_loss: 0.00342, time: 974.08627\n","Train Step 3500/3810, summary_loss: 0.42887, class_loss: 0.25832, box_loss: 0.00341, time: 1049.03272\n","Train Step 3750/3810, summary_loss: 0.42784, class_loss: 0.25777, box_loss: 0.00340, time: 1123.49820\n","Train Epoch: 10, summary loss: 0.42798, class loss: 0.25773, box_loss: 0.00340, time: 1141.36498\n","Val 0000000010_0000000000.csv Epoch: 10, summary loss: 0.59739, class loss: 0.44555, box_loss: 0.00304, time: 172.50081\n","Val 0000000054_0000000000.csv Epoch: 10, summary loss: 0.60062, class loss: 0.46120, box_loss: 0.00279, time: 172.50508\n","Val 0000000056_0000000000.csv Epoch: 10, summary loss: 0.51663, class loss: 0.40054, box_loss: 0.00232, time: 172.50776\n","Val 0000000062_0000000000.csv Epoch: 10, summary loss: 0.94582, class loss: 0.60645, box_loss: 0.00679, time: 172.51011\n","Val 0000000067_0000000005.csv Epoch: 10, summary loss: 0.11089, class loss: 0.11089, box_loss: 0.00000, time: 172.51233\n","Val 0000000067_0000000012.csv Epoch: 10, summary loss: 0.08674, class loss: 0.08674, box_loss: 0.00000, time: 172.51455\n","Val 0000000067_0000000014.csv Epoch: 10, summary loss: 0.31266, class loss: 0.22514, box_loss: 0.00175, time: 172.51675\n","Val 0000000067_0000000015.csv Epoch: 10, summary loss: 0.47669, class loss: 0.30687, box_loss: 0.00340, time: 172.51892\n","Val 0000000067_0000000019.csv Epoch: 10, summary loss: 0.42464, class loss: 0.22725, box_loss: 0.00395, time: 172.52109\n","Val 0000000067_0000000024.csv Epoch: 10, summary loss: 0.32361, class loss: 0.16570, box_loss: 0.00316, time: 172.52329\n","Val 0000000067_0000000025.csv Epoch: 10, summary loss: 0.35355, class loss: 0.19539, box_loss: 0.00316, time: 172.52544\n","Val 0000000067_0000000026.csv Epoch: 10, summary loss: 0.10282, class loss: 0.10282, box_loss: 0.00000, time: 172.52763\n","Val 0000000067_0000000027.csv Epoch: 10, summary loss: 0.14409, class loss: 0.14409, box_loss: 0.00000, time: 172.52989\n","Val 0000000067_0000000028.csv Epoch: 10, summary loss: 0.13714, class loss: 0.12243, box_loss: 0.00029, time: 172.53230\n","Val 0000000067_0000000029.csv Epoch: 10, summary loss: 0.09919, class loss: 0.09919, box_loss: 0.00000, time: 172.53483\n","Val 0000000067_0000000031.csv Epoch: 10, summary loss: 0.33761, class loss: 0.22081, box_loss: 0.00234, time: 172.53793\n","Val 0000000067_0000000032.csv Epoch: 10, summary loss: 0.39356, class loss: 0.29066, box_loss: 0.00206, time: 172.54012\n","Val 0000000067_0000000040.csv Epoch: 10, summary loss: 0.46118, class loss: 0.39923, box_loss: 0.00124, time: 172.55869\n","Val 0000000067_0000000041.csv Epoch: 10, summary loss: 0.68061, class loss: 0.39493, box_loss: 0.00571, time: 172.56263\n","Val 0000000067_0000000045.csv Epoch: 10, summary loss: 0.32978, class loss: 0.26371, box_loss: 0.00132, time: 172.56482\n","Val 0000000067_0000000046.csv Epoch: 10, summary loss: 0.19048, class loss: 0.12211, box_loss: 0.00137, time: 172.56699\n","Val 0000000067_0000000050.csv Epoch: 10, summary loss: 0.83628, class loss: 0.50131, box_loss: 0.00670, time: 172.56912\n","Val 0000000067_0000000052.csv Epoch: 10, summary loss: 0.80977, class loss: 0.56244, box_loss: 0.00495, time: 172.57125\n","Val 0000000067_0000000055.csv Epoch: 10, summary loss: 0.68000, class loss: 0.64803, box_loss: 0.00064, time: 172.57337\n","Val 0000000067_0000000058.csv Epoch: 10, summary loss: 0.61932, class loss: 0.44948, box_loss: 0.00340, time: 172.57549\n","Val 0000000067_0000000059.csv Epoch: 10, summary loss: 0.51171, class loss: 0.51171, box_loss: 0.00000, time: 172.57764\n","Val 0000000351_0000000000.csv Epoch: 10, summary loss: 0.55347, class loss: 0.35612, box_loss: 0.00395, time: 172.57976\n","Val 0000000354_0000000000.csv Epoch: 10, summary loss: 1.37244, class loss: 0.94006, box_loss: 0.00865, time: 172.58189\n","Val 0000000359_0000000000.csv Epoch: 10, summary loss: 0.77236, class loss: 0.40017, box_loss: 0.00744, time: 172.58398\n","Val 0000000363_0000000000.csv Epoch: 10, summary loss: 0.35145, class loss: 0.35145, box_loss: 0.00000, time: 172.58607\n","Val 0000000364_0000000000.csv Epoch: 10, summary loss: 0.68717, class loss: 0.66139, box_loss: 0.00052, time: 172.58813\n","Val 0000000367_0000000000.csv Epoch: 10, summary loss: 0.80471, class loss: 0.58875, box_loss: 0.00432, time: 172.59108\n","Val Epoch: 10, summary loss: 0.48826, class loss: 0.35508, box_loss: 0.00266, time: 172.60055\n","Adjusting learning rate of group 0 to 1.6871e-04.\n","\n","2021-06-01T06:41:35.825939\n","LR: 0.00016870641355124118\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.89212\n","Train Step 250/3810, summary_loss: 0.41404, class_loss: 0.24890, box_loss: 0.00330, time: 76.30820\n","Train Step 500/3810, summary_loss: 0.41724, class_loss: 0.24829, box_loss: 0.00338, time: 151.20815\n","Train Step 750/3810, summary_loss: 0.41926, class_loss: 0.25058, box_loss: 0.00337, time: 225.88675\n","Train Step 1000/3810, summary_loss: 0.42464, class_loss: 0.25334, box_loss: 0.00343, time: 300.75344\n","Train Step 1250/3810, summary_loss: 0.43516, class_loss: 0.26174, box_loss: 0.00347, time: 375.57116\n","Train Step 1500/3810, summary_loss: 0.43603, class_loss: 0.26349, box_loss: 0.00345, time: 449.95117\n","Train Step 1750/3810, summary_loss: 0.44176, class_loss: 0.26831, box_loss: 0.00347, time: 525.32820\n","Train Step 2000/3810, summary_loss: 0.44354, class_loss: 0.26920, box_loss: 0.00349, time: 599.82493\n","Train Step 2250/3810, summary_loss: 0.44203, class_loss: 0.26802, box_loss: 0.00348, time: 674.55888\n","Train Step 2500/3810, summary_loss: 0.44254, class_loss: 0.26818, box_loss: 0.00349, time: 750.17793\n","Train Step 2750/3810, summary_loss: 0.44117, class_loss: 0.26722, box_loss: 0.00348, time: 825.28703\n","Train Step 3000/3810, summary_loss: 0.44033, class_loss: 0.26684, box_loss: 0.00347, time: 900.75063\n","Train Step 3250/3810, summary_loss: 0.43866, class_loss: 0.26531, box_loss: 0.00347, time: 975.34223\n","Train Step 3500/3810, summary_loss: 0.43698, class_loss: 0.26407, box_loss: 0.00346, time: 1050.26156\n","Train Step 3750/3810, summary_loss: 0.43526, class_loss: 0.26290, box_loss: 0.00345, time: 1124.95633\n","Train Epoch: 11, summary loss: 0.43533, class loss: 0.26285, box_loss: 0.00345, time: 1142.48906\n","Val 0000000010_0000000000.csv Epoch: 11, summary loss: 0.77171, class loss: 0.50996, box_loss: 0.00523, time: 172.35417\n","Val 0000000054_0000000000.csv Epoch: 11, summary loss: 0.66823, class loss: 0.44324, box_loss: 0.00450, time: 172.35766\n","Val 0000000056_0000000000.csv Epoch: 11, summary loss: 0.46311, class loss: 0.33605, box_loss: 0.00254, time: 172.37119\n","Val 0000000062_0000000000.csv Epoch: 11, summary loss: 0.70546, class loss: 0.45842, box_loss: 0.00494, time: 172.37371\n","Val 0000000067_0000000005.csv Epoch: 11, summary loss: 0.04849, class loss: 0.04849, box_loss: 0.00000, time: 172.37596\n","Val 0000000067_0000000012.csv Epoch: 11, summary loss: 0.22686, class loss: 0.22686, box_loss: 0.00000, time: 172.37824\n","Val 0000000067_0000000014.csv Epoch: 11, summary loss: 0.27718, class loss: 0.17276, box_loss: 0.00209, time: 172.38045\n","Val 0000000067_0000000015.csv Epoch: 11, summary loss: 0.57578, class loss: 0.25902, box_loss: 0.00634, time: 172.38270\n","Val 0000000067_0000000019.csv Epoch: 11, summary loss: 0.45646, class loss: 0.25799, box_loss: 0.00397, time: 172.38481\n","Val 0000000067_0000000024.csv Epoch: 11, summary loss: 0.36805, class loss: 0.17240, box_loss: 0.00391, time: 172.38698\n","Val 0000000067_0000000025.csv Epoch: 11, summary loss: 0.38856, class loss: 0.20644, box_loss: 0.00364, time: 172.38917\n","Val 0000000067_0000000026.csv Epoch: 11, summary loss: 0.06835, class loss: 0.06835, box_loss: 0.00000, time: 172.39137\n","Val 0000000067_0000000027.csv Epoch: 11, summary loss: 0.06333, class loss: 0.06333, box_loss: 0.00000, time: 172.39353\n","Val 0000000067_0000000028.csv Epoch: 11, summary loss: 0.15859, class loss: 0.13070, box_loss: 0.00056, time: 172.39571\n","Val 0000000067_0000000029.csv Epoch: 11, summary loss: 0.10628, class loss: 0.10628, box_loss: 0.00000, time: 172.39788\n","Val 0000000067_0000000031.csv Epoch: 11, summary loss: 0.36869, class loss: 0.20885, box_loss: 0.00320, time: 172.40002\n","Val 0000000067_0000000032.csv Epoch: 11, summary loss: 0.52363, class loss: 0.34704, box_loss: 0.00353, time: 172.40220\n","Val 0000000067_0000000040.csv Epoch: 11, summary loss: 0.38280, class loss: 0.30687, box_loss: 0.00152, time: 172.40439\n","Val 0000000067_0000000041.csv Epoch: 11, summary loss: 0.74248, class loss: 0.39273, box_loss: 0.00700, time: 172.40656\n","Val 0000000067_0000000045.csv Epoch: 11, summary loss: 0.41935, class loss: 0.33159, box_loss: 0.00176, time: 172.40869\n","Val 0000000067_0000000046.csv Epoch: 11, summary loss: 0.19972, class loss: 0.12726, box_loss: 0.00145, time: 172.41085\n","Val 0000000067_0000000050.csv Epoch: 11, summary loss: 0.84827, class loss: 0.51753, box_loss: 0.00661, time: 172.41312\n","Val 0000000067_0000000052.csv Epoch: 11, summary loss: 0.98828, class loss: 0.66890, box_loss: 0.00639, time: 172.41526\n","Val 0000000067_0000000055.csv Epoch: 11, summary loss: 0.77914, class loss: 0.71800, box_loss: 0.00122, time: 172.41740\n","Val 0000000067_0000000058.csv Epoch: 11, summary loss: 0.71961, class loss: 0.50036, box_loss: 0.00439, time: 172.41953\n","Val 0000000067_0000000059.csv Epoch: 11, summary loss: 0.58489, class loss: 0.58489, box_loss: 0.00000, time: 172.42168\n","Val 0000000351_0000000000.csv Epoch: 11, summary loss: 0.52230, class loss: 0.29900, box_loss: 0.00447, time: 172.42378\n","Val 0000000354_0000000000.csv Epoch: 11, summary loss: 1.20299, class loss: 0.77030, box_loss: 0.00865, time: 172.42591\n","Val 0000000359_0000000000.csv Epoch: 11, summary loss: 0.67183, class loss: 0.37707, box_loss: 0.00590, time: 172.42827\n","Val 0000000363_0000000000.csv Epoch: 11, summary loss: 0.67877, class loss: 0.67877, box_loss: 0.00000, time: 172.43048\n","Val 0000000364_0000000000.csv Epoch: 11, summary loss: 0.84003, class loss: 0.81551, box_loss: 0.00049, time: 172.43266\n","Val 0000000367_0000000000.csv Epoch: 11, summary loss: 1.10762, class loss: 0.92207, box_loss: 0.00371, time: 172.43484\n","Val Epoch: 11, summary loss: 0.52896, class loss: 0.37584, box_loss: 0.00306, time: 172.45509\n","Adjusting learning rate of group 0 to 1.6612e-04.\n","\n","2021-06-01T07:03:31.078471\n","LR: 0.00016611679999999998\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.72645\n","Train Step 250/3810, summary_loss: 0.42321, class_loss: 0.25063, box_loss: 0.00345, time: 76.13413\n","Train Step 500/3810, summary_loss: 0.41348, class_loss: 0.24662, box_loss: 0.00334, time: 150.32285\n","Train Step 750/3810, summary_loss: 0.41468, class_loss: 0.24821, box_loss: 0.00333, time: 225.55479\n","Train Step 1000/3810, summary_loss: 0.41427, class_loss: 0.24788, box_loss: 0.00333, time: 299.96957\n","Train Step 1250/3810, summary_loss: 0.41401, class_loss: 0.24751, box_loss: 0.00333, time: 375.46421\n","Train Step 1500/3810, summary_loss: 0.41829, class_loss: 0.25115, box_loss: 0.00334, time: 450.82013\n","Train Step 1750/3810, summary_loss: 0.41790, class_loss: 0.25015, box_loss: 0.00335, time: 525.40789\n","Train Step 2000/3810, summary_loss: 0.41656, class_loss: 0.24955, box_loss: 0.00334, time: 599.92706\n","Train Step 2250/3810, summary_loss: 0.41618, class_loss: 0.24945, box_loss: 0.00333, time: 674.65554\n","Train Step 2500/3810, summary_loss: 0.41496, class_loss: 0.24924, box_loss: 0.00331, time: 750.13298\n","Train Step 2750/3810, summary_loss: 0.41669, class_loss: 0.24900, box_loss: 0.00335, time: 824.77903\n","Train Step 3000/3810, summary_loss: 0.41523, class_loss: 0.24801, box_loss: 0.00334, time: 899.77297\n","Train Step 3250/3810, summary_loss: 0.41650, class_loss: 0.24876, box_loss: 0.00335, time: 974.63226\n","Train Step 3500/3810, summary_loss: 0.41704, class_loss: 0.24959, box_loss: 0.00335, time: 1049.89441\n","Train Step 3750/3810, summary_loss: 0.41700, class_loss: 0.24961, box_loss: 0.00335, time: 1125.10000\n","Train Epoch: 12, summary loss: 0.41673, class loss: 0.24946, box_loss: 0.00335, time: 1142.82232\n","Val 0000000010_0000000000.csv Epoch: 12, summary loss: 0.70913, class loss: 0.50027, box_loss: 0.00418, time: 173.64073\n","Val 0000000054_0000000000.csv Epoch: 12, summary loss: 1.57631, class loss: 1.45724, box_loss: 0.00238, time: 173.64404\n","Val 0000000056_0000000000.csv Epoch: 12, summary loss: 0.40758, class loss: 0.30151, box_loss: 0.00212, time: 173.64684\n","Val 0000000062_0000000000.csv Epoch: 12, summary loss: 1.60971, class loss: 1.22822, box_loss: 0.00763, time: 173.64907\n","Val 0000000067_0000000005.csv Epoch: 12, summary loss: 0.04278, class loss: 0.04278, box_loss: 0.00000, time: 173.65121\n","Val 0000000067_0000000012.csv Epoch: 12, summary loss: 0.14283, class loss: 0.14283, box_loss: 0.00000, time: 173.65345\n","Val 0000000067_0000000014.csv Epoch: 12, summary loss: 0.25377, class loss: 0.17144, box_loss: 0.00165, time: 173.65556\n","Val 0000000067_0000000015.csv Epoch: 12, summary loss: 0.48103, class loss: 0.26129, box_loss: 0.00439, time: 173.65766\n","Val 0000000067_0000000019.csv Epoch: 12, summary loss: 0.41546, class loss: 0.21303, box_loss: 0.00405, time: 173.65978\n","Val 0000000067_0000000024.csv Epoch: 12, summary loss: 0.34235, class loss: 0.14744, box_loss: 0.00390, time: 173.66200\n","Val 0000000067_0000000025.csv Epoch: 12, summary loss: 0.34087, class loss: 0.20351, box_loss: 0.00275, time: 173.66412\n","Val 0000000067_0000000026.csv Epoch: 12, summary loss: 0.04904, class loss: 0.04904, box_loss: 0.00000, time: 173.66619\n","Val 0000000067_0000000027.csv Epoch: 12, summary loss: 0.05279, class loss: 0.05279, box_loss: 0.00000, time: 173.66830\n","Val 0000000067_0000000028.csv Epoch: 12, summary loss: 0.10468, class loss: 0.08209, box_loss: 0.00045, time: 173.67041\n","Val 0000000067_0000000029.csv Epoch: 12, summary loss: 0.06890, class loss: 0.06890, box_loss: 0.00000, time: 173.67253\n","Val 0000000067_0000000031.csv Epoch: 12, summary loss: 0.33574, class loss: 0.19964, box_loss: 0.00272, time: 173.67461\n","Val 0000000067_0000000032.csv Epoch: 12, summary loss: 0.41618, class loss: 0.31181, box_loss: 0.00209, time: 173.67682\n","Val 0000000067_0000000040.csv Epoch: 12, summary loss: 0.36278, class loss: 0.29471, box_loss: 0.00136, time: 173.67899\n","Val 0000000067_0000000041.csv Epoch: 12, summary loss: 0.68390, class loss: 0.37963, box_loss: 0.00609, time: 173.68110\n","Val 0000000067_0000000045.csv Epoch: 12, summary loss: 0.35315, class loss: 0.27304, box_loss: 0.00160, time: 173.68319\n","Val 0000000067_0000000046.csv Epoch: 12, summary loss: 0.20237, class loss: 0.13058, box_loss: 0.00144, time: 173.70382\n","Val 0000000067_0000000050.csv Epoch: 12, summary loss: 0.97260, class loss: 0.61831, box_loss: 0.00709, time: 173.70649\n","Val 0000000067_0000000052.csv Epoch: 12, summary loss: 0.95599, class loss: 0.67731, box_loss: 0.00557, time: 173.70889\n","Val 0000000067_0000000055.csv Epoch: 12, summary loss: 0.69331, class loss: 0.65279, box_loss: 0.00081, time: 173.71121\n","Val 0000000067_0000000058.csv Epoch: 12, summary loss: 0.66787, class loss: 0.49630, box_loss: 0.00343, time: 173.71341\n","Val 0000000067_0000000059.csv Epoch: 12, summary loss: 0.56103, class loss: 0.56103, box_loss: 0.00000, time: 173.71565\n","Val 0000000351_0000000000.csv Epoch: 12, summary loss: 0.52983, class loss: 0.31376, box_loss: 0.00432, time: 173.71781\n","Val 0000000354_0000000000.csv Epoch: 12, summary loss: 1.35650, class loss: 1.01440, box_loss: 0.00684, time: 173.72002\n","Val 0000000359_0000000000.csv Epoch: 12, summary loss: 0.76240, class loss: 0.40465, box_loss: 0.00715, time: 173.72220\n","Val 0000000363_0000000000.csv Epoch: 12, summary loss: 1.03009, class loss: 1.03009, box_loss: 0.00000, time: 173.72435\n","Val 0000000364_0000000000.csv Epoch: 12, summary loss: 2.27489, class loss: 2.24799, box_loss: 0.00054, time: 173.72654\n","Val 0000000367_0000000000.csv Epoch: 12, summary loss: 1.11869, class loss: 0.83832, box_loss: 0.00561, time: 173.72866\n","Val Epoch: 12, summary loss: 0.62108, class loss: 0.48021, box_loss: 0.00282, time: 173.73096\n","Adjusting learning rate of group 0 to 1.6357e-04.\n","\n","2021-06-01T07:25:27.938776\n","LR: 0.0001635669365578602\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.75646\n","Train Step 250/3810, summary_loss: 0.42078, class_loss: 0.25290, box_loss: 0.00336, time: 76.10207\n","Train Step 500/3810, summary_loss: 0.41856, class_loss: 0.25073, box_loss: 0.00336, time: 151.39041\n","Train Step 750/3810, summary_loss: 0.41650, class_loss: 0.24995, box_loss: 0.00333, time: 227.09814\n","Train Step 1000/3810, summary_loss: 0.41179, class_loss: 0.24672, box_loss: 0.00330, time: 301.76469\n","Train Step 1250/3810, summary_loss: 0.40965, class_loss: 0.24552, box_loss: 0.00328, time: 376.62440\n","Train Step 1500/3810, summary_loss: 0.40970, class_loss: 0.24589, box_loss: 0.00328, time: 452.23827\n","Train Step 1750/3810, summary_loss: 0.40919, class_loss: 0.24581, box_loss: 0.00327, time: 527.85882\n","Train Step 2000/3810, summary_loss: 0.40887, class_loss: 0.24574, box_loss: 0.00326, time: 603.35739\n","Train Step 2250/3810, summary_loss: 0.40887, class_loss: 0.24558, box_loss: 0.00327, time: 678.68217\n","Train Step 2500/3810, summary_loss: 0.40905, class_loss: 0.24568, box_loss: 0.00327, time: 753.94567\n","Train Step 2750/3810, summary_loss: 0.40904, class_loss: 0.24539, box_loss: 0.00327, time: 828.75193\n","Train Step 3000/3810, summary_loss: 0.40823, class_loss: 0.24491, box_loss: 0.00327, time: 904.02362\n","Train Step 3250/3810, summary_loss: 0.40884, class_loss: 0.24531, box_loss: 0.00327, time: 979.42755\n","Train Step 3500/3810, summary_loss: 0.40948, class_loss: 0.24579, box_loss: 0.00327, time: 1054.95748\n","Train Step 3750/3810, summary_loss: 0.40926, class_loss: 0.24565, box_loss: 0.00327, time: 1130.41405\n","Train Epoch: 13, summary loss: 0.40897, class loss: 0.24551, box_loss: 0.00327, time: 1148.31218\n","Val 0000000010_0000000000.csv Epoch: 13, summary loss: 0.76671, class loss: 0.51147, box_loss: 0.00510, time: 173.84450\n","Val 0000000054_0000000000.csv Epoch: 13, summary loss: 0.75078, class loss: 0.56568, box_loss: 0.00370, time: 173.84794\n","Val 0000000056_0000000000.csv Epoch: 13, summary loss: 0.36487, class loss: 0.26456, box_loss: 0.00201, time: 173.85044\n","Val 0000000062_0000000000.csv Epoch: 13, summary loss: 1.34300, class loss: 1.03347, box_loss: 0.00619, time: 173.85245\n","Val 0000000067_0000000005.csv Epoch: 13, summary loss: 0.08103, class loss: 0.08103, box_loss: 0.00000, time: 173.85440\n","Val 0000000067_0000000012.csv Epoch: 13, summary loss: 0.27209, class loss: 0.27209, box_loss: 0.00000, time: 173.85633\n","Val 0000000067_0000000014.csv Epoch: 13, summary loss: 0.23918, class loss: 0.18127, box_loss: 0.00116, time: 173.85831\n","Val 0000000067_0000000015.csv Epoch: 13, summary loss: 0.42734, class loss: 0.23106, box_loss: 0.00393, time: 173.86042\n","Val 0000000067_0000000019.csv Epoch: 13, summary loss: 0.39391, class loss: 0.21470, box_loss: 0.00358, time: 173.87461\n","Val 0000000067_0000000024.csv Epoch: 13, summary loss: 0.35201, class loss: 0.16680, box_loss: 0.00370, time: 173.87872\n","Val 0000000067_0000000025.csv Epoch: 13, summary loss: 0.33964, class loss: 0.20027, box_loss: 0.00279, time: 173.88094\n","Val 0000000067_0000000026.csv Epoch: 13, summary loss: 0.11003, class loss: 0.11003, box_loss: 0.00000, time: 173.88313\n","Val 0000000067_0000000027.csv Epoch: 13, summary loss: 0.13892, class loss: 0.13892, box_loss: 0.00000, time: 173.88529\n","Val 0000000067_0000000028.csv Epoch: 13, summary loss: 0.15177, class loss: 0.12998, box_loss: 0.00044, time: 173.88739\n","Val 0000000067_0000000029.csv Epoch: 13, summary loss: 0.10810, class loss: 0.10810, box_loss: 0.00000, time: 173.88958\n","Val 0000000067_0000000031.csv Epoch: 13, summary loss: 0.32157, class loss: 0.20878, box_loss: 0.00226, time: 173.89174\n","Val 0000000067_0000000032.csv Epoch: 13, summary loss: 0.42078, class loss: 0.31557, box_loss: 0.00210, time: 173.89390\n","Val 0000000067_0000000040.csv Epoch: 13, summary loss: 0.29441, class loss: 0.24228, box_loss: 0.00104, time: 173.89614\n","Val 0000000067_0000000041.csv Epoch: 13, summary loss: 0.69525, class loss: 0.39180, box_loss: 0.00607, time: 173.89824\n","Val 0000000067_0000000045.csv Epoch: 13, summary loss: 0.34929, class loss: 0.27600, box_loss: 0.00147, time: 173.90033\n","Val 0000000067_0000000046.csv Epoch: 13, summary loss: 0.20940, class loss: 0.14767, box_loss: 0.00123, time: 173.90245\n","Val 0000000067_0000000050.csv Epoch: 13, summary loss: 0.96850, class loss: 0.62476, box_loss: 0.00687, time: 173.90455\n","Val 0000000067_0000000052.csv Epoch: 13, summary loss: 0.84788, class loss: 0.64851, box_loss: 0.00399, time: 173.90663\n","Val 0000000067_0000000055.csv Epoch: 13, summary loss: 0.52028, class loss: 0.49406, box_loss: 0.00052, time: 173.90871\n","Val 0000000067_0000000058.csv Epoch: 13, summary loss: 0.98624, class loss: 0.79842, box_loss: 0.00376, time: 173.91077\n","Val 0000000067_0000000059.csv Epoch: 13, summary loss: 0.63775, class loss: 0.63775, box_loss: 0.00000, time: 173.91289\n","Val 0000000351_0000000000.csv Epoch: 13, summary loss: 0.55670, class loss: 0.33593, box_loss: 0.00442, time: 173.91501\n","Val 0000000354_0000000000.csv Epoch: 13, summary loss: 1.72820, class loss: 1.27496, box_loss: 0.00906, time: 173.91711\n","Val 0000000359_0000000000.csv Epoch: 13, summary loss: 0.87923, class loss: 0.44841, box_loss: 0.00862, time: 173.91919\n","Val 0000000363_0000000000.csv Epoch: 13, summary loss: 0.64047, class loss: 0.64047, box_loss: 0.00000, time: 173.92129\n","Val 0000000364_0000000000.csv Epoch: 13, summary loss: 0.98513, class loss: 0.96541, box_loss: 0.00039, time: 173.92337\n","Val 0000000367_0000000000.csv Epoch: 13, summary loss: 0.94307, class loss: 0.69152, box_loss: 0.00503, time: 173.92544\n","Val Epoch: 13, summary loss: 0.55699, class loss: 0.41724, box_loss: 0.00279, time: 173.92767\n","Adjusting learning rate of group 0 to 1.6106e-04.\n","\n","2021-06-01T07:47:30.499609\n","LR: 0.00016105621306769134\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.97512\n","Train Step 250/3810, summary_loss: 0.40727, class_loss: 0.24859, box_loss: 0.00317, time: 76.79370\n","Train Step 500/3810, summary_loss: 0.40614, class_loss: 0.24664, box_loss: 0.00319, time: 151.62778\n","Train Step 750/3810, summary_loss: 0.40898, class_loss: 0.24907, box_loss: 0.00320, time: 226.99421\n","Train Step 1000/3810, summary_loss: 0.41164, class_loss: 0.25149, box_loss: 0.00320, time: 302.07076\n","Train Step 1250/3810, summary_loss: 0.41097, class_loss: 0.25040, box_loss: 0.00321, time: 376.79962\n","Train Step 1500/3810, summary_loss: 0.40845, class_loss: 0.24829, box_loss: 0.00320, time: 451.24425\n","Train Step 1750/3810, summary_loss: 0.40854, class_loss: 0.24771, box_loss: 0.00322, time: 526.29370\n","Train Step 2000/3810, summary_loss: 0.40915, class_loss: 0.24761, box_loss: 0.00323, time: 600.80533\n","Train Step 2250/3810, summary_loss: 0.40965, class_loss: 0.24767, box_loss: 0.00324, time: 675.68496\n","Train Step 2500/3810, summary_loss: 0.40894, class_loss: 0.24724, box_loss: 0.00323, time: 750.24047\n","Train Step 2750/3810, summary_loss: 0.41013, class_loss: 0.24837, box_loss: 0.00324, time: 825.01078\n","Train Step 3000/3810, summary_loss: 0.41153, class_loss: 0.25011, box_loss: 0.00323, time: 900.13247\n","Train Step 3250/3810, summary_loss: 0.41133, class_loss: 0.25003, box_loss: 0.00323, time: 975.18063\n","Train Step 3500/3810, summary_loss: 0.41119, class_loss: 0.24992, box_loss: 0.00323, time: 1050.01174\n","Train Step 3750/3810, summary_loss: 0.41086, class_loss: 0.24927, box_loss: 0.00323, time: 1124.60539\n","Train Epoch: 14, summary loss: 0.41048, class loss: 0.24904, box_loss: 0.00323, time: 1142.38357\n","Val 0000000010_0000000000.csv Epoch: 14, summary loss: 0.79534, class loss: 0.61779, box_loss: 0.00355, time: 173.03232\n","Val 0000000054_0000000000.csv Epoch: 14, summary loss: 0.67377, class loss: 0.49531, box_loss: 0.00357, time: 173.03540\n","Val 0000000056_0000000000.csv Epoch: 14, summary loss: 0.56656, class loss: 0.38250, box_loss: 0.00368, time: 173.03775\n","Val 0000000062_0000000000.csv Epoch: 14, summary loss: 0.82467, class loss: 0.59230, box_loss: 0.00465, time: 173.03998\n","Val 0000000067_0000000005.csv Epoch: 14, summary loss: 0.07315, class loss: 0.07315, box_loss: 0.00000, time: 173.04217\n","Val 0000000067_0000000012.csv Epoch: 14, summary loss: 0.15112, class loss: 0.15112, box_loss: 0.00000, time: 173.04434\n","Val 0000000067_0000000014.csv Epoch: 14, summary loss: 0.23980, class loss: 0.17837, box_loss: 0.00123, time: 173.04656\n","Val 0000000067_0000000015.csv Epoch: 14, summary loss: 0.39637, class loss: 0.21630, box_loss: 0.00360, time: 173.04869\n","Val 0000000067_0000000019.csv Epoch: 14, summary loss: 0.42116, class loss: 0.24850, box_loss: 0.00345, time: 173.05082\n","Val 0000000067_0000000024.csv Epoch: 14, summary loss: 0.32978, class loss: 0.14753, box_loss: 0.00364, time: 173.05302\n","Val 0000000067_0000000025.csv Epoch: 14, summary loss: 0.35568, class loss: 0.21991, box_loss: 0.00272, time: 173.05513\n","Val 0000000067_0000000026.csv Epoch: 14, summary loss: 0.05079, class loss: 0.05079, box_loss: 0.00000, time: 173.05728\n","Val 0000000067_0000000027.csv Epoch: 14, summary loss: 0.08773, class loss: 0.08773, box_loss: 0.00000, time: 173.05940\n","Val 0000000067_0000000028.csv Epoch: 14, summary loss: 0.10335, class loss: 0.08035, box_loss: 0.00046, time: 173.06156\n","Val 0000000067_0000000029.csv Epoch: 14, summary loss: 0.10607, class loss: 0.10607, box_loss: 0.00000, time: 173.06369\n","Val 0000000067_0000000031.csv Epoch: 14, summary loss: 0.33073, class loss: 0.18530, box_loss: 0.00291, time: 173.06579\n","Val 0000000067_0000000032.csv Epoch: 14, summary loss: 0.41006, class loss: 0.28042, box_loss: 0.00259, time: 173.06789\n","Val 0000000067_0000000040.csv Epoch: 14, summary loss: 0.42134, class loss: 0.35073, box_loss: 0.00141, time: 173.07001\n","Val 0000000067_0000000041.csv Epoch: 14, summary loss: 0.71238, class loss: 0.37415, box_loss: 0.00676, time: 173.07212\n","Val 0000000067_0000000045.csv Epoch: 14, summary loss: 0.40581, class loss: 0.33225, box_loss: 0.00147, time: 173.07420\n","Val 0000000067_0000000046.csv Epoch: 14, summary loss: 0.17766, class loss: 0.11405, box_loss: 0.00127, time: 173.07634\n","Val 0000000067_0000000050.csv Epoch: 14, summary loss: 0.93097, class loss: 0.59083, box_loss: 0.00680, time: 173.07847\n","Val 0000000067_0000000052.csv Epoch: 14, summary loss: 0.89132, class loss: 0.60893, box_loss: 0.00565, time: 173.08073\n","Val 0000000067_0000000055.csv Epoch: 14, summary loss: 0.51042, class loss: 0.46381, box_loss: 0.00093, time: 173.08285\n","Val 0000000067_0000000058.csv Epoch: 14, summary loss: 1.14008, class loss: 0.94147, box_loss: 0.00397, time: 173.08496\n","Val 0000000067_0000000059.csv Epoch: 14, summary loss: 0.32609, class loss: 0.32609, box_loss: 0.00000, time: 173.08708\n","Val 0000000351_0000000000.csv Epoch: 14, summary loss: 0.51751, class loss: 0.30023, box_loss: 0.00435, time: 173.08919\n","Val 0000000354_0000000000.csv Epoch: 14, summary loss: 1.26359, class loss: 0.91641, box_loss: 0.00694, time: 173.10887\n","Val 0000000359_0000000000.csv Epoch: 14, summary loss: 0.72372, class loss: 0.41675, box_loss: 0.00614, time: 173.11320\n","Val 0000000363_0000000000.csv Epoch: 14, summary loss: 0.21040, class loss: 0.21040, box_loss: 0.00000, time: 173.11555\n","Val 0000000364_0000000000.csv Epoch: 14, summary loss: 0.78197, class loss: 0.75668, box_loss: 0.00051, time: 173.11771\n","Val 0000000367_0000000000.csv Epoch: 14, summary loss: 1.77844, class loss: 1.51177, box_loss: 0.00533, time: 173.11982\n","Val Epoch: 14, summary loss: 0.52212, class loss: 0.38525, box_loss: 0.00274, time: 173.12210\n","Adjusting learning rate of group 0 to 1.5858e-04.\n","\n","2021-06-01T08:09:26.313470\n","LR: 0.00015858402873816668\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.91174\n","Train Step 250/3810, summary_loss: 0.40951, class_loss: 0.24475, box_loss: 0.00330, time: 76.50952\n","Train Step 500/3810, summary_loss: 0.40883, class_loss: 0.24675, box_loss: 0.00324, time: 151.31674\n","Train Step 750/3810, summary_loss: 0.40211, class_loss: 0.24312, box_loss: 0.00318, time: 226.28968\n","Train Step 1000/3810, summary_loss: 0.40111, class_loss: 0.24439, box_loss: 0.00313, time: 301.04318\n","Train Step 1250/3810, summary_loss: 0.40474, class_loss: 0.24660, box_loss: 0.00316, time: 375.91503\n","Train Step 1500/3810, summary_loss: 0.40206, class_loss: 0.24479, box_loss: 0.00315, time: 451.24581\n","Train Step 1750/3810, summary_loss: 0.40505, class_loss: 0.24642, box_loss: 0.00317, time: 525.99293\n","Train Step 2000/3810, summary_loss: 0.40271, class_loss: 0.24471, box_loss: 0.00316, time: 601.01653\n","Train Step 2250/3810, summary_loss: 0.40303, class_loss: 0.24503, box_loss: 0.00316, time: 675.57402\n","Train Step 2500/3810, summary_loss: 0.40561, class_loss: 0.24624, box_loss: 0.00319, time: 750.48613\n","Train Step 2750/3810, summary_loss: 0.40732, class_loss: 0.24759, box_loss: 0.00319, time: 824.99097\n","Train Step 3000/3810, summary_loss: 0.40868, class_loss: 0.24804, box_loss: 0.00321, time: 899.44172\n","Train Step 3250/3810, summary_loss: 0.40900, class_loss: 0.24809, box_loss: 0.00322, time: 974.71079\n","Train Step 3500/3810, summary_loss: 0.40740, class_loss: 0.24718, box_loss: 0.00320, time: 1049.91601\n","Train Step 3750/3810, summary_loss: 0.40668, class_loss: 0.24663, box_loss: 0.00320, time: 1125.90897\n","Train Epoch: 15, summary loss: 0.40669, class loss: 0.24656, box_loss: 0.00320, time: 1143.67506\n","Val 0000000010_0000000000.csv Epoch: 15, summary loss: 0.57470, class loss: 0.42410, box_loss: 0.00301, time: 174.34451\n","Val 0000000054_0000000000.csv Epoch: 15, summary loss: 0.62676, class loss: 0.47802, box_loss: 0.00297, time: 174.34813\n","Val 0000000056_0000000000.csv Epoch: 15, summary loss: 0.40484, class loss: 0.25997, box_loss: 0.00290, time: 174.35058\n","Val 0000000062_0000000000.csv Epoch: 15, summary loss: 1.59479, class loss: 1.32625, box_loss: 0.00537, time: 174.35262\n","Val 0000000067_0000000005.csv Epoch: 15, summary loss: 0.24835, class loss: 0.24835, box_loss: 0.00000, time: 174.35487\n","Val 0000000067_0000000012.csv Epoch: 15, summary loss: 0.93093, class loss: 0.93093, box_loss: 0.00000, time: 174.35708\n","Val 0000000067_0000000014.csv Epoch: 15, summary loss: 0.45355, class loss: 0.39945, box_loss: 0.00108, time: 174.35930\n","Val 0000000067_0000000015.csv Epoch: 15, summary loss: 0.75063, class loss: 0.52915, box_loss: 0.00443, time: 174.36149\n","Val 0000000067_0000000019.csv Epoch: 15, summary loss: 0.59785, class loss: 0.42243, box_loss: 0.00351, time: 174.36367\n","Val 0000000067_0000000024.csv Epoch: 15, summary loss: 0.43454, class loss: 0.25083, box_loss: 0.00367, time: 174.36588\n","Val 0000000067_0000000025.csv Epoch: 15, summary loss: 0.34833, class loss: 0.22529, box_loss: 0.00246, time: 174.36804\n","Val 0000000067_0000000026.csv Epoch: 15, summary loss: 0.13938, class loss: 0.13938, box_loss: 0.00000, time: 174.37019\n","Val 0000000067_0000000027.csv Epoch: 15, summary loss: 0.27564, class loss: 0.27564, box_loss: 0.00000, time: 174.37234\n","Val 0000000067_0000000028.csv Epoch: 15, summary loss: 0.32379, class loss: 0.30459, box_loss: 0.00038, time: 174.37455\n","Val 0000000067_0000000029.csv Epoch: 15, summary loss: 0.29039, class loss: 0.29039, box_loss: 0.00000, time: 174.37675\n","Val 0000000067_0000000031.csv Epoch: 15, summary loss: 0.41313, class loss: 0.27110, box_loss: 0.00284, time: 174.37892\n","Val 0000000067_0000000032.csv Epoch: 15, summary loss: 0.74551, class loss: 0.60634, box_loss: 0.00278, time: 174.39809\n","Val 0000000067_0000000040.csv Epoch: 15, summary loss: 1.13090, class loss: 1.05321, box_loss: 0.00155, time: 174.40038\n","Val 0000000067_0000000041.csv Epoch: 15, summary loss: 0.69224, class loss: 0.35873, box_loss: 0.00667, time: 174.40262\n","Val 0000000067_0000000045.csv Epoch: 15, summary loss: 0.66237, class loss: 0.57344, box_loss: 0.00178, time: 174.40478\n","Val 0000000067_0000000046.csv Epoch: 15, summary loss: 0.19958, class loss: 0.13593, box_loss: 0.00127, time: 174.40693\n","Val 0000000067_0000000050.csv Epoch: 15, summary loss: 1.07699, class loss: 0.80764, box_loss: 0.00539, time: 174.40911\n","Val 0000000067_0000000052.csv Epoch: 15, summary loss: 0.98968, class loss: 0.87795, box_loss: 0.00223, time: 174.41125\n","Val 0000000067_0000000055.csv Epoch: 15, summary loss: 1.03629, class loss: 1.00690, box_loss: 0.00059, time: 174.41343\n","Val 0000000067_0000000058.csv Epoch: 15, summary loss: 2.37865, class loss: 2.18216, box_loss: 0.00393, time: 174.41554\n","Val 0000000067_0000000059.csv Epoch: 15, summary loss: 1.09573, class loss: 1.09573, box_loss: 0.00000, time: 174.41767\n","Val 0000000351_0000000000.csv Epoch: 15, summary loss: 0.58392, class loss: 0.35887, box_loss: 0.00450, time: 174.41977\n","Val 0000000354_0000000000.csv Epoch: 15, summary loss: 2.31323, class loss: 1.78757, box_loss: 0.01051, time: 174.42187\n","Val 0000000359_0000000000.csv Epoch: 15, summary loss: 0.97458, class loss: 0.60377, box_loss: 0.00742, time: 174.42397\n","Val 0000000363_0000000000.csv Epoch: 15, summary loss: 0.79614, class loss: 0.79614, box_loss: 0.00000, time: 174.42609\n","Val 0000000364_0000000000.csv Epoch: 15, summary loss: 1.18812, class loss: 1.17054, box_loss: 0.00035, time: 174.42818\n","Val 0000000367_0000000000.csv Epoch: 15, summary loss: 0.86465, class loss: 0.63754, box_loss: 0.00454, time: 174.43026\n","Val Epoch: 15, summary loss: 0.78551, class loss: 0.65089, box_loss: 0.00269, time: 174.43280\n","Adjusting learning rate of group 0 to 1.5615e-04.\n","\n","2021-06-01T08:31:24.738119\n","LR: 0.00015614979199999996\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.85549\n","Train Step 250/3810, summary_loss: 0.42942, class_loss: 0.26505, box_loss: 0.00329, time: 76.72763\n","Train Step 500/3810, summary_loss: 0.41535, class_loss: 0.25549, box_loss: 0.00320, time: 151.89494\n","Train Step 750/3810, summary_loss: 0.41482, class_loss: 0.25393, box_loss: 0.00322, time: 227.31735\n","Train Step 1000/3810, summary_loss: 0.41039, class_loss: 0.25046, box_loss: 0.00320, time: 302.46663\n","Train Step 1250/3810, summary_loss: 0.40655, class_loss: 0.24773, box_loss: 0.00318, time: 377.44491\n","Train Step 1500/3810, summary_loss: 0.40218, class_loss: 0.24486, box_loss: 0.00315, time: 452.70720\n","Train Step 1750/3810, summary_loss: 0.39963, class_loss: 0.24261, box_loss: 0.00314, time: 527.42613\n","Train Step 2000/3810, summary_loss: 0.40081, class_loss: 0.24316, box_loss: 0.00315, time: 602.41228\n","Train Step 2250/3810, summary_loss: 0.40467, class_loss: 0.24630, box_loss: 0.00317, time: 677.26563\n","Train Step 2500/3810, summary_loss: 0.40353, class_loss: 0.24561, box_loss: 0.00316, time: 752.90114\n","Train Step 2750/3810, summary_loss: 0.40315, class_loss: 0.24535, box_loss: 0.00316, time: 827.88223\n","Train Step 3000/3810, summary_loss: 0.40116, class_loss: 0.24409, box_loss: 0.00314, time: 903.17978\n","Train Step 3250/3810, summary_loss: 0.39970, class_loss: 0.24315, box_loss: 0.00313, time: 978.39891\n","Train Step 3500/3810, summary_loss: 0.40081, class_loss: 0.24415, box_loss: 0.00313, time: 1053.70242\n","Train Step 3750/3810, summary_loss: 0.40146, class_loss: 0.24440, box_loss: 0.00314, time: 1128.79439\n","Train Epoch: 16, summary loss: 0.40210, class loss: 0.24469, box_loss: 0.00315, time: 1146.80787\n","Val 0000000010_0000000000.csv Epoch: 16, summary loss: 0.68936, class loss: 0.44658, box_loss: 0.00486, time: 173.61024\n","Val 0000000054_0000000000.csv Epoch: 16, summary loss: 0.60540, class loss: 0.46818, box_loss: 0.00274, time: 173.61394\n","Val 0000000056_0000000000.csv Epoch: 16, summary loss: 0.47068, class loss: 0.34186, box_loss: 0.00258, time: 173.61654\n","Val 0000000062_0000000000.csv Epoch: 16, summary loss: 1.03857, class loss: 0.77596, box_loss: 0.00525, time: 173.61874\n","Val 0000000067_0000000005.csv Epoch: 16, summary loss: 0.03121, class loss: 0.03121, box_loss: 0.00000, time: 173.63291\n","Val 0000000067_0000000012.csv Epoch: 16, summary loss: 0.23447, class loss: 0.23447, box_loss: 0.00000, time: 173.63505\n","Val 0000000067_0000000014.csv Epoch: 16, summary loss: 0.27434, class loss: 0.17004, box_loss: 0.00209, time: 173.63711\n","Val 0000000067_0000000015.csv Epoch: 16, summary loss: 0.61043, class loss: 0.24504, box_loss: 0.00731, time: 173.63913\n","Val 0000000067_0000000019.csv Epoch: 16, summary loss: 0.44422, class loss: 0.22137, box_loss: 0.00446, time: 173.64115\n","Val 0000000067_0000000024.csv Epoch: 16, summary loss: 0.39943, class loss: 0.16893, box_loss: 0.00461, time: 173.64319\n","Val 0000000067_0000000025.csv Epoch: 16, summary loss: 0.55285, class loss: 0.19613, box_loss: 0.00713, time: 173.64522\n","Val 0000000067_0000000026.csv Epoch: 16, summary loss: 0.03325, class loss: 0.03325, box_loss: 0.00000, time: 173.64726\n","Val 0000000067_0000000027.csv Epoch: 16, summary loss: 0.08438, class loss: 0.08438, box_loss: 0.00000, time: 173.64930\n","Val 0000000067_0000000028.csv Epoch: 16, summary loss: 0.10562, class loss: 0.07100, box_loss: 0.00069, time: 173.65134\n","Val 0000000067_0000000029.csv Epoch: 16, summary loss: 0.05649, class loss: 0.05649, box_loss: 0.00000, time: 173.65340\n","Val 0000000067_0000000031.csv Epoch: 16, summary loss: 0.41104, class loss: 0.21008, box_loss: 0.00402, time: 173.65546\n","Val 0000000067_0000000032.csv Epoch: 16, summary loss: 0.54356, class loss: 0.32431, box_loss: 0.00438, time: 173.65746\n","Val 0000000067_0000000040.csv Epoch: 16, summary loss: 0.28562, class loss: 0.22800, box_loss: 0.00115, time: 173.65956\n","Val 0000000067_0000000041.csv Epoch: 16, summary loss: 0.80018, class loss: 0.41028, box_loss: 0.00780, time: 173.66158\n","Val 0000000067_0000000045.csv Epoch: 16, summary loss: 0.37959, class loss: 0.26553, box_loss: 0.00228, time: 173.66359\n","Val 0000000067_0000000046.csv Epoch: 16, summary loss: 0.15836, class loss: 0.10071, box_loss: 0.00115, time: 173.66561\n","Val 0000000067_0000000050.csv Epoch: 16, summary loss: 0.97507, class loss: 0.62561, box_loss: 0.00699, time: 173.66763\n","Val 0000000067_0000000052.csv Epoch: 16, summary loss: 0.77069, class loss: 0.58808, box_loss: 0.00365, time: 173.66965\n","Val 0000000067_0000000055.csv Epoch: 16, summary loss: 0.42867, class loss: 0.36795, box_loss: 0.00121, time: 173.67170\n","Val 0000000067_0000000058.csv Epoch: 16, summary loss: 0.94881, class loss: 0.58879, box_loss: 0.00720, time: 173.67373\n","Val 0000000067_0000000059.csv Epoch: 16, summary loss: 0.49464, class loss: 0.49464, box_loss: 0.00000, time: 173.67576\n","Val 0000000351_0000000000.csv Epoch: 16, summary loss: 0.68186, class loss: 0.42612, box_loss: 0.00511, time: 173.67778\n","Val 0000000354_0000000000.csv Epoch: 16, summary loss: 2.83754, class loss: 2.40597, box_loss: 0.00863, time: 173.67981\n","Val 0000000359_0000000000.csv Epoch: 16, summary loss: 0.88575, class loss: 0.55517, box_loss: 0.00661, time: 173.68190\n","Val 0000000363_0000000000.csv Epoch: 16, summary loss: 0.33600, class loss: 0.33600, box_loss: 0.00000, time: 173.68395\n","Val 0000000364_0000000000.csv Epoch: 16, summary loss: 0.44302, class loss: 0.42441, box_loss: 0.00037, time: 173.68595\n","Val 0000000367_0000000000.csv Epoch: 16, summary loss: 0.81100, class loss: 0.59207, box_loss: 0.00438, time: 173.68804\n","Val Epoch: 16, summary loss: 0.55694, class loss: 0.39027, box_loss: 0.00333, time: 173.69028\n","Adjusting learning rate of group 0 to 1.5375e-04.\n","\n","2021-06-01T08:53:25.544666\n","LR: 0.00015375292036438857\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.84893\n","Train Step 250/3810, summary_loss: 0.42504, class_loss: 0.26792, box_loss: 0.00314, time: 76.11722\n","Train Step 500/3810, summary_loss: 0.41192, class_loss: 0.25621, box_loss: 0.00311, time: 150.37349\n","Train Step 750/3810, summary_loss: 0.40557, class_loss: 0.25094, box_loss: 0.00309, time: 225.16676\n","Train Step 1000/3810, summary_loss: 0.40056, class_loss: 0.24711, box_loss: 0.00307, time: 299.63085\n","Train Step 1250/3810, summary_loss: 0.39837, class_loss: 0.24471, box_loss: 0.00307, time: 374.52209\n","Train Step 1500/3810, summary_loss: 0.39541, class_loss: 0.24207, box_loss: 0.00307, time: 449.10966\n","Train Step 1750/3810, summary_loss: 0.39623, class_loss: 0.24210, box_loss: 0.00308, time: 524.25005\n","Train Step 2000/3810, summary_loss: 0.39644, class_loss: 0.24195, box_loss: 0.00309, time: 598.48048\n","Train Step 2250/3810, summary_loss: 0.39450, class_loss: 0.24105, box_loss: 0.00307, time: 672.91567\n","Train Step 2500/3810, summary_loss: 0.39365, class_loss: 0.24012, box_loss: 0.00307, time: 747.67595\n","Train Step 2750/3810, summary_loss: 0.39458, class_loss: 0.24007, box_loss: 0.00309, time: 822.25049\n","Train Step 3000/3810, summary_loss: 0.39413, class_loss: 0.23990, box_loss: 0.00308, time: 897.13415\n","Train Step 3250/3810, summary_loss: 0.39392, class_loss: 0.23976, box_loss: 0.00308, time: 972.28677\n","Train Step 3500/3810, summary_loss: 0.39322, class_loss: 0.23918, box_loss: 0.00308, time: 1047.04425\n","Train Step 3750/3810, summary_loss: 0.39387, class_loss: 0.23984, box_loss: 0.00308, time: 1121.69297\n","Train Epoch: 17, summary loss: 0.39385, class loss: 0.23984, box_loss: 0.00308, time: 1139.26979\n","Val 0000000010_0000000000.csv Epoch: 17, summary loss: 0.78885, class loss: 0.60127, box_loss: 0.00375, time: 173.76863\n","Val 0000000054_0000000000.csv Epoch: 17, summary loss: 1.17961, class loss: 0.99425, box_loss: 0.00371, time: 173.77270\n","Val 0000000056_0000000000.csv Epoch: 17, summary loss: 0.35145, class loss: 0.27228, box_loss: 0.00158, time: 173.77567\n","Val 0000000062_0000000000.csv Epoch: 17, summary loss: 0.75952, class loss: 0.45538, box_loss: 0.00608, time: 173.77817\n","Val 0000000067_0000000005.csv Epoch: 17, summary loss: 0.02769, class loss: 0.02769, box_loss: 0.00000, time: 173.78065\n","Val 0000000067_0000000012.csv Epoch: 17, summary loss: 0.08512, class loss: 0.08512, box_loss: 0.00000, time: 173.78300\n","Val 0000000067_0000000014.csv Epoch: 17, summary loss: 0.21735, class loss: 0.16359, box_loss: 0.00108, time: 173.78541\n","Val 0000000067_0000000015.csv Epoch: 17, summary loss: 0.43755, class loss: 0.22921, box_loss: 0.00417, time: 173.78793\n","Val 0000000067_0000000019.csv Epoch: 17, summary loss: 0.43086, class loss: 0.21131, box_loss: 0.00439, time: 173.79028\n","Val 0000000067_0000000024.csv Epoch: 17, summary loss: 0.33958, class loss: 0.18981, box_loss: 0.00300, time: 173.79270\n","Val 0000000067_0000000025.csv Epoch: 17, summary loss: 0.35727, class loss: 0.19900, box_loss: 0.00317, time: 173.79508\n","Val 0000000067_0000000026.csv Epoch: 17, summary loss: 0.04759, class loss: 0.04759, box_loss: 0.00000, time: 173.79749\n","Val 0000000067_0000000027.csv Epoch: 17, summary loss: 0.07094, class loss: 0.07094, box_loss: 0.00000, time: 173.79984\n","Val 0000000067_0000000028.csv Epoch: 17, summary loss: 0.09071, class loss: 0.07064, box_loss: 0.00040, time: 173.80218\n","Val 0000000067_0000000029.csv Epoch: 17, summary loss: 0.04965, class loss: 0.04965, box_loss: 0.00000, time: 173.80458\n","Val 0000000067_0000000031.csv Epoch: 17, summary loss: 0.33148, class loss: 0.19519, box_loss: 0.00273, time: 173.80691\n","Val 0000000067_0000000032.csv Epoch: 17, summary loss: 0.40585, class loss: 0.31021, box_loss: 0.00191, time: 173.80924\n","Val 0000000067_0000000040.csv Epoch: 17, summary loss: 0.37096, class loss: 0.30974, box_loss: 0.00122, time: 173.81165\n","Val 0000000067_0000000041.csv Epoch: 17, summary loss: 0.67482, class loss: 0.35495, box_loss: 0.00640, time: 173.81409\n","Val 0000000067_0000000045.csv Epoch: 17, summary loss: 0.39197, class loss: 0.30640, box_loss: 0.00171, time: 173.81652\n","Val 0000000067_0000000046.csv Epoch: 17, summary loss: 0.17147, class loss: 0.11207, box_loss: 0.00119, time: 173.81884\n","Val 0000000067_0000000050.csv Epoch: 17, summary loss: 0.91115, class loss: 0.60646, box_loss: 0.00609, time: 173.82120\n","Val 0000000067_0000000052.csv Epoch: 17, summary loss: 0.64403, class loss: 0.51575, box_loss: 0.00257, time: 173.84262\n","Val 0000000067_0000000055.csv Epoch: 17, summary loss: 0.33669, class loss: 0.31347, box_loss: 0.00046, time: 173.84517\n","Val 0000000067_0000000058.csv Epoch: 17, summary loss: 0.88262, class loss: 0.66477, box_loss: 0.00436, time: 173.84768\n","Val 0000000067_0000000059.csv Epoch: 17, summary loss: 0.48687, class loss: 0.48687, box_loss: 0.00000, time: 173.85010\n","Val 0000000351_0000000000.csv Epoch: 17, summary loss: 0.75477, class loss: 0.50375, box_loss: 0.00502, time: 173.85257\n","Val 0000000354_0000000000.csv Epoch: 17, summary loss: 1.68544, class loss: 1.27578, box_loss: 0.00819, time: 173.85500\n","Val 0000000359_0000000000.csv Epoch: 17, summary loss: 0.85319, class loss: 0.51597, box_loss: 0.00674, time: 173.85736\n","Val 0000000363_0000000000.csv Epoch: 17, summary loss: 0.35555, class loss: 0.35555, box_loss: 0.00000, time: 173.85979\n","Val 0000000364_0000000000.csv Epoch: 17, summary loss: 1.13848, class loss: 1.11599, box_loss: 0.00045, time: 173.86229\n","Val 0000000367_0000000000.csv Epoch: 17, summary loss: 1.16443, class loss: 0.88587, box_loss: 0.00557, time: 173.86481\n","Val Epoch: 17, summary loss: 0.52480, class loss: 0.39052, box_loss: 0.00269, time: 173.86750\n","Adjusting learning rate of group 0 to 1.5139e-04.\n","\n","2021-06-01T09:15:19.021250\n","LR: 0.00015139284028362984\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 1.00704\n","Train Step 250/3810, summary_loss: 0.39196, class_loss: 0.24049, box_loss: 0.00303, time: 76.73475\n","Train Step 500/3810, summary_loss: 0.38817, class_loss: 0.23617, box_loss: 0.00304, time: 151.48894\n","Train Step 750/3810, summary_loss: 0.38877, class_loss: 0.23638, box_loss: 0.00305, time: 226.69038\n","Train Step 1000/3810, summary_loss: 0.38954, class_loss: 0.23605, box_loss: 0.00307, time: 301.26667\n","Train Step 1250/3810, summary_loss: 0.38626, class_loss: 0.23492, box_loss: 0.00303, time: 376.55247\n","Train Step 1500/3810, summary_loss: 0.38227, class_loss: 0.23308, box_loss: 0.00298, time: 451.44023\n","Train Step 1750/3810, summary_loss: 0.38242, class_loss: 0.23297, box_loss: 0.00299, time: 526.34036\n","Train Step 2000/3810, summary_loss: 0.38371, class_loss: 0.23336, box_loss: 0.00301, time: 601.53716\n","Train Step 2250/3810, summary_loss: 0.38316, class_loss: 0.23305, box_loss: 0.00300, time: 676.81687\n","Train Step 2500/3810, summary_loss: 0.38664, class_loss: 0.23576, box_loss: 0.00302, time: 751.84604\n","Train Step 2750/3810, summary_loss: 0.38760, class_loss: 0.23611, box_loss: 0.00303, time: 826.68042\n","Train Step 3000/3810, summary_loss: 0.38998, class_loss: 0.23747, box_loss: 0.00305, time: 901.50006\n","Train Step 3250/3810, summary_loss: 0.38888, class_loss: 0.23677, box_loss: 0.00304, time: 976.69770\n","Train Step 3500/3810, summary_loss: 0.38845, class_loss: 0.23625, box_loss: 0.00304, time: 1051.67203\n","Train Step 3750/3810, summary_loss: 0.39032, class_loss: 0.23769, box_loss: 0.00305, time: 1126.60552\n","Train Epoch: 18, summary loss: 0.39017, class loss: 0.23758, box_loss: 0.00305, time: 1144.52545\n","Val 0000000010_0000000000.csv Epoch: 18, summary loss: 0.86176, class loss: 0.58239, box_loss: 0.00559, time: 173.39456\n","Val 0000000054_0000000000.csv Epoch: 18, summary loss: 0.97437, class loss: 0.74598, box_loss: 0.00457, time: 173.39833\n","Val 0000000056_0000000000.csv Epoch: 18, summary loss: 0.41278, class loss: 0.29463, box_loss: 0.00236, time: 173.40094\n","Val 0000000062_0000000000.csv Epoch: 18, summary loss: 0.69029, class loss: 0.43255, box_loss: 0.00515, time: 173.40315\n","Val 0000000067_0000000005.csv Epoch: 18, summary loss: 0.05119, class loss: 0.05119, box_loss: 0.00000, time: 173.40532\n","Val 0000000067_0000000012.csv Epoch: 18, summary loss: 0.16795, class loss: 0.16795, box_loss: 0.00000, time: 173.40745\n","Val 0000000067_0000000014.csv Epoch: 18, summary loss: 0.27413, class loss: 0.18742, box_loss: 0.00173, time: 173.40953\n","Val 0000000067_0000000015.csv Epoch: 18, summary loss: 0.50267, class loss: 0.27215, box_loss: 0.00461, time: 173.41163\n","Val 0000000067_0000000019.csv Epoch: 18, summary loss: 0.40538, class loss: 0.22063, box_loss: 0.00369, time: 173.41370\n","Val 0000000067_0000000024.csv Epoch: 18, summary loss: 0.36027, class loss: 0.16033, box_loss: 0.00400, time: 173.41580\n","Val 0000000067_0000000025.csv Epoch: 18, summary loss: 0.33701, class loss: 0.20174, box_loss: 0.00271, time: 173.43243\n","Val 0000000067_0000000026.csv Epoch: 18, summary loss: 0.06898, class loss: 0.06898, box_loss: 0.00000, time: 173.43496\n","Val 0000000067_0000000027.csv Epoch: 18, summary loss: 0.09670, class loss: 0.09670, box_loss: 0.00000, time: 173.43724\n","Val 0000000067_0000000028.csv Epoch: 18, summary loss: 0.14414, class loss: 0.11576, box_loss: 0.00057, time: 173.43938\n","Val 0000000067_0000000029.csv Epoch: 18, summary loss: 0.13597, class loss: 0.13597, box_loss: 0.00000, time: 173.44149\n","Val 0000000067_0000000031.csv Epoch: 18, summary loss: 0.30465, class loss: 0.19238, box_loss: 0.00225, time: 173.44356\n","Val 0000000067_0000000032.csv Epoch: 18, summary loss: 0.45694, class loss: 0.33567, box_loss: 0.00243, time: 173.44564\n","Val 0000000067_0000000040.csv Epoch: 18, summary loss: 0.39293, class loss: 0.33579, box_loss: 0.00114, time: 173.44772\n","Val 0000000067_0000000041.csv Epoch: 18, summary loss: 0.64672, class loss: 0.35201, box_loss: 0.00589, time: 173.45012\n","Val 0000000067_0000000045.csv Epoch: 18, summary loss: 0.43054, class loss: 0.35322, box_loss: 0.00155, time: 173.45229\n","Val 0000000067_0000000046.csv Epoch: 18, summary loss: 0.17168, class loss: 0.10988, box_loss: 0.00124, time: 173.45445\n","Val 0000000067_0000000050.csv Epoch: 18, summary loss: 0.85579, class loss: 0.53608, box_loss: 0.00639, time: 173.45655\n","Val 0000000067_0000000052.csv Epoch: 18, summary loss: 0.82834, class loss: 0.65004, box_loss: 0.00357, time: 173.45866\n","Val 0000000067_0000000055.csv Epoch: 18, summary loss: 0.39814, class loss: 0.34824, box_loss: 0.00100, time: 173.46078\n","Val 0000000067_0000000058.csv Epoch: 18, summary loss: 0.66769, class loss: 0.49964, box_loss: 0.00336, time: 173.46290\n","Val 0000000067_0000000059.csv Epoch: 18, summary loss: 0.57893, class loss: 0.57893, box_loss: 0.00000, time: 173.46503\n","Val 0000000351_0000000000.csv Epoch: 18, summary loss: 0.62256, class loss: 0.39883, box_loss: 0.00447, time: 173.46720\n","Val 0000000354_0000000000.csv Epoch: 18, summary loss: 2.17735, class loss: 1.41792, box_loss: 0.01519, time: 173.46932\n","Val 0000000359_0000000000.csv Epoch: 18, summary loss: 0.79746, class loss: 0.37317, box_loss: 0.00849, time: 173.47145\n","Val 0000000363_0000000000.csv Epoch: 18, summary loss: 0.48066, class loss: 0.48066, box_loss: 0.00000, time: 173.47375\n","Val 0000000364_0000000000.csv Epoch: 18, summary loss: 1.16850, class loss: 1.14787, box_loss: 0.00041, time: 173.47589\n","Val 0000000367_0000000000.csv Epoch: 18, summary loss: 1.30668, class loss: 1.02580, box_loss: 0.00562, time: 173.47806\n","Val Epoch: 18, summary loss: 0.55529, class loss: 0.40220, box_loss: 0.00306, time: 173.48031\n","Adjusting learning rate of group 0 to 1.4907e-04.\n","\n","2021-06-01T09:37:17.326638\n","LR: 0.00014906898701387667\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.86820\n","Train Step 250/3810, summary_loss: 0.40360, class_loss: 0.24457, box_loss: 0.00318, time: 76.26825\n","Train Step 500/3810, summary_loss: 0.39080, class_loss: 0.23755, box_loss: 0.00306, time: 151.17627\n","Train Step 750/3810, summary_loss: 0.39002, class_loss: 0.23776, box_loss: 0.00305, time: 225.74039\n","Train Step 1000/3810, summary_loss: 0.38725, class_loss: 0.23654, box_loss: 0.00301, time: 300.55312\n","Train Step 1250/3810, summary_loss: 0.39050, class_loss: 0.23896, box_loss: 0.00303, time: 375.88746\n","Train Step 1500/3810, summary_loss: 0.39079, class_loss: 0.23915, box_loss: 0.00303, time: 450.83394\n","Train Step 1750/3810, summary_loss: 0.38989, class_loss: 0.23811, box_loss: 0.00304, time: 525.65807\n","Train Step 2000/3810, summary_loss: 0.38909, class_loss: 0.23741, box_loss: 0.00303, time: 600.25058\n","Train Step 2250/3810, summary_loss: 0.38753, class_loss: 0.23635, box_loss: 0.00302, time: 675.74563\n","Train Step 2500/3810, summary_loss: 0.38458, class_loss: 0.23426, box_loss: 0.00301, time: 750.37560\n","Train Step 2750/3810, summary_loss: 0.38297, class_loss: 0.23317, box_loss: 0.00300, time: 825.30009\n","Train Step 3000/3810, summary_loss: 0.38174, class_loss: 0.23240, box_loss: 0.00299, time: 900.08468\n","Train Step 3250/3810, summary_loss: 0.38169, class_loss: 0.23232, box_loss: 0.00299, time: 975.02016\n","Train Step 3500/3810, summary_loss: 0.38302, class_loss: 0.23317, box_loss: 0.00300, time: 1049.71964\n","Train Step 3750/3810, summary_loss: 0.38352, class_loss: 0.23345, box_loss: 0.00300, time: 1123.80097\n","Train Epoch: 19, summary loss: 0.38356, class loss: 0.23342, box_loss: 0.00300, time: 1141.47429\n","Val 0000000010_0000000000.csv Epoch: 19, summary loss: 0.97338, class loss: 0.59717, box_loss: 0.00752, time: 172.99010\n","Val 0000000054_0000000000.csv Epoch: 19, summary loss: 0.53803, class loss: 0.39809, box_loss: 0.00280, time: 172.99336\n","Val 0000000056_0000000000.csv Epoch: 19, summary loss: 0.40658, class loss: 0.27230, box_loss: 0.00269, time: 172.99629\n","Val 0000000062_0000000000.csv Epoch: 19, summary loss: 0.78742, class loss: 0.50768, box_loss: 0.00559, time: 172.99864\n","Val 0000000067_0000000005.csv Epoch: 19, summary loss: 0.04348, class loss: 0.04348, box_loss: 0.00000, time: 173.00090\n","Val 0000000067_0000000012.csv Epoch: 19, summary loss: 0.30394, class loss: 0.30394, box_loss: 0.00000, time: 173.00337\n","Val 0000000067_0000000014.csv Epoch: 19, summary loss: 0.33949, class loss: 0.23269, box_loss: 0.00214, time: 173.00575\n","Val 0000000067_0000000015.csv Epoch: 19, summary loss: 0.64446, class loss: 0.32437, box_loss: 0.00640, time: 173.00829\n","Val 0000000067_0000000019.csv Epoch: 19, summary loss: 0.44142, class loss: 0.24459, box_loss: 0.00394, time: 173.01051\n","Val 0000000067_0000000024.csv Epoch: 19, summary loss: 0.37028, class loss: 0.16380, box_loss: 0.00413, time: 173.01267\n","Val 0000000067_0000000025.csv Epoch: 19, summary loss: 0.37425, class loss: 0.20455, box_loss: 0.00339, time: 173.01483\n","Val 0000000067_0000000026.csv Epoch: 19, summary loss: 0.05079, class loss: 0.05079, box_loss: 0.00000, time: 173.01714\n","Val 0000000067_0000000027.csv Epoch: 19, summary loss: 0.10060, class loss: 0.10060, box_loss: 0.00000, time: 173.01949\n","Val 0000000067_0000000028.csv Epoch: 19, summary loss: 0.17470, class loss: 0.14362, box_loss: 0.00062, time: 173.02176\n","Val 0000000067_0000000029.csv Epoch: 19, summary loss: 0.11938, class loss: 0.11938, box_loss: 0.00000, time: 173.02412\n","Val 0000000067_0000000031.csv Epoch: 19, summary loss: 0.42583, class loss: 0.23563, box_loss: 0.00380, time: 173.02659\n","Val 0000000067_0000000032.csv Epoch: 19, summary loss: 0.50789, class loss: 0.34908, box_loss: 0.00318, time: 173.02901\n","Val 0000000067_0000000040.csv Epoch: 19, summary loss: 0.51569, class loss: 0.44600, box_loss: 0.00139, time: 173.03150\n","Val 0000000067_0000000041.csv Epoch: 19, summary loss: 0.81641, class loss: 0.41653, box_loss: 0.00800, time: 173.03395\n","Val 0000000067_0000000045.csv Epoch: 19, summary loss: 0.44370, class loss: 0.33744, box_loss: 0.00213, time: 173.03640\n","Val 0000000067_0000000046.csv Epoch: 19, summary loss: 0.17192, class loss: 0.11419, box_loss: 0.00115, time: 173.03902\n","Val 0000000067_0000000050.csv Epoch: 19, summary loss: 0.91706, class loss: 0.59438, box_loss: 0.00645, time: 173.04149\n","Val 0000000067_0000000052.csv Epoch: 19, summary loss: 0.80995, class loss: 0.64466, box_loss: 0.00331, time: 173.04390\n","Val 0000000067_0000000055.csv Epoch: 19, summary loss: 0.62263, class loss: 0.57081, box_loss: 0.00104, time: 173.04635\n","Val 0000000067_0000000058.csv Epoch: 19, summary loss: 0.63400, class loss: 0.41445, box_loss: 0.00439, time: 173.04877\n","Val 0000000067_0000000059.csv Epoch: 19, summary loss: 0.58685, class loss: 0.58685, box_loss: 0.00000, time: 173.05116\n","Val 0000000351_0000000000.csv Epoch: 19, summary loss: 0.54501, class loss: 0.34436, box_loss: 0.00401, time: 173.05361\n","Val 0000000354_0000000000.csv Epoch: 19, summary loss: 1.66199, class loss: 1.26687, box_loss: 0.00790, time: 173.05604\n","Val 0000000359_0000000000.csv Epoch: 19, summary loss: 0.78120, class loss: 0.45254, box_loss: 0.00657, time: 173.05850\n","Val 0000000363_0000000000.csv Epoch: 19, summary loss: 0.36001, class loss: 0.36001, box_loss: 0.00000, time: 173.08377\n","Val 0000000364_0000000000.csv Epoch: 19, summary loss: 0.38511, class loss: 0.36482, box_loss: 0.00041, time: 173.08634\n","Val 0000000367_0000000000.csv Epoch: 19, summary loss: 2.44173, class loss: 2.18413, box_loss: 0.00515, time: 173.08886\n","Val Epoch: 19, summary loss: 0.57172, class loss: 0.41843, box_loss: 0.00307, time: 173.09156\n","Adjusting learning rate of group 0 to 1.4678e-04.\n","\n","2021-06-01T09:59:12.224377\n","LR: 0.00014678080447999995\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.64525\n","Train Step 250/3810, summary_loss: 0.36554, class_loss: 0.22426, box_loss: 0.00283, time: 76.65919\n","Train Step 500/3810, summary_loss: 0.36326, class_loss: 0.22284, box_loss: 0.00281, time: 151.60104\n","Train Step 750/3810, summary_loss: 0.37049, class_loss: 0.22687, box_loss: 0.00287, time: 226.52932\n","Train Step 1000/3810, summary_loss: 0.37870, class_loss: 0.23152, box_loss: 0.00294, time: 300.88992\n","Train Step 1250/3810, summary_loss: 0.37852, class_loss: 0.23073, box_loss: 0.00296, time: 375.66135\n","Train Step 1500/3810, summary_loss: 0.37656, class_loss: 0.22990, box_loss: 0.00293, time: 450.80013\n","Train Step 1750/3810, summary_loss: 0.37509, class_loss: 0.22921, box_loss: 0.00292, time: 525.97710\n","Train Step 2000/3810, summary_loss: 0.37568, class_loss: 0.23005, box_loss: 0.00291, time: 600.70826\n","Train Step 2250/3810, summary_loss: 0.37359, class_loss: 0.22898, box_loss: 0.00289, time: 675.19463\n","Train Step 2500/3810, summary_loss: 0.37222, class_loss: 0.22805, box_loss: 0.00288, time: 749.60954\n","Train Step 2750/3810, summary_loss: 0.37315, class_loss: 0.22831, box_loss: 0.00290, time: 824.43076\n","Train Step 3000/3810, summary_loss: 0.37336, class_loss: 0.22845, box_loss: 0.00290, time: 899.01499\n","Train Step 3250/3810, summary_loss: 0.37319, class_loss: 0.22821, box_loss: 0.00290, time: 973.53481\n","Train Step 3500/3810, summary_loss: 0.37291, class_loss: 0.22788, box_loss: 0.00290, time: 1048.47906\n","Train Step 3750/3810, summary_loss: 0.37540, class_loss: 0.22990, box_loss: 0.00291, time: 1122.95184\n","Train Epoch: 20, summary loss: 0.37516, class loss: 0.22969, box_loss: 0.00291, time: 1140.78915\n","Val 0000000010_0000000000.csv Epoch: 20, summary loss: 0.75632, class loss: 0.55808, box_loss: 0.00396, time: 173.56005\n","Val 0000000054_0000000000.csv Epoch: 20, summary loss: 0.59346, class loss: 0.40564, box_loss: 0.00376, time: 173.56365\n","Val 0000000056_0000000000.csv Epoch: 20, summary loss: 0.32880, class loss: 0.23670, box_loss: 0.00184, time: 173.56631\n","Val 0000000062_0000000000.csv Epoch: 20, summary loss: 0.79479, class loss: 0.54907, box_loss: 0.00491, time: 173.56870\n","Val 0000000067_0000000005.csv Epoch: 20, summary loss: 0.02557, class loss: 0.02557, box_loss: 0.00000, time: 173.57095\n","Val 0000000067_0000000012.csv Epoch: 20, summary loss: 0.13544, class loss: 0.13544, box_loss: 0.00000, time: 173.57315\n","Val 0000000067_0000000014.csv Epoch: 20, summary loss: 0.25139, class loss: 0.15158, box_loss: 0.00200, time: 173.57524\n","Val 0000000067_0000000015.csv Epoch: 20, summary loss: 0.50965, class loss: 0.26163, box_loss: 0.00496, time: 173.57732\n","Val 0000000067_0000000019.csv Epoch: 20, summary loss: 0.40275, class loss: 0.18912, box_loss: 0.00427, time: 173.57936\n","Val 0000000067_0000000024.csv Epoch: 20, summary loss: 0.39111, class loss: 0.17923, box_loss: 0.00424, time: 173.58147\n","Val 0000000067_0000000025.csv Epoch: 20, summary loss: 0.33891, class loss: 0.20123, box_loss: 0.00275, time: 173.58357\n","Val 0000000067_0000000026.csv Epoch: 20, summary loss: 0.04896, class loss: 0.04896, box_loss: 0.00000, time: 173.58568\n","Val 0000000067_0000000027.csv Epoch: 20, summary loss: 0.05080, class loss: 0.05080, box_loss: 0.00000, time: 173.58779\n","Val 0000000067_0000000028.csv Epoch: 20, summary loss: 0.12069, class loss: 0.09164, box_loss: 0.00058, time: 173.58992\n","Val 0000000067_0000000029.csv Epoch: 20, summary loss: 0.04876, class loss: 0.04876, box_loss: 0.00000, time: 173.59201\n","Val 0000000067_0000000031.csv Epoch: 20, summary loss: 0.34071, class loss: 0.19543, box_loss: 0.00291, time: 173.59413\n","Val 0000000067_0000000032.csv Epoch: 20, summary loss: 0.40884, class loss: 0.26245, box_loss: 0.00293, time: 173.61193\n","Val 0000000067_0000000040.csv Epoch: 20, summary loss: 0.27201, class loss: 0.20032, box_loss: 0.00143, time: 173.61597\n","Val 0000000067_0000000041.csv Epoch: 20, summary loss: 0.69414, class loss: 0.37671, box_loss: 0.00635, time: 173.61806\n","Val 0000000067_0000000045.csv Epoch: 20, summary loss: 0.38156, class loss: 0.29754, box_loss: 0.00168, time: 173.62013\n","Val 0000000067_0000000046.csv Epoch: 20, summary loss: 0.18755, class loss: 0.12859, box_loss: 0.00118, time: 173.62218\n","Val 0000000067_0000000050.csv Epoch: 20, summary loss: 0.83590, class loss: 0.48249, box_loss: 0.00707, time: 173.62427\n","Val 0000000067_0000000052.csv Epoch: 20, summary loss: 0.88340, class loss: 0.58292, box_loss: 0.00601, time: 173.62634\n","Val 0000000067_0000000055.csv Epoch: 20, summary loss: 0.37035, class loss: 0.32035, box_loss: 0.00100, time: 173.62836\n","Val 0000000067_0000000058.csv Epoch: 20, summary loss: 0.62104, class loss: 0.42579, box_loss: 0.00390, time: 173.63045\n","Val 0000000067_0000000059.csv Epoch: 20, summary loss: 0.39153, class loss: 0.39153, box_loss: 0.00000, time: 173.63257\n","Val 0000000351_0000000000.csv Epoch: 20, summary loss: 0.62456, class loss: 0.40081, box_loss: 0.00448, time: 173.63467\n","Val 0000000354_0000000000.csv Epoch: 20, summary loss: 1.59282, class loss: 1.17134, box_loss: 0.00843, time: 173.63674\n","Val 0000000359_0000000000.csv Epoch: 20, summary loss: 0.74841, class loss: 0.39554, box_loss: 0.00706, time: 173.63880\n","Val 0000000363_0000000000.csv Epoch: 20, summary loss: 0.62896, class loss: 0.62896, box_loss: 0.00000, time: 173.64108\n","Val 0000000364_0000000000.csv Epoch: 20, summary loss: 0.92912, class loss: 0.90760, box_loss: 0.00043, time: 173.64325\n","Val 0000000367_0000000000.csv Epoch: 20, summary loss: 0.79670, class loss: 0.55622, box_loss: 0.00481, time: 173.64537\n","Val Epoch: 20, summary loss: 0.48453, class loss: 0.33931, box_loss: 0.00290, time: 173.64778\n","Adjusting learning rate of group 0 to 1.4453e-04.\n","\n","2021-06-01T10:21:07.270204\n","LR: 0.00014452774514252525\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.98173\n","Train Step 250/3810, summary_loss: 0.37670, class_loss: 0.23131, box_loss: 0.00291, time: 76.13181\n","Train Step 500/3810, summary_loss: 0.38167, class_loss: 0.23452, box_loss: 0.00294, time: 150.90109\n","Train Step 750/3810, summary_loss: 0.37694, class_loss: 0.23146, box_loss: 0.00291, time: 225.75831\n","Train Step 1000/3810, summary_loss: 0.37574, class_loss: 0.23004, box_loss: 0.00291, time: 300.80296\n","Train Step 1250/3810, summary_loss: 0.37538, class_loss: 0.23024, box_loss: 0.00290, time: 375.83780\n","Train Step 1500/3810, summary_loss: 0.37253, class_loss: 0.22845, box_loss: 0.00288, time: 451.22417\n","Train Step 1750/3810, summary_loss: 0.37333, class_loss: 0.22918, box_loss: 0.00288, time: 525.94823\n","Train Step 2000/3810, summary_loss: 0.37362, class_loss: 0.22891, box_loss: 0.00289, time: 601.69609\n","Train Step 2250/3810, summary_loss: 0.37374, class_loss: 0.22876, box_loss: 0.00290, time: 676.56391\n","Train Step 2500/3810, summary_loss: 0.37329, class_loss: 0.22884, box_loss: 0.00289, time: 751.82713\n","Train Step 2750/3810, summary_loss: 0.37315, class_loss: 0.22843, box_loss: 0.00289, time: 826.43681\n","Train Step 3000/3810, summary_loss: 0.37472, class_loss: 0.22968, box_loss: 0.00290, time: 901.56253\n","Train Step 3250/3810, summary_loss: 0.37445, class_loss: 0.22934, box_loss: 0.00290, time: 976.57323\n","Train Step 3500/3810, summary_loss: 0.37414, class_loss: 0.22922, box_loss: 0.00290, time: 1051.67116\n","Train Step 3750/3810, summary_loss: 0.37491, class_loss: 0.22951, box_loss: 0.00291, time: 1126.74516\n","Train Epoch: 21, summary loss: 0.37473, class loss: 0.22941, box_loss: 0.00291, time: 1144.76883\n","Val 0000000010_0000000000.csv Epoch: 21, summary loss: 0.67603, class loss: 0.47721, box_loss: 0.00398, time: 173.95055\n","Val 0000000054_0000000000.csv Epoch: 21, summary loss: 0.52625, class loss: 0.38497, box_loss: 0.00283, time: 173.95444\n","Val 0000000056_0000000000.csv Epoch: 21, summary loss: 0.33419, class loss: 0.25210, box_loss: 0.00164, time: 173.95688\n","Val 0000000062_0000000000.csv Epoch: 21, summary loss: 0.82675, class loss: 0.49244, box_loss: 0.00669, time: 173.96991\n","Val 0000000067_0000000005.csv Epoch: 21, summary loss: 0.02172, class loss: 0.02172, box_loss: 0.00000, time: 173.97222\n","Val 0000000067_0000000012.csv Epoch: 21, summary loss: 0.13393, class loss: 0.13393, box_loss: 0.00000, time: 173.97434\n","Val 0000000067_0000000014.csv Epoch: 21, summary loss: 0.24413, class loss: 0.17453, box_loss: 0.00139, time: 173.97638\n","Val 0000000067_0000000015.csv Epoch: 21, summary loss: 0.52904, class loss: 0.31940, box_loss: 0.00419, time: 173.97840\n","Val 0000000067_0000000019.csv Epoch: 21, summary loss: 0.40434, class loss: 0.22931, box_loss: 0.00350, time: 173.98048\n","Val 0000000067_0000000024.csv Epoch: 21, summary loss: 0.34893, class loss: 0.16680, box_loss: 0.00364, time: 173.98251\n","Val 0000000067_0000000025.csv Epoch: 21, summary loss: 0.33030, class loss: 0.18921, box_loss: 0.00282, time: 173.98456\n","Val 0000000067_0000000026.csv Epoch: 21, summary loss: 0.07115, class loss: 0.07115, box_loss: 0.00000, time: 173.98663\n","Val 0000000067_0000000027.csv Epoch: 21, summary loss: 0.07974, class loss: 0.07974, box_loss: 0.00000, time: 173.98867\n","Val 0000000067_0000000028.csv Epoch: 21, summary loss: 0.13179, class loss: 0.11060, box_loss: 0.00042, time: 173.99070\n","Val 0000000067_0000000029.csv Epoch: 21, summary loss: 0.07252, class loss: 0.07252, box_loss: 0.00000, time: 173.99273\n","Val 0000000067_0000000031.csv Epoch: 21, summary loss: 0.36057, class loss: 0.21606, box_loss: 0.00289, time: 173.99475\n","Val 0000000067_0000000032.csv Epoch: 21, summary loss: 0.41662, class loss: 0.29881, box_loss: 0.00236, time: 173.99680\n","Val 0000000067_0000000040.csv Epoch: 21, summary loss: 0.40314, class loss: 0.34006, box_loss: 0.00126, time: 173.99885\n","Val 0000000067_0000000041.csv Epoch: 21, summary loss: 0.69175, class loss: 0.37614, box_loss: 0.00631, time: 174.00089\n","Val 0000000067_0000000045.csv Epoch: 21, summary loss: 0.38669, class loss: 0.30554, box_loss: 0.00162, time: 174.00294\n","Val 0000000067_0000000046.csv Epoch: 21, summary loss: 0.18093, class loss: 0.12557, box_loss: 0.00111, time: 174.00498\n","Val 0000000067_0000000050.csv Epoch: 21, summary loss: 0.78637, class loss: 0.50205, box_loss: 0.00569, time: 174.00704\n","Val 0000000067_0000000052.csv Epoch: 21, summary loss: 0.76425, class loss: 0.55826, box_loss: 0.00412, time: 174.00908\n","Val 0000000067_0000000055.csv Epoch: 21, summary loss: 0.34835, class loss: 0.31039, box_loss: 0.00076, time: 174.01109\n","Val 0000000067_0000000058.csv Epoch: 21, summary loss: 0.63982, class loss: 0.39379, box_loss: 0.00492, time: 174.01310\n","Val 0000000067_0000000059.csv Epoch: 21, summary loss: 0.48720, class loss: 0.48720, box_loss: 0.00000, time: 174.01512\n","Val 0000000351_0000000000.csv Epoch: 21, summary loss: 0.63861, class loss: 0.40551, box_loss: 0.00466, time: 174.01716\n","Val 0000000354_0000000000.csv Epoch: 21, summary loss: 1.34992, class loss: 0.94122, box_loss: 0.00817, time: 174.01918\n","Val 0000000359_0000000000.csv Epoch: 21, summary loss: 0.72742, class loss: 0.38823, box_loss: 0.00678, time: 174.02127\n","Val 0000000363_0000000000.csv Epoch: 21, summary loss: 0.59173, class loss: 0.59173, box_loss: 0.00000, time: 174.02356\n","Val 0000000364_0000000000.csv Epoch: 21, summary loss: 0.57963, class loss: 0.55557, box_loss: 0.00048, time: 174.02565\n","Val 0000000367_0000000000.csv Epoch: 21, summary loss: 1.28935, class loss: 1.04330, box_loss: 0.00492, time: 174.02770\n","Val Epoch: 21, summary loss: 0.48041, class loss: 0.34422, box_loss: 0.00272, time: 174.02984\n","Adjusting learning rate of group 0 to 1.4231e-04.\n","\n","2021-06-01T10:43:06.691646\n","LR: 0.00014230926986661204\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.75251\n","Train Step 250/3810, summary_loss: 0.37502, class_loss: 0.22984, box_loss: 0.00290, time: 76.22598\n","Train Step 500/3810, summary_loss: 0.37210, class_loss: 0.22794, box_loss: 0.00288, time: 150.90954\n","Train Step 750/3810, summary_loss: 0.37501, class_loss: 0.23008, box_loss: 0.00290, time: 225.45214\n","Train Step 1000/3810, summary_loss: 0.37486, class_loss: 0.22936, box_loss: 0.00291, time: 300.34313\n","Train Step 1250/3810, summary_loss: 0.37806, class_loss: 0.23272, box_loss: 0.00291, time: 374.74402\n","Train Step 1500/3810, summary_loss: 0.37570, class_loss: 0.23115, box_loss: 0.00289, time: 449.81135\n","Train Step 1750/3810, summary_loss: 0.37431, class_loss: 0.23033, box_loss: 0.00288, time: 524.36011\n","Train Step 2000/3810, summary_loss: 0.37352, class_loss: 0.22941, box_loss: 0.00288, time: 598.26694\n","Train Step 2250/3810, summary_loss: 0.37452, class_loss: 0.22976, box_loss: 0.00290, time: 672.41013\n","Train Step 2500/3810, summary_loss: 0.37453, class_loss: 0.22963, box_loss: 0.00290, time: 746.05148\n","Train Step 2750/3810, summary_loss: 0.37323, class_loss: 0.22888, box_loss: 0.00289, time: 819.36904\n","Train Step 3000/3810, summary_loss: 0.37261, class_loss: 0.22856, box_loss: 0.00288, time: 892.93103\n","Train Step 3250/3810, summary_loss: 0.37257, class_loss: 0.22871, box_loss: 0.00288, time: 966.44589\n","Train Step 3500/3810, summary_loss: 0.37204, class_loss: 0.22815, box_loss: 0.00288, time: 1040.05495\n","Train Step 3750/3810, summary_loss: 0.37235, class_loss: 0.22828, box_loss: 0.00288, time: 1113.76107\n","Train Epoch: 22, summary loss: 0.37259, class loss: 0.22833, box_loss: 0.00289, time: 1131.46770\n","Val 0000000010_0000000000.csv Epoch: 22, summary loss: 0.91184, class loss: 0.55879, box_loss: 0.00706, time: 170.65373\n","Val 0000000054_0000000000.csv Epoch: 22, summary loss: 0.48517, class loss: 0.36504, box_loss: 0.00240, time: 170.65680\n","Val 0000000056_0000000000.csv Epoch: 22, summary loss: 0.46839, class loss: 0.26999, box_loss: 0.00397, time: 170.66114\n","Val 0000000062_0000000000.csv Epoch: 22, summary loss: 0.72818, class loss: 0.51259, box_loss: 0.00431, time: 170.66330\n","Val 0000000067_0000000005.csv Epoch: 22, summary loss: 0.02380, class loss: 0.02380, box_loss: 0.00000, time: 170.66537\n","Val 0000000067_0000000012.csv Epoch: 22, summary loss: 0.12226, class loss: 0.12226, box_loss: 0.00000, time: 170.66748\n","Val 0000000067_0000000014.csv Epoch: 22, summary loss: 0.27314, class loss: 0.15454, box_loss: 0.00237, time: 170.66958\n","Val 0000000067_0000000015.csv Epoch: 22, summary loss: 0.64607, class loss: 0.24778, box_loss: 0.00797, time: 170.67171\n","Val 0000000067_0000000019.csv Epoch: 22, summary loss: 0.45183, class loss: 0.21283, box_loss: 0.00478, time: 170.67381\n","Val 0000000067_0000000024.csv Epoch: 22, summary loss: 0.39268, class loss: 0.15915, box_loss: 0.00467, time: 170.67593\n","Val 0000000067_0000000025.csv Epoch: 22, summary loss: 0.61669, class loss: 0.19279, box_loss: 0.00848, time: 170.67810\n","Val 0000000067_0000000026.csv Epoch: 22, summary loss: 0.05587, class loss: 0.05587, box_loss: 0.00000, time: 170.68025\n","Val 0000000067_0000000027.csv Epoch: 22, summary loss: 0.11002, class loss: 0.11002, box_loss: 0.00000, time: 170.68237\n","Val 0000000067_0000000028.csv Epoch: 22, summary loss: 0.12639, class loss: 0.08884, box_loss: 0.00075, time: 170.68477\n","Val 0000000067_0000000029.csv Epoch: 22, summary loss: 0.05744, class loss: 0.05744, box_loss: 0.00000, time: 170.68689\n","Val 0000000067_0000000031.csv Epoch: 22, summary loss: 0.42744, class loss: 0.20671, box_loss: 0.00441, time: 170.68901\n","Val 0000000067_0000000032.csv Epoch: 22, summary loss: 0.55587, class loss: 0.29917, box_loss: 0.00513, time: 170.69116\n","Val 0000000067_0000000040.csv Epoch: 22, summary loss: 0.30108, class loss: 0.22033, box_loss: 0.00161, time: 170.69329\n","Val 0000000067_0000000041.csv Epoch: 22, summary loss: 0.72553, class loss: 0.38962, box_loss: 0.00672, time: 170.69541\n","Val 0000000067_0000000045.csv Epoch: 22, summary loss: 0.39556, class loss: 0.26961, box_loss: 0.00252, time: 170.71382\n","Val 0000000067_0000000046.csv Epoch: 22, summary loss: 0.17061, class loss: 0.11251, box_loss: 0.00116, time: 170.71588\n","Val 0000000067_0000000050.csv Epoch: 22, summary loss: 0.91064, class loss: 0.62080, box_loss: 0.00580, time: 170.71795\n","Val 0000000067_0000000052.csv Epoch: 22, summary loss: 0.62634, class loss: 0.48428, box_loss: 0.00284, time: 170.72001\n","Val 0000000067_0000000055.csv Epoch: 22, summary loss: 0.44003, class loss: 0.35860, box_loss: 0.00163, time: 170.72211\n","Val 0000000067_0000000058.csv Epoch: 22, summary loss: 0.79104, class loss: 0.40476, box_loss: 0.00773, time: 170.72413\n","Val 0000000067_0000000059.csv Epoch: 22, summary loss: 0.45236, class loss: 0.45236, box_loss: 0.00000, time: 170.72618\n","Val 0000000351_0000000000.csv Epoch: 22, summary loss: 0.60120, class loss: 0.38291, box_loss: 0.00437, time: 170.72822\n","Val 0000000354_0000000000.csv Epoch: 22, summary loss: 1.17390, class loss: 0.73105, box_loss: 0.00886, time: 170.73027\n","Val 0000000359_0000000000.csv Epoch: 22, summary loss: 0.93014, class loss: 0.58641, box_loss: 0.00687, time: 170.73232\n","Val 0000000363_0000000000.csv Epoch: 22, summary loss: 0.55206, class loss: 0.55206, box_loss: 0.00000, time: 170.73444\n","Val 0000000364_0000000000.csv Epoch: 22, summary loss: 0.36954, class loss: 0.34625, box_loss: 0.00047, time: 170.73655\n","Val 0000000367_0000000000.csv Epoch: 22, summary loss: 1.03888, class loss: 0.80891, box_loss: 0.00460, time: 170.73863\n","Val Epoch: 22, summary loss: 0.49787, class loss: 0.32369, box_loss: 0.00348, time: 170.74087\n","Adjusting learning rate of group 0 to 1.4012e-04.\n","\n","2021-06-01T11:04:49.196027\n","LR: 0.00014012484779304406\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.82681\n","Train Step 250/3810, summary_loss: 0.35785, class_loss: 0.21945, box_loss: 0.00277, time: 75.44240\n","Train Step 500/3810, summary_loss: 0.36663, class_loss: 0.22222, box_loss: 0.00289, time: 148.95742\n","Train Step 750/3810, summary_loss: 0.36794, class_loss: 0.22354, box_loss: 0.00289, time: 222.44502\n","Train Step 1000/3810, summary_loss: 0.36745, class_loss: 0.22456, box_loss: 0.00286, time: 296.13915\n","Train Step 1250/3810, summary_loss: 0.36774, class_loss: 0.22457, box_loss: 0.00286, time: 369.69173\n","Train Step 1500/3810, summary_loss: 0.36940, class_loss: 0.22525, box_loss: 0.00288, time: 444.00561\n","Train Step 1750/3810, summary_loss: 0.36795, class_loss: 0.22442, box_loss: 0.00287, time: 517.46140\n","Train Step 2000/3810, summary_loss: 0.36751, class_loss: 0.22466, box_loss: 0.00286, time: 591.00322\n","Train Step 2250/3810, summary_loss: 0.36667, class_loss: 0.22416, box_loss: 0.00285, time: 664.22490\n","Train Step 2500/3810, summary_loss: 0.36701, class_loss: 0.22447, box_loss: 0.00285, time: 738.01974\n","Train Step 2750/3810, summary_loss: 0.36854, class_loss: 0.22535, box_loss: 0.00286, time: 811.58915\n","Train Step 3000/3810, summary_loss: 0.36951, class_loss: 0.22608, box_loss: 0.00287, time: 885.78332\n","Train Step 3250/3810, summary_loss: 0.36926, class_loss: 0.22601, box_loss: 0.00287, time: 959.18463\n","Train Step 3500/3810, summary_loss: 0.36960, class_loss: 0.22638, box_loss: 0.00286, time: 1032.72821\n","Train Step 3750/3810, summary_loss: 0.36901, class_loss: 0.22626, box_loss: 0.00286, time: 1106.37571\n","Train Epoch: 23, summary loss: 0.36897, class loss: 0.22630, box_loss: 0.00285, time: 1124.05093\n","Val 0000000010_0000000000.csv Epoch: 23, summary loss: 0.75686, class loss: 0.56874, box_loss: 0.00376, time: 170.56551\n","Val 0000000054_0000000000.csv Epoch: 23, summary loss: 0.49100, class loss: 0.36406, box_loss: 0.00254, time: 170.56872\n","Val 0000000056_0000000000.csv Epoch: 23, summary loss: 0.29500, class loss: 0.21472, box_loss: 0.00161, time: 170.57123\n","Val 0000000062_0000000000.csv Epoch: 23, summary loss: 0.70297, class loss: 0.48890, box_loss: 0.00428, time: 170.57339\n","Val 0000000067_0000000005.csv Epoch: 23, summary loss: 0.01559, class loss: 0.01559, box_loss: 0.00000, time: 170.57559\n","Val 0000000067_0000000012.csv Epoch: 23, summary loss: 0.08016, class loss: 0.08016, box_loss: 0.00000, time: 170.59226\n","Val 0000000067_0000000014.csv Epoch: 23, summary loss: 0.22482, class loss: 0.11875, box_loss: 0.00212, time: 170.59453\n","Val 0000000067_0000000015.csv Epoch: 23, summary loss: 0.58836, class loss: 0.23180, box_loss: 0.00713, time: 170.59671\n","Val 0000000067_0000000019.csv Epoch: 23, summary loss: 0.38705, class loss: 0.18224, box_loss: 0.00410, time: 170.59889\n","Val 0000000067_0000000024.csv Epoch: 23, summary loss: 0.33238, class loss: 0.16170, box_loss: 0.00341, time: 170.60108\n","Val 0000000067_0000000025.csv Epoch: 23, summary loss: 0.42234, class loss: 0.17836, box_loss: 0.00488, time: 170.60326\n","Val 0000000067_0000000026.csv Epoch: 23, summary loss: 0.03666, class loss: 0.03666, box_loss: 0.00000, time: 170.60547\n","Val 0000000067_0000000027.csv Epoch: 23, summary loss: 0.04457, class loss: 0.04457, box_loss: 0.00000, time: 170.60768\n","Val 0000000067_0000000028.csv Epoch: 23, summary loss: 0.08707, class loss: 0.05831, box_loss: 0.00058, time: 170.60978\n","Val 0000000067_0000000029.csv Epoch: 23, summary loss: 0.04290, class loss: 0.04290, box_loss: 0.00000, time: 170.61195\n","Val 0000000067_0000000031.csv Epoch: 23, summary loss: 0.39771, class loss: 0.20329, box_loss: 0.00389, time: 170.61410\n","Val 0000000067_0000000032.csv Epoch: 23, summary loss: 0.46495, class loss: 0.25931, box_loss: 0.00411, time: 170.61639\n","Val 0000000067_0000000040.csv Epoch: 23, summary loss: 0.24598, class loss: 0.18827, box_loss: 0.00115, time: 170.61867\n","Val 0000000067_0000000041.csv Epoch: 23, summary loss: 0.67755, class loss: 0.37601, box_loss: 0.00603, time: 170.62077\n","Val 0000000067_0000000045.csv Epoch: 23, summary loss: 0.29315, class loss: 0.21013, box_loss: 0.00166, time: 170.62285\n","Val 0000000067_0000000046.csv Epoch: 23, summary loss: 0.15474, class loss: 0.10653, box_loss: 0.00096, time: 170.62498\n","Val 0000000067_0000000050.csv Epoch: 23, summary loss: 0.93500, class loss: 0.63109, box_loss: 0.00608, time: 170.62711\n","Val 0000000067_0000000052.csv Epoch: 23, summary loss: 0.53101, class loss: 0.40574, box_loss: 0.00251, time: 170.62921\n","Val 0000000067_0000000055.csv Epoch: 23, summary loss: 0.47916, class loss: 0.42519, box_loss: 0.00108, time: 170.63132\n","Val 0000000067_0000000058.csv Epoch: 23, summary loss: 0.57195, class loss: 0.33646, box_loss: 0.00471, time: 170.63350\n","Val 0000000067_0000000059.csv Epoch: 23, summary loss: 0.38196, class loss: 0.38196, box_loss: 0.00000, time: 170.63565\n","Val 0000000351_0000000000.csv Epoch: 23, summary loss: 0.79514, class loss: 0.54922, box_loss: 0.00492, time: 170.63774\n","Val 0000000354_0000000000.csv Epoch: 23, summary loss: 1.20404, class loss: 0.78565, box_loss: 0.00837, time: 170.63983\n","Val 0000000359_0000000000.csv Epoch: 23, summary loss: 0.99554, class loss: 0.59309, box_loss: 0.00805, time: 170.64194\n","Val 0000000363_0000000000.csv Epoch: 23, summary loss: 0.88133, class loss: 0.88133, box_loss: 0.00000, time: 170.64406\n","Val 0000000364_0000000000.csv Epoch: 23, summary loss: 0.68483, class loss: 0.66003, box_loss: 0.00050, time: 170.64613\n","Val 0000000367_0000000000.csv Epoch: 23, summary loss: 0.93910, class loss: 0.66140, box_loss: 0.00555, time: 170.64828\n","Val Epoch: 23, summary loss: 0.47315, class loss: 0.32632, box_loss: 0.00294, time: 170.65060\n","Adjusting learning rate of group 0 to 1.3797e-04.\n","\n","2021-06-01T11:26:24.517567\n","LR: 0.00013797395621119993\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.86892\n","Train Step 250/3810, summary_loss: 0.37314, class_loss: 0.22815, box_loss: 0.00290, time: 75.66756\n","Train Step 500/3810, summary_loss: 0.37951, class_loss: 0.23238, box_loss: 0.00294, time: 149.09266\n","Train Step 750/3810, summary_loss: 0.37305, class_loss: 0.22824, box_loss: 0.00290, time: 222.60132\n","Train Step 1000/3810, summary_loss: 0.37610, class_loss: 0.22878, box_loss: 0.00295, time: 296.03545\n","Train Step 1250/3810, summary_loss: 0.37209, class_loss: 0.22618, box_loss: 0.00292, time: 370.21829\n","Train Step 1500/3810, summary_loss: 0.36910, class_loss: 0.22504, box_loss: 0.00288, time: 443.92261\n","Train Step 1750/3810, summary_loss: 0.36769, class_loss: 0.22430, box_loss: 0.00287, time: 518.12397\n","Train Step 2000/3810, summary_loss: 0.36754, class_loss: 0.22464, box_loss: 0.00286, time: 591.62430\n","Train Step 2250/3810, summary_loss: 0.36779, class_loss: 0.22479, box_loss: 0.00286, time: 665.78659\n","Train Step 2500/3810, summary_loss: 0.36704, class_loss: 0.22498, box_loss: 0.00284, time: 739.94888\n","Train Step 2750/3810, summary_loss: 0.36608, class_loss: 0.22445, box_loss: 0.00283, time: 813.62753\n","Train Step 3000/3810, summary_loss: 0.36605, class_loss: 0.22444, box_loss: 0.00283, time: 888.13202\n","Train Step 3250/3810, summary_loss: 0.36488, class_loss: 0.22357, box_loss: 0.00283, time: 961.81162\n","Train Step 3500/3810, summary_loss: 0.36512, class_loss: 0.22375, box_loss: 0.00283, time: 1036.04084\n","Train Step 3750/3810, summary_loss: 0.36371, class_loss: 0.22292, box_loss: 0.00282, time: 1109.30285\n","Train Epoch: 24, summary loss: 0.36354, class loss: 0.22279, box_loss: 0.00281, time: 1126.97932\n","Val 0000000010_0000000000.csv Epoch: 24, summary loss: 0.60107, class loss: 0.44936, box_loss: 0.00303, time: 171.49263\n","Val 0000000054_0000000000.csv Epoch: 24, summary loss: 0.53157, class loss: 0.42880, box_loss: 0.00206, time: 171.49595\n","Val 0000000056_0000000000.csv Epoch: 24, summary loss: 0.33804, class loss: 0.24568, box_loss: 0.00185, time: 171.49825\n","Val 0000000062_0000000000.csv Epoch: 24, summary loss: 0.98440, class loss: 0.72982, box_loss: 0.00509, time: 171.50011\n","Val 0000000067_0000000005.csv Epoch: 24, summary loss: 0.01846, class loss: 0.01846, box_loss: 0.00000, time: 171.50200\n","Val 0000000067_0000000012.csv Epoch: 24, summary loss: 0.15438, class loss: 0.15438, box_loss: 0.00000, time: 171.50406\n","Val 0000000067_0000000014.csv Epoch: 24, summary loss: 0.20232, class loss: 0.13775, box_loss: 0.00129, time: 171.50596\n","Val 0000000067_0000000015.csv Epoch: 24, summary loss: 0.42585, class loss: 0.23199, box_loss: 0.00388, time: 171.50788\n","Val 0000000067_0000000019.csv Epoch: 24, summary loss: 0.38442, class loss: 0.20774, box_loss: 0.00353, time: 171.50995\n","Val 0000000067_0000000024.csv Epoch: 24, summary loss: 0.32754, class loss: 0.13760, box_loss: 0.00380, time: 171.51203\n","Val 0000000067_0000000025.csv Epoch: 24, summary loss: 0.37113, class loss: 0.20199, box_loss: 0.00338, time: 171.51411\n","Val 0000000067_0000000026.csv Epoch: 24, summary loss: 0.02199, class loss: 0.02199, box_loss: 0.00000, time: 171.51618\n","Val 0000000067_0000000027.csv Epoch: 24, summary loss: 0.05879, class loss: 0.05879, box_loss: 0.00000, time: 171.51822\n","Val 0000000067_0000000028.csv Epoch: 24, summary loss: 0.08043, class loss: 0.05896, box_loss: 0.00043, time: 171.52029\n","Val 0000000067_0000000029.csv Epoch: 24, summary loss: 0.04127, class loss: 0.04127, box_loss: 0.00000, time: 171.52235\n","Val 0000000067_0000000031.csv Epoch: 24, summary loss: 0.30083, class loss: 0.16967, box_loss: 0.00262, time: 171.52441\n","Val 0000000067_0000000032.csv Epoch: 24, summary loss: 0.40321, class loss: 0.27039, box_loss: 0.00266, time: 171.54505\n","Val 0000000067_0000000040.csv Epoch: 24, summary loss: 0.27548, class loss: 0.20986, box_loss: 0.00131, time: 171.54720\n","Val 0000000067_0000000041.csv Epoch: 24, summary loss: 0.73174, class loss: 0.38804, box_loss: 0.00687, time: 171.54928\n","Val 0000000067_0000000045.csv Epoch: 24, summary loss: 0.35694, class loss: 0.27516, box_loss: 0.00164, time: 171.55132\n","Val 0000000067_0000000046.csv Epoch: 24, summary loss: 0.16440, class loss: 0.10773, box_loss: 0.00113, time: 171.55336\n","Val 0000000067_0000000050.csv Epoch: 24, summary loss: 0.90380, class loss: 0.63129, box_loss: 0.00545, time: 171.55540\n","Val 0000000067_0000000052.csv Epoch: 24, summary loss: 0.72282, class loss: 0.58415, box_loss: 0.00277, time: 171.55748\n","Val 0000000067_0000000055.csv Epoch: 24, summary loss: 0.49888, class loss: 0.46031, box_loss: 0.00077, time: 171.55954\n","Val 0000000067_0000000058.csv Epoch: 24, summary loss: 0.63233, class loss: 0.38710, box_loss: 0.00490, time: 171.56165\n","Val 0000000067_0000000059.csv Epoch: 24, summary loss: 0.47439, class loss: 0.47439, box_loss: 0.00000, time: 171.56379\n","Val 0000000351_0000000000.csv Epoch: 24, summary loss: 0.55767, class loss: 0.36159, box_loss: 0.00392, time: 171.56582\n","Val 0000000354_0000000000.csv Epoch: 24, summary loss: 1.41241, class loss: 1.01091, box_loss: 0.00803, time: 171.56788\n","Val 0000000359_0000000000.csv Epoch: 24, summary loss: 0.85250, class loss: 0.52296, box_loss: 0.00659, time: 171.56990\n","Val 0000000363_0000000000.csv Epoch: 24, summary loss: 0.28427, class loss: 0.28427, box_loss: 0.00000, time: 171.57194\n","Val 0000000364_0000000000.csv Epoch: 24, summary loss: 0.50722, class loss: 0.48177, box_loss: 0.00051, time: 171.57400\n","Val 0000000367_0000000000.csv Epoch: 24, summary loss: 0.76203, class loss: 0.49755, box_loss: 0.00529, time: 171.57602\n","Val Epoch: 24, summary loss: 0.44946, class loss: 0.32005, box_loss: 0.00259, time: 171.57821\n","Adjusting learning rate of group 0 to 1.3586e-04.\n","\n","2021-06-01T11:48:03.666746\n","LR: 0.0001358560804339737\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 1.00642\n","Train Step 250/3810, summary_loss: 0.36881, class_loss: 0.22303, box_loss: 0.00292, time: 75.20953\n","Train Step 500/3810, summary_loss: 0.37229, class_loss: 0.22563, box_loss: 0.00293, time: 148.77826\n","Train Step 750/3810, summary_loss: 0.36829, class_loss: 0.22326, box_loss: 0.00290, time: 222.55593\n","Train Step 1000/3810, summary_loss: 0.36447, class_loss: 0.22195, box_loss: 0.00285, time: 296.44914\n","Train Step 1250/3810, summary_loss: 0.36587, class_loss: 0.22321, box_loss: 0.00285, time: 370.72724\n","Train Step 1500/3810, summary_loss: 0.36807, class_loss: 0.22515, box_loss: 0.00286, time: 444.59498\n","Train Step 1750/3810, summary_loss: 0.36867, class_loss: 0.22612, box_loss: 0.00285, time: 518.25327\n","Train Step 2000/3810, summary_loss: 0.36992, class_loss: 0.22723, box_loss: 0.00285, time: 592.34592\n","Train Step 2250/3810, summary_loss: 0.36804, class_loss: 0.22623, box_loss: 0.00284, time: 665.83494\n","Train Step 2500/3810, summary_loss: 0.36729, class_loss: 0.22610, box_loss: 0.00282, time: 739.31725\n","Train Step 2750/3810, summary_loss: 0.36675, class_loss: 0.22565, box_loss: 0.00282, time: 813.11496\n","Train Step 3000/3810, summary_loss: 0.36619, class_loss: 0.22532, box_loss: 0.00282, time: 886.82316\n","Train Step 3250/3810, summary_loss: 0.36543, class_loss: 0.22463, box_loss: 0.00282, time: 960.71796\n","Train Step 3500/3810, summary_loss: 0.36535, class_loss: 0.22478, box_loss: 0.00281, time: 1034.27472\n","Train Step 3750/3810, summary_loss: 0.36395, class_loss: 0.22395, box_loss: 0.00280, time: 1108.47456\n","Train Epoch: 25, summary loss: 0.36384, class loss: 0.22388, box_loss: 0.00280, time: 1126.14110\n","Val 0000000010_0000000000.csv Epoch: 25, summary loss: 0.68774, class loss: 0.47377, box_loss: 0.00428, time: 171.10652\n","Val 0000000054_0000000000.csv Epoch: 25, summary loss: 0.63590, class loss: 0.47450, box_loss: 0.00323, time: 171.10961\n","Val 0000000056_0000000000.csv Epoch: 25, summary loss: 0.33184, class loss: 0.22966, box_loss: 0.00204, time: 171.11229\n","Val 0000000062_0000000000.csv Epoch: 25, summary loss: 0.88390, class loss: 0.60188, box_loss: 0.00564, time: 171.11572\n","Val 0000000067_0000000005.csv Epoch: 25, summary loss: 0.01208, class loss: 0.01208, box_loss: 0.00000, time: 171.11779\n","Val 0000000067_0000000012.csv Epoch: 25, summary loss: 0.03251, class loss: 0.03251, box_loss: 0.00000, time: 171.11992\n","Val 0000000067_0000000014.csv Epoch: 25, summary loss: 0.18538, class loss: 0.12786, box_loss: 0.00115, time: 171.12196\n","Val 0000000067_0000000015.csv Epoch: 25, summary loss: 0.48349, class loss: 0.21724, box_loss: 0.00533, time: 171.12401\n","Val 0000000067_0000000019.csv Epoch: 25, summary loss: 0.36641, class loss: 0.19585, box_loss: 0.00341, time: 171.12602\n","Val 0000000067_0000000024.csv Epoch: 25, summary loss: 0.34164, class loss: 0.18676, box_loss: 0.00310, time: 171.12799\n","Val 0000000067_0000000025.csv Epoch: 25, summary loss: 0.33323, class loss: 0.19783, box_loss: 0.00271, time: 171.12993\n","Val 0000000067_0000000026.csv Epoch: 25, summary loss: 0.04974, class loss: 0.04974, box_loss: 0.00000, time: 171.13203\n","Val 0000000067_0000000027.csv Epoch: 25, summary loss: 0.05800, class loss: 0.05800, box_loss: 0.00000, time: 171.13408\n","Val 0000000067_0000000028.csv Epoch: 25, summary loss: 0.08558, class loss: 0.06270, box_loss: 0.00046, time: 171.13621\n","Val 0000000067_0000000029.csv Epoch: 25, summary loss: 0.08093, class loss: 0.08093, box_loss: 0.00000, time: 171.13824\n","Val 0000000067_0000000031.csv Epoch: 25, summary loss: 0.34724, class loss: 0.19238, box_loss: 0.00310, time: 171.14029\n","Val 0000000067_0000000032.csv Epoch: 25, summary loss: 0.41230, class loss: 0.25249, box_loss: 0.00320, time: 171.14242\n","Val 0000000067_0000000040.csv Epoch: 25, summary loss: 0.25920, class loss: 0.18973, box_loss: 0.00139, time: 171.14449\n","Val 0000000067_0000000041.csv Epoch: 25, summary loss: 0.72469, class loss: 0.38198, box_loss: 0.00685, time: 171.14650\n","Val 0000000067_0000000045.csv Epoch: 25, summary loss: 0.38026, class loss: 0.30116, box_loss: 0.00158, time: 171.14852\n","Val 0000000067_0000000046.csv Epoch: 25, summary loss: 0.17758, class loss: 0.11745, box_loss: 0.00120, time: 171.15055\n","Val 0000000067_0000000050.csv Epoch: 25, summary loss: 0.84873, class loss: 0.58043, box_loss: 0.00537, time: 171.15259\n","Val 0000000067_0000000052.csv Epoch: 25, summary loss: 0.50706, class loss: 0.37821, box_loss: 0.00258, time: 171.15464\n","Val 0000000067_0000000055.csv Epoch: 25, summary loss: 0.43470, class loss: 0.38554, box_loss: 0.00098, time: 171.15669\n","Val 0000000067_0000000058.csv Epoch: 25, summary loss: 0.62406, class loss: 0.38063, box_loss: 0.00487, time: 171.15873\n","Val 0000000067_0000000059.csv Epoch: 25, summary loss: 0.37996, class loss: 0.37996, box_loss: 0.00000, time: 171.16075\n","Val 0000000351_0000000000.csv Epoch: 25, summary loss: 0.75016, class loss: 0.47868, box_loss: 0.00543, time: 171.18179\n","Val 0000000354_0000000000.csv Epoch: 25, summary loss: 1.50014, class loss: 1.06361, box_loss: 0.00873, time: 171.18386\n","Val 0000000359_0000000000.csv Epoch: 25, summary loss: 0.82411, class loss: 0.44285, box_loss: 0.00763, time: 171.18586\n","Val 0000000363_0000000000.csv Epoch: 25, summary loss: 0.27910, class loss: 0.27910, box_loss: 0.00000, time: 171.18786\n","Val 0000000364_0000000000.csv Epoch: 25, summary loss: 0.83054, class loss: 0.81042, box_loss: 0.00040, time: 171.18986\n","Val 0000000367_0000000000.csv Epoch: 25, summary loss: 0.78390, class loss: 0.54918, box_loss: 0.00469, time: 171.19183\n","Val Epoch: 25, summary loss: 0.45725, class loss: 0.31766, box_loss: 0.00279, time: 171.19397\n","Adjusting learning rate of group 0 to 1.3377e-04.\n","\n","2021-06-01T12:09:41.301860\n","LR: 0.00013377071367461527\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.70170\n","Train Step 250/3810, summary_loss: 0.36380, class_loss: 0.22303, box_loss: 0.00282, time: 75.04190\n","Train Step 500/3810, summary_loss: 0.35971, class_loss: 0.21892, box_loss: 0.00282, time: 148.89900\n","Train Step 750/3810, summary_loss: 0.35730, class_loss: 0.21846, box_loss: 0.00278, time: 222.80129\n","Train Step 1000/3810, summary_loss: 0.35547, class_loss: 0.21785, box_loss: 0.00275, time: 296.71037\n","Train Step 1250/3810, summary_loss: 0.35364, class_loss: 0.21756, box_loss: 0.00272, time: 370.30524\n","Train Step 1500/3810, summary_loss: 0.35493, class_loss: 0.21819, box_loss: 0.00273, time: 443.91851\n","Train Step 1750/3810, summary_loss: 0.35532, class_loss: 0.21799, box_loss: 0.00275, time: 517.44083\n","Train Step 2000/3810, summary_loss: 0.35381, class_loss: 0.21692, box_loss: 0.00274, time: 591.01846\n","Train Step 2250/3810, summary_loss: 0.35320, class_loss: 0.21651, box_loss: 0.00273, time: 664.89773\n","Train Step 2500/3810, summary_loss: 0.35351, class_loss: 0.21707, box_loss: 0.00273, time: 738.54963\n","Train Step 2750/3810, summary_loss: 0.35305, class_loss: 0.21707, box_loss: 0.00272, time: 812.28066\n","Train Step 3000/3810, summary_loss: 0.35302, class_loss: 0.21711, box_loss: 0.00272, time: 885.41122\n","Train Step 3250/3810, summary_loss: 0.35457, class_loss: 0.21809, box_loss: 0.00273, time: 959.52462\n","Train Step 3500/3810, summary_loss: 0.35536, class_loss: 0.21847, box_loss: 0.00274, time: 1033.34234\n","Train Step 3750/3810, summary_loss: 0.35533, class_loss: 0.21879, box_loss: 0.00273, time: 1107.83218\n","Train Epoch: 26, summary loss: 0.35528, class loss: 0.21885, box_loss: 0.00273, time: 1125.34662\n","Val 0000000010_0000000000.csv Epoch: 26, summary loss: 0.74501, class loss: 0.56169, box_loss: 0.00367, time: 171.84175\n","Val 0000000054_0000000000.csv Epoch: 26, summary loss: 0.57028, class loss: 0.45991, box_loss: 0.00221, time: 171.84461\n","Val 0000000056_0000000000.csv Epoch: 26, summary loss: 0.58461, class loss: 0.49461, box_loss: 0.00180, time: 171.84707\n","Val 0000000062_0000000000.csv Epoch: 26, summary loss: 1.02011, class loss: 0.71334, box_loss: 0.00614, time: 171.84920\n","Val 0000000067_0000000005.csv Epoch: 26, summary loss: 0.02322, class loss: 0.02322, box_loss: 0.00000, time: 171.85129\n","Val 0000000067_0000000012.csv Epoch: 26, summary loss: 0.14039, class loss: 0.14039, box_loss: 0.00000, time: 171.85334\n","Val 0000000067_0000000014.csv Epoch: 26, summary loss: 0.21216, class loss: 0.12891, box_loss: 0.00167, time: 171.85544\n","Val 0000000067_0000000015.csv Epoch: 26, summary loss: 0.44098, class loss: 0.23903, box_loss: 0.00404, time: 171.85750\n","Val 0000000067_0000000019.csv Epoch: 26, summary loss: 0.41129, class loss: 0.21695, box_loss: 0.00389, time: 171.87214\n","Val 0000000067_0000000024.csv Epoch: 26, summary loss: 0.31489, class loss: 0.16423, box_loss: 0.00301, time: 171.87645\n","Val 0000000067_0000000025.csv Epoch: 26, summary loss: 0.32739, class loss: 0.20589, box_loss: 0.00243, time: 171.87853\n","Val 0000000067_0000000026.csv Epoch: 26, summary loss: 0.05847, class loss: 0.05847, box_loss: 0.00000, time: 171.88283\n","Val 0000000067_0000000027.csv Epoch: 26, summary loss: 0.08052, class loss: 0.08052, box_loss: 0.00000, time: 171.88494\n","Val 0000000067_0000000028.csv Epoch: 26, summary loss: 0.11631, class loss: 0.09632, box_loss: 0.00040, time: 171.88702\n","Val 0000000067_0000000029.csv Epoch: 26, summary loss: 0.06556, class loss: 0.06556, box_loss: 0.00000, time: 171.88906\n","Val 0000000067_0000000031.csv Epoch: 26, summary loss: 0.35393, class loss: 0.20170, box_loss: 0.00304, time: 171.89113\n","Val 0000000067_0000000032.csv Epoch: 26, summary loss: 0.37338, class loss: 0.26446, box_loss: 0.00218, time: 171.89328\n","Val 0000000067_0000000040.csv Epoch: 26, summary loss: 0.28474, class loss: 0.20646, box_loss: 0.00157, time: 171.89540\n","Val 0000000067_0000000041.csv Epoch: 26, summary loss: 0.74301, class loss: 0.41176, box_loss: 0.00662, time: 171.89746\n","Val 0000000067_0000000045.csv Epoch: 26, summary loss: 0.34742, class loss: 0.26369, box_loss: 0.00167, time: 171.89950\n","Val 0000000067_0000000046.csv Epoch: 26, summary loss: 0.18825, class loss: 0.12645, box_loss: 0.00124, time: 171.90161\n","Val 0000000067_0000000050.csv Epoch: 26, summary loss: 0.82382, class loss: 0.53242, box_loss: 0.00583, time: 171.90363\n","Val 0000000067_0000000052.csv Epoch: 26, summary loss: 0.60490, class loss: 0.47801, box_loss: 0.00254, time: 171.90569\n","Val 0000000067_0000000055.csv Epoch: 26, summary loss: 0.34262, class loss: 0.30736, box_loss: 0.00071, time: 171.90773\n","Val 0000000067_0000000058.csv Epoch: 26, summary loss: 0.53417, class loss: 0.34957, box_loss: 0.00369, time: 171.90975\n","Val 0000000067_0000000059.csv Epoch: 26, summary loss: 0.35096, class loss: 0.35096, box_loss: 0.00000, time: 171.91182\n","Val 0000000351_0000000000.csv Epoch: 26, summary loss: 0.74417, class loss: 0.51383, box_loss: 0.00461, time: 171.91387\n","Val 0000000354_0000000000.csv Epoch: 26, summary loss: 3.23587, class loss: 2.94215, box_loss: 0.00587, time: 171.91599\n","Val 0000000359_0000000000.csv Epoch: 26, summary loss: 0.92170, class loss: 0.42442, box_loss: 0.00995, time: 171.91804\n","Val 0000000363_0000000000.csv Epoch: 26, summary loss: 0.71656, class loss: 0.71656, box_loss: 0.00000, time: 171.92008\n","Val 0000000364_0000000000.csv Epoch: 26, summary loss: 0.57090, class loss: 0.54887, box_loss: 0.00044, time: 171.92212\n","Val 0000000367_0000000000.csv Epoch: 26, summary loss: 0.96747, class loss: 0.73230, box_loss: 0.00470, time: 171.92416\n","Val Epoch: 26, summary loss: 0.53797, class loss: 0.40687, box_loss: 0.00262, time: 171.92635\n","Adjusting learning rate of group 0 to 1.3172e-04.\n","\n","2021-06-01T12:31:18.868032\n","LR: 0.00013171735692546138\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.86553\n","Train Step 250/3810, summary_loss: 0.36608, class_loss: 0.22450, box_loss: 0.00283, time: 75.09292\n","Train Step 500/3810, summary_loss: 0.35807, class_loss: 0.21994, box_loss: 0.00276, time: 149.47330\n","Train Step 750/3810, summary_loss: 0.35898, class_loss: 0.22027, box_loss: 0.00277, time: 223.30188\n","Train Step 1000/3810, summary_loss: 0.35252, class_loss: 0.21669, box_loss: 0.00272, time: 297.20857\n","Train Step 1250/3810, summary_loss: 0.35074, class_loss: 0.21584, box_loss: 0.00270, time: 371.06721\n","Train Step 1500/3810, summary_loss: 0.35188, class_loss: 0.21673, box_loss: 0.00270, time: 445.04699\n","Train Step 1750/3810, summary_loss: 0.34986, class_loss: 0.21580, box_loss: 0.00268, time: 518.45725\n","Train Step 2000/3810, summary_loss: 0.34983, class_loss: 0.21587, box_loss: 0.00268, time: 592.65641\n","Train Step 2250/3810, summary_loss: 0.35073, class_loss: 0.21611, box_loss: 0.00269, time: 667.09801\n","Train Step 2500/3810, summary_loss: 0.35246, class_loss: 0.21752, box_loss: 0.00270, time: 740.91851\n","Train Step 2750/3810, summary_loss: 0.35296, class_loss: 0.21787, box_loss: 0.00270, time: 815.19287\n","Train Step 3000/3810, summary_loss: 0.35373, class_loss: 0.21827, box_loss: 0.00271, time: 888.78438\n","Train Step 3250/3810, summary_loss: 0.35423, class_loss: 0.21858, box_loss: 0.00271, time: 963.22927\n","Train Step 3500/3810, summary_loss: 0.35439, class_loss: 0.21853, box_loss: 0.00272, time: 1037.00386\n","Train Step 3750/3810, summary_loss: 0.35524, class_loss: 0.21912, box_loss: 0.00272, time: 1111.23401\n","Train Epoch: 27, summary loss: 0.35505, class loss: 0.21902, box_loss: 0.00272, time: 1128.70994\n","Val 0000000010_0000000000.csv Epoch: 27, summary loss: 2.45885, class loss: 1.90028, box_loss: 0.01117, time: 171.69520\n","Val 0000000054_0000000000.csv Epoch: 27, summary loss: 0.64232, class loss: 0.49076, box_loss: 0.00303, time: 171.69799\n","Val 0000000056_0000000000.csv Epoch: 27, summary loss: 0.28384, class loss: 0.21100, box_loss: 0.00146, time: 171.70045\n","Val 0000000062_0000000000.csv Epoch: 27, summary loss: 0.86064, class loss: 0.59686, box_loss: 0.00528, time: 171.70262\n","Val 0000000067_0000000005.csv Epoch: 27, summary loss: 0.02326, class loss: 0.02326, box_loss: 0.00000, time: 171.70472\n","Val 0000000067_0000000012.csv Epoch: 27, summary loss: 0.14397, class loss: 0.14397, box_loss: 0.00000, time: 171.70680\n","Val 0000000067_0000000014.csv Epoch: 27, summary loss: 0.20954, class loss: 0.13311, box_loss: 0.00153, time: 171.70888\n","Val 0000000067_0000000015.csv Epoch: 27, summary loss: 0.44618, class loss: 0.25066, box_loss: 0.00391, time: 171.71098\n","Val 0000000067_0000000019.csv Epoch: 27, summary loss: 0.38033, class loss: 0.20178, box_loss: 0.00357, time: 171.71297\n","Val 0000000067_0000000024.csv Epoch: 27, summary loss: 0.31864, class loss: 0.14505, box_loss: 0.00347, time: 171.71503\n","Val 0000000067_0000000025.csv Epoch: 27, summary loss: 0.34485, class loss: 0.20841, box_loss: 0.00273, time: 171.71711\n","Val 0000000067_0000000026.csv Epoch: 27, summary loss: 0.04769, class loss: 0.04769, box_loss: 0.00000, time: 171.71916\n","Val 0000000067_0000000027.csv Epoch: 27, summary loss: 0.12064, class loss: 0.12064, box_loss: 0.00000, time: 171.72123\n","Val 0000000067_0000000028.csv Epoch: 27, summary loss: 0.08676, class loss: 0.06565, box_loss: 0.00042, time: 171.72337\n","Val 0000000067_0000000029.csv Epoch: 27, summary loss: 0.07107, class loss: 0.07107, box_loss: 0.00000, time: 171.72545\n","Val 0000000067_0000000031.csv Epoch: 27, summary loss: 0.33423, class loss: 0.18697, box_loss: 0.00295, time: 171.72749\n","Val 0000000067_0000000032.csv Epoch: 27, summary loss: 0.39991, class loss: 0.28892, box_loss: 0.00222, time: 171.72962\n","Val 0000000067_0000000040.csv Epoch: 27, summary loss: 0.27525, class loss: 0.21773, box_loss: 0.00115, time: 171.73170\n","Val 0000000067_0000000041.csv Epoch: 27, summary loss: 0.69055, class loss: 0.37606, box_loss: 0.00629, time: 171.73386\n","Val 0000000067_0000000045.csv Epoch: 27, summary loss: 0.34305, class loss: 0.26882, box_loss: 0.00148, time: 171.73604\n","Val 0000000067_0000000046.csv Epoch: 27, summary loss: 0.17646, class loss: 0.11723, box_loss: 0.00118, time: 171.73813\n","Val 0000000067_0000000050.csv Epoch: 27, summary loss: 0.93375, class loss: 0.63732, box_loss: 0.00593, time: 171.74020\n","Val 0000000067_0000000052.csv Epoch: 27, summary loss: 0.44970, class loss: 0.33761, box_loss: 0.00224, time: 171.75939\n","Val 0000000067_0000000055.csv Epoch: 27, summary loss: 0.37946, class loss: 0.33992, box_loss: 0.00079, time: 171.76306\n","Val 0000000067_0000000058.csv Epoch: 27, summary loss: 0.62429, class loss: 0.38378, box_loss: 0.00481, time: 171.76519\n","Val 0000000067_0000000059.csv Epoch: 27, summary loss: 0.39110, class loss: 0.39110, box_loss: 0.00000, time: 171.76733\n","Val 0000000351_0000000000.csv Epoch: 27, summary loss: 0.61576, class loss: 0.37094, box_loss: 0.00490, time: 171.76946\n","Val 0000000354_0000000000.csv Epoch: 27, summary loss: 0.80155, class loss: 0.53879, box_loss: 0.00526, time: 171.77158\n","Val 0000000359_0000000000.csv Epoch: 27, summary loss: 0.76254, class loss: 0.40931, box_loss: 0.00706, time: 171.77369\n","Val 0000000363_0000000000.csv Epoch: 27, summary loss: 0.46645, class loss: 0.46645, box_loss: 0.00000, time: 171.77585\n","Val 0000000364_0000000000.csv Epoch: 27, summary loss: 1.10330, class loss: 1.08304, box_loss: 0.00041, time: 171.77799\n","Val 0000000367_0000000000.csv Epoch: 27, summary loss: 1.11040, class loss: 0.83053, box_loss: 0.00560, time: 171.78009\n","Val Epoch: 27, summary loss: 0.50926, class loss: 0.37046, box_loss: 0.00278, time: 171.78233\n","Adjusting learning rate of group 0 to 1.2970e-04.\n","\n","2021-06-01T12:52:59.654202\n","LR: 0.0001296955188385279\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.86857\n","Train Step 250/3810, summary_loss: 0.38532, class_loss: 0.24254, box_loss: 0.00286, time: 75.48475\n","Train Step 500/3810, summary_loss: 0.36682, class_loss: 0.22888, box_loss: 0.00276, time: 149.12794\n","Train Step 750/3810, summary_loss: 0.36056, class_loss: 0.22365, box_loss: 0.00274, time: 223.07148\n","Train Step 1000/3810, summary_loss: 0.35676, class_loss: 0.22085, box_loss: 0.00272, time: 296.79819\n","Train Step 1250/3810, summary_loss: 0.35572, class_loss: 0.21938, box_loss: 0.00273, time: 369.88693\n","Train Step 1500/3810, summary_loss: 0.35426, class_loss: 0.21896, box_loss: 0.00271, time: 443.64629\n","Train Step 1750/3810, summary_loss: 0.35640, class_loss: 0.21997, box_loss: 0.00273, time: 517.44136\n","Train Step 2000/3810, summary_loss: 0.35632, class_loss: 0.21989, box_loss: 0.00273, time: 591.16201\n","Train Step 2250/3810, summary_loss: 0.35594, class_loss: 0.21984, box_loss: 0.00272, time: 664.84622\n","Train Step 2500/3810, summary_loss: 0.35565, class_loss: 0.21942, box_loss: 0.00272, time: 738.91840\n","Train Step 2750/3810, summary_loss: 0.35553, class_loss: 0.22014, box_loss: 0.00271, time: 812.58436\n","Train Step 3000/3810, summary_loss: 0.35800, class_loss: 0.22219, box_loss: 0.00272, time: 885.76312\n","Train Step 3250/3810, summary_loss: 0.35719, class_loss: 0.22164, box_loss: 0.00271, time: 959.17155\n","Train Step 3500/3810, summary_loss: 0.35722, class_loss: 0.22159, box_loss: 0.00271, time: 1032.67611\n","Train Step 3750/3810, summary_loss: 0.35727, class_loss: 0.22146, box_loss: 0.00272, time: 1106.74428\n","Train Epoch: 28, summary loss: 0.35696, class loss: 0.22128, box_loss: 0.00271, time: 1124.40062\n","Val 0000000010_0000000000.csv Epoch: 28, summary loss: 0.75100, class loss: 0.54055, box_loss: 0.00421, time: 171.10111\n","Val 0000000054_0000000000.csv Epoch: 28, summary loss: 0.65610, class loss: 0.49813, box_loss: 0.00316, time: 171.10410\n","Val 0000000056_0000000000.csv Epoch: 28, summary loss: 0.32845, class loss: 0.24889, box_loss: 0.00159, time: 171.10637\n","Val 0000000062_0000000000.csv Epoch: 28, summary loss: 0.89562, class loss: 0.66262, box_loss: 0.00466, time: 171.10827\n","Val 0000000067_0000000005.csv Epoch: 28, summary loss: 0.01084, class loss: 0.01084, box_loss: 0.00000, time: 171.11039\n","Val 0000000067_0000000012.csv Epoch: 28, summary loss: 0.13050, class loss: 0.13050, box_loss: 0.00000, time: 171.11251\n","Val 0000000067_0000000014.csv Epoch: 28, summary loss: 0.19874, class loss: 0.12234, box_loss: 0.00153, time: 171.12869\n","Val 0000000067_0000000015.csv Epoch: 28, summary loss: 0.44393, class loss: 0.21700, box_loss: 0.00454, time: 171.13218\n","Val 0000000067_0000000019.csv Epoch: 28, summary loss: 0.36496, class loss: 0.18093, box_loss: 0.00368, time: 171.13427\n","Val 0000000067_0000000024.csv Epoch: 28, summary loss: 0.32962, class loss: 0.15630, box_loss: 0.00347, time: 171.13642\n","Val 0000000067_0000000025.csv Epoch: 28, summary loss: 0.30771, class loss: 0.19052, box_loss: 0.00234, time: 171.13854\n","Val 0000000067_0000000026.csv Epoch: 28, summary loss: 0.02633, class loss: 0.02633, box_loss: 0.00000, time: 171.14062\n","Val 0000000067_0000000027.csv Epoch: 28, summary loss: 0.05472, class loss: 0.05472, box_loss: 0.00000, time: 171.14287\n","Val 0000000067_0000000028.csv Epoch: 28, summary loss: 0.08680, class loss: 0.06072, box_loss: 0.00052, time: 171.14494\n","Val 0000000067_0000000029.csv Epoch: 28, summary loss: 0.04175, class loss: 0.04175, box_loss: 0.00000, time: 171.14698\n","Val 0000000067_0000000031.csv Epoch: 28, summary loss: 0.35590, class loss: 0.20408, box_loss: 0.00304, time: 171.14902\n","Val 0000000067_0000000032.csv Epoch: 28, summary loss: 0.39143, class loss: 0.26470, box_loss: 0.00253, time: 171.15108\n","Val 0000000067_0000000040.csv Epoch: 28, summary loss: 0.25127, class loss: 0.18314, box_loss: 0.00136, time: 171.15316\n","Val 0000000067_0000000041.csv Epoch: 28, summary loss: 0.70999, class loss: 0.39484, box_loss: 0.00630, time: 171.15520\n","Val 0000000067_0000000045.csv Epoch: 28, summary loss: 0.29125, class loss: 0.21965, box_loss: 0.00143, time: 171.15732\n","Val 0000000067_0000000046.csv Epoch: 28, summary loss: 0.17908, class loss: 0.11426, box_loss: 0.00130, time: 171.15942\n","Val 0000000067_0000000050.csv Epoch: 28, summary loss: 0.91551, class loss: 0.58925, box_loss: 0.00653, time: 171.16160\n","Val 0000000067_0000000052.csv Epoch: 28, summary loss: 0.58457, class loss: 0.40091, box_loss: 0.00367, time: 171.16369\n","Val 0000000067_0000000055.csv Epoch: 28, summary loss: 0.32880, class loss: 0.29023, box_loss: 0.00077, time: 171.16572\n","Val 0000000067_0000000058.csv Epoch: 28, summary loss: 0.54055, class loss: 0.33198, box_loss: 0.00417, time: 171.16783\n","Val 0000000067_0000000059.csv Epoch: 28, summary loss: 0.40737, class loss: 0.40737, box_loss: 0.00000, time: 171.16993\n","Val 0000000351_0000000000.csv Epoch: 28, summary loss: 0.64665, class loss: 0.42224, box_loss: 0.00449, time: 171.17201\n","Val 0000000354_0000000000.csv Epoch: 28, summary loss: 1.45614, class loss: 1.12926, box_loss: 0.00654, time: 171.17407\n","Val 0000000359_0000000000.csv Epoch: 28, summary loss: 0.77695, class loss: 0.41263, box_loss: 0.00729, time: 171.17615\n","Val 0000000363_0000000000.csv Epoch: 28, summary loss: 0.18128, class loss: 0.18128, box_loss: 0.00000, time: 171.17825\n","Val 0000000364_0000000000.csv Epoch: 28, summary loss: 0.49303, class loss: 0.47162, box_loss: 0.00043, time: 171.18034\n","Val 0000000367_0000000000.csv Epoch: 28, summary loss: 0.86469, class loss: 0.60260, box_loss: 0.00524, time: 171.18243\n","Val Epoch: 28, summary loss: 0.43755, class loss: 0.30507, box_loss: 0.00265, time: 171.18472\n","Adjusting learning rate of group 0 to 1.2770e-04.\n","\n","2021-06-01T13:14:35.825908\n","LR: 0.00012770471560793524\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.92196\n","Train Step 250/3810, summary_loss: 0.34797, class_loss: 0.21606, box_loss: 0.00264, time: 76.10099\n","Train Step 500/3810, summary_loss: 0.34041, class_loss: 0.21162, box_loss: 0.00258, time: 149.72757\n","Train Step 750/3810, summary_loss: 0.33372, class_loss: 0.20779, box_loss: 0.00252, time: 223.52900\n","Train Step 1000/3810, summary_loss: 0.33185, class_loss: 0.20583, box_loss: 0.00252, time: 297.18179\n","Train Step 1250/3810, summary_loss: 0.33467, class_loss: 0.20767, box_loss: 0.00254, time: 371.69519\n","Train Step 1500/3810, summary_loss: 0.33755, class_loss: 0.20891, box_loss: 0.00257, time: 446.19819\n","Train Step 1750/3810, summary_loss: 0.33906, class_loss: 0.20954, box_loss: 0.00259, time: 519.64805\n","Train Step 2000/3810, summary_loss: 0.34061, class_loss: 0.21034, box_loss: 0.00261, time: 593.43383\n","Train Step 2250/3810, summary_loss: 0.33947, class_loss: 0.20976, box_loss: 0.00259, time: 667.28545\n","Train Step 2500/3810, summary_loss: 0.34071, class_loss: 0.21087, box_loss: 0.00260, time: 741.37020\n","Train Step 2750/3810, summary_loss: 0.34243, class_loss: 0.21173, box_loss: 0.00261, time: 815.49930\n","Train Step 3000/3810, summary_loss: 0.34327, class_loss: 0.21225, box_loss: 0.00262, time: 889.31238\n","Train Step 3250/3810, summary_loss: 0.34484, class_loss: 0.21353, box_loss: 0.00263, time: 962.98428\n","Train Step 3500/3810, summary_loss: 0.34532, class_loss: 0.21378, box_loss: 0.00263, time: 1036.77947\n","Train Step 3750/3810, summary_loss: 0.34637, class_loss: 0.21403, box_loss: 0.00265, time: 1110.88040\n","Train Epoch: 29, summary loss: 0.34658, class loss: 0.21409, box_loss: 0.00265, time: 1128.54001\n","Val 0000000010_0000000000.csv Epoch: 29, summary loss: 0.94079, class loss: 0.65750, box_loss: 0.00567, time: 172.22867\n","Val 0000000054_0000000000.csv Epoch: 29, summary loss: 0.47161, class loss: 0.35565, box_loss: 0.00232, time: 172.23134\n","Val 0000000056_0000000000.csv Epoch: 29, summary loss: 0.30142, class loss: 0.22570, box_loss: 0.00151, time: 172.23390\n","Val 0000000062_0000000000.csv Epoch: 29, summary loss: 0.87665, class loss: 0.63290, box_loss: 0.00488, time: 172.23580\n","Val 0000000067_0000000005.csv Epoch: 29, summary loss: 0.02125, class loss: 0.02125, box_loss: 0.00000, time: 172.23770\n","Val 0000000067_0000000012.csv Epoch: 29, summary loss: 0.12868, class loss: 0.12868, box_loss: 0.00000, time: 172.23987\n","Val 0000000067_0000000014.csv Epoch: 29, summary loss: 0.23241, class loss: 0.11820, box_loss: 0.00228, time: 172.24211\n","Val 0000000067_0000000015.csv Epoch: 29, summary loss: 0.57290, class loss: 0.22102, box_loss: 0.00704, time: 172.24427\n","Val 0000000067_0000000019.csv Epoch: 29, summary loss: 0.40742, class loss: 0.18495, box_loss: 0.00445, time: 172.24642\n","Val 0000000067_0000000024.csv Epoch: 29, summary loss: 0.36152, class loss: 0.16509, box_loss: 0.00393, time: 172.24880\n","Val 0000000067_0000000025.csv Epoch: 29, summary loss: 0.38319, class loss: 0.18263, box_loss: 0.00401, time: 172.25099\n","Val 0000000067_0000000026.csv Epoch: 29, summary loss: 0.07968, class loss: 0.07968, box_loss: 0.00000, time: 172.25315\n","Val 0000000067_0000000027.csv Epoch: 29, summary loss: 0.04890, class loss: 0.04890, box_loss: 0.00000, time: 172.25527\n","Val 0000000067_0000000028.csv Epoch: 29, summary loss: 0.11372, class loss: 0.07959, box_loss: 0.00068, time: 172.25743\n","Val 0000000067_0000000029.csv Epoch: 29, summary loss: 0.06320, class loss: 0.06320, box_loss: 0.00000, time: 172.25959\n","Val 0000000067_0000000031.csv Epoch: 29, summary loss: 0.40716, class loss: 0.21774, box_loss: 0.00379, time: 172.26180\n","Val 0000000067_0000000032.csv Epoch: 29, summary loss: 0.44666, class loss: 0.28989, box_loss: 0.00314, time: 172.26395\n","Val 0000000067_0000000040.csv Epoch: 29, summary loss: 0.25772, class loss: 0.19577, box_loss: 0.00124, time: 172.28282\n","Val 0000000067_0000000041.csv Epoch: 29, summary loss: 0.65723, class loss: 0.36050, box_loss: 0.00593, time: 172.28501\n","Val 0000000067_0000000045.csv Epoch: 29, summary loss: 0.41645, class loss: 0.31272, box_loss: 0.00207, time: 172.28713\n","Val 0000000067_0000000046.csv Epoch: 29, summary loss: 0.17637, class loss: 0.11195, box_loss: 0.00129, time: 172.28925\n","Val 0000000067_0000000050.csv Epoch: 29, summary loss: 0.89452, class loss: 0.61630, box_loss: 0.00556, time: 172.29138\n","Val 0000000067_0000000052.csv Epoch: 29, summary loss: 0.52985, class loss: 0.38820, box_loss: 0.00283, time: 172.29350\n","Val 0000000067_0000000055.csv Epoch: 29, summary loss: 0.38157, class loss: 0.32869, box_loss: 0.00106, time: 172.29559\n","Val 0000000067_0000000058.csv Epoch: 29, summary loss: 0.84940, class loss: 0.53056, box_loss: 0.00638, time: 172.29771\n","Val 0000000067_0000000059.csv Epoch: 29, summary loss: 0.42603, class loss: 0.42603, box_loss: 0.00000, time: 172.29984\n","Val 0000000351_0000000000.csv Epoch: 29, summary loss: 0.55555, class loss: 0.33743, box_loss: 0.00436, time: 172.30193\n","Val 0000000354_0000000000.csv Epoch: 29, summary loss: 1.04505, class loss: 0.67618, box_loss: 0.00738, time: 172.30402\n","Val 0000000359_0000000000.csv Epoch: 29, summary loss: 0.80253, class loss: 0.46509, box_loss: 0.00675, time: 172.30613\n","Val 0000000363_0000000000.csv Epoch: 29, summary loss: 0.47118, class loss: 0.47118, box_loss: 0.00000, time: 172.30825\n","Val 0000000364_0000000000.csv Epoch: 29, summary loss: 1.49350, class loss: 1.46967, box_loss: 0.00048, time: 172.31034\n","Val 0000000367_0000000000.csv Epoch: 29, summary loss: 0.82450, class loss: 0.55245, box_loss: 0.00544, time: 172.31242\n","Val Epoch: 29, summary loss: 0.48871, class loss: 0.34110, box_loss: 0.00295, time: 172.31469\n","Adjusting learning rate of group 0 to 1.2574e-04.\n","\n","2021-06-01T13:36:16.982215\n","LR: 0.00012574447085413832\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.81577\n","Train Step 250/3810, summary_loss: 0.35880, class_loss: 0.21498, box_loss: 0.00288, time: 75.04284\n","Train Step 500/3810, summary_loss: 0.35416, class_loss: 0.21582, box_loss: 0.00277, time: 148.81719\n","Train Step 750/3810, summary_loss: 0.34955, class_loss: 0.21445, box_loss: 0.00270, time: 222.53381\n","Train Step 1000/3810, summary_loss: 0.34665, class_loss: 0.21368, box_loss: 0.00266, time: 296.75428\n","Train Step 1250/3810, summary_loss: 0.34513, class_loss: 0.21315, box_loss: 0.00264, time: 371.00453\n","Train Step 1500/3810, summary_loss: 0.34609, class_loss: 0.21407, box_loss: 0.00264, time: 444.31650\n","Train Step 1750/3810, summary_loss: 0.34697, class_loss: 0.21488, box_loss: 0.00264, time: 518.11171\n","Train Step 2000/3810, summary_loss: 0.34799, class_loss: 0.21530, box_loss: 0.00265, time: 592.31032\n","Train Step 2250/3810, summary_loss: 0.34765, class_loss: 0.21525, box_loss: 0.00265, time: 666.31896\n","Train Step 2500/3810, summary_loss: 0.34698, class_loss: 0.21502, box_loss: 0.00264, time: 739.77571\n","Train Step 2750/3810, summary_loss: 0.34742, class_loss: 0.21508, box_loss: 0.00265, time: 813.61949\n","Train Step 3000/3810, summary_loss: 0.34796, class_loss: 0.21527, box_loss: 0.00265, time: 887.10859\n","Train Step 3250/3810, summary_loss: 0.34942, class_loss: 0.21614, box_loss: 0.00267, time: 960.58929\n","Train Step 3500/3810, summary_loss: 0.34986, class_loss: 0.21632, box_loss: 0.00267, time: 1034.18015\n","Train Step 3750/3810, summary_loss: 0.34989, class_loss: 0.21659, box_loss: 0.00267, time: 1107.48669\n","Train Epoch: 30, summary loss: 0.34979, class loss: 0.21653, box_loss: 0.00267, time: 1125.03454\n","Val 0000000010_0000000000.csv Epoch: 30, summary loss: 0.71683, class loss: 0.49212, box_loss: 0.00449, time: 170.52504\n","Val 0000000054_0000000000.csv Epoch: 30, summary loss: 0.42145, class loss: 0.34730, box_loss: 0.00148, time: 170.52767\n","Val 0000000056_0000000000.csv Epoch: 30, summary loss: 0.34532, class loss: 0.25038, box_loss: 0.00190, time: 170.53003\n","Val 0000000062_0000000000.csv Epoch: 30, summary loss: 1.00195, class loss: 0.72649, box_loss: 0.00551, time: 170.53220\n","Val 0000000067_0000000005.csv Epoch: 30, summary loss: 0.02602, class loss: 0.02602, box_loss: 0.00000, time: 170.53433\n","Val 0000000067_0000000012.csv Epoch: 30, summary loss: 0.15134, class loss: 0.15134, box_loss: 0.00000, time: 170.53645\n","Val 0000000067_0000000014.csv Epoch: 30, summary loss: 0.19916, class loss: 0.13259, box_loss: 0.00133, time: 170.53851\n","Val 0000000067_0000000015.csv Epoch: 30, summary loss: 0.44972, class loss: 0.23847, box_loss: 0.00422, time: 170.54057\n","Val 0000000067_0000000019.csv Epoch: 30, summary loss: 0.38810, class loss: 0.20037, box_loss: 0.00375, time: 170.54268\n","Val 0000000067_0000000024.csv Epoch: 30, summary loss: 0.30226, class loss: 0.13846, box_loss: 0.00328, time: 170.54675\n","Val 0000000067_0000000025.csv Epoch: 30, summary loss: 0.33445, class loss: 0.18654, box_loss: 0.00296, time: 170.54883\n","Val 0000000067_0000000026.csv Epoch: 30, summary loss: 0.01884, class loss: 0.01884, box_loss: 0.00000, time: 170.55088\n","Val 0000000067_0000000027.csv Epoch: 30, summary loss: 0.06518, class loss: 0.06518, box_loss: 0.00000, time: 170.55294\n","Val 0000000067_0000000028.csv Epoch: 30, summary loss: 0.07473, class loss: 0.04805, box_loss: 0.00053, time: 170.55502\n","Val 0000000067_0000000029.csv Epoch: 30, summary loss: 0.04016, class loss: 0.04016, box_loss: 0.00000, time: 170.55710\n","Val 0000000067_0000000031.csv Epoch: 30, summary loss: 0.35321, class loss: 0.19461, box_loss: 0.00317, time: 170.55917\n","Val 0000000067_0000000032.csv Epoch: 30, summary loss: 0.37184, class loss: 0.26288, box_loss: 0.00218, time: 170.56129\n","Val 0000000067_0000000040.csv Epoch: 30, summary loss: 0.23052, class loss: 0.16677, box_loss: 0.00127, time: 170.56338\n","Val 0000000067_0000000041.csv Epoch: 30, summary loss: 0.67557, class loss: 0.36868, box_loss: 0.00614, time: 170.56546\n","Val 0000000067_0000000045.csv Epoch: 30, summary loss: 0.30191, class loss: 0.22162, box_loss: 0.00161, time: 170.56752\n","Val 0000000067_0000000046.csv Epoch: 30, summary loss: 0.16413, class loss: 0.10297, box_loss: 0.00122, time: 170.56956\n","Val 0000000067_0000000050.csv Epoch: 30, summary loss: 0.86328, class loss: 0.60997, box_loss: 0.00507, time: 170.57164\n","Val 0000000067_0000000052.csv Epoch: 30, summary loss: 0.51113, class loss: 0.40144, box_loss: 0.00219, time: 170.57377\n","Val 0000000067_0000000055.csv Epoch: 30, summary loss: 0.41982, class loss: 0.38925, box_loss: 0.00061, time: 170.57583\n","Val 0000000067_0000000058.csv Epoch: 30, summary loss: 0.56412, class loss: 0.32038, box_loss: 0.00487, time: 170.57788\n","Val 0000000067_0000000059.csv Epoch: 30, summary loss: 0.35707, class loss: 0.35707, box_loss: 0.00000, time: 170.57986\n","Val 0000000351_0000000000.csv Epoch: 30, summary loss: 0.60495, class loss: 0.37939, box_loss: 0.00451, time: 170.58191\n","Val 0000000354_0000000000.csv Epoch: 30, summary loss: 1.76828, class loss: 1.34551, box_loss: 0.00846, time: 170.60280\n","Val 0000000359_0000000000.csv Epoch: 30, summary loss: 0.87181, class loss: 0.44080, box_loss: 0.00862, time: 170.60489\n","Val 0000000363_0000000000.csv Epoch: 30, summary loss: 0.34262, class loss: 0.34262, box_loss: 0.00000, time: 170.60696\n","Val 0000000364_0000000000.csv Epoch: 30, summary loss: 0.77166, class loss: 0.75147, box_loss: 0.00040, time: 170.60901\n","Val 0000000367_0000000000.csv Epoch: 30, summary loss: 1.01785, class loss: 0.76658, box_loss: 0.00503, time: 170.61110\n","Val Epoch: 30, summary loss: 0.46016, class loss: 0.32764, box_loss: 0.00265, time: 170.61338\n","Adjusting learning rate of group 0 to 1.2381e-04.\n","\n","2021-06-01T13:57:52.962896\n","LR: 0.00012381431550993364\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.81067\n","Train Step 250/3810, summary_loss: 0.34712, class_loss: 0.21453, box_loss: 0.00265, time: 74.36507\n","Train Step 500/3810, summary_loss: 0.34945, class_loss: 0.21579, box_loss: 0.00267, time: 147.78293\n","Train Step 750/3810, summary_loss: 0.35064, class_loss: 0.21933, box_loss: 0.00263, time: 221.77723\n","Train Step 1000/3810, summary_loss: 0.35325, class_loss: 0.22180, box_loss: 0.00263, time: 295.90888\n","Train Step 1250/3810, summary_loss: 0.35333, class_loss: 0.22207, box_loss: 0.00263, time: 369.76160\n","Train Step 1500/3810, summary_loss: 0.35230, class_loss: 0.22076, box_loss: 0.00263, time: 444.24081\n","Train Step 1750/3810, summary_loss: 0.35056, class_loss: 0.21958, box_loss: 0.00262, time: 518.46861\n","Train Step 2000/3810, summary_loss: 0.34924, class_loss: 0.21861, box_loss: 0.00261, time: 592.08270\n","Train Step 2250/3810, summary_loss: 0.34743, class_loss: 0.21718, box_loss: 0.00260, time: 665.72610\n","Train Step 2500/3810, summary_loss: 0.34658, class_loss: 0.21654, box_loss: 0.00260, time: 739.45911\n","Train Step 2750/3810, summary_loss: 0.34622, class_loss: 0.21610, box_loss: 0.00260, time: 813.60614\n","Train Step 3000/3810, summary_loss: 0.34498, class_loss: 0.21515, box_loss: 0.00260, time: 887.68536\n","Train Step 3250/3810, summary_loss: 0.34461, class_loss: 0.21478, box_loss: 0.00260, time: 961.66582\n","Train Step 3500/3810, summary_loss: 0.34537, class_loss: 0.21526, box_loss: 0.00260, time: 1035.24850\n","Train Step 3750/3810, summary_loss: 0.34481, class_loss: 0.21490, box_loss: 0.00260, time: 1108.99786\n","Train Epoch: 31, summary loss: 0.34485, class loss: 0.21491, box_loss: 0.00260, time: 1126.58831\n","Val 0000000010_0000000000.csv Epoch: 31, summary loss: 0.59461, class loss: 0.40271, box_loss: 0.00384, time: 171.23593\n","Val 0000000054_0000000000.csv Epoch: 31, summary loss: 0.72175, class loss: 0.54223, box_loss: 0.00359, time: 171.23901\n","Val 0000000056_0000000000.csv Epoch: 31, summary loss: 0.37265, class loss: 0.28941, box_loss: 0.00166, time: 171.24179\n","Val 0000000062_0000000000.csv Epoch: 31, summary loss: 0.91848, class loss: 0.69304, box_loss: 0.00451, time: 171.25202\n","Val 0000000067_0000000005.csv Epoch: 31, summary loss: 0.02213, class loss: 0.02213, box_loss: 0.00000, time: 171.25587\n","Val 0000000067_0000000012.csv Epoch: 31, summary loss: 0.16692, class loss: 0.16692, box_loss: 0.00000, time: 171.25805\n","Val 0000000067_0000000014.csv Epoch: 31, summary loss: 0.23091, class loss: 0.13094, box_loss: 0.00200, time: 171.26022\n","Val 0000000067_0000000015.csv Epoch: 31, summary loss: 0.53034, class loss: 0.23037, box_loss: 0.00600, time: 171.26236\n","Val 0000000067_0000000019.csv Epoch: 31, summary loss: 0.43505, class loss: 0.22172, box_loss: 0.00427, time: 171.26448\n","Val 0000000067_0000000024.csv Epoch: 31, summary loss: 0.37523, class loss: 0.15950, box_loss: 0.00431, time: 171.26660\n","Val 0000000067_0000000025.csv Epoch: 31, summary loss: 0.37563, class loss: 0.20026, box_loss: 0.00351, time: 171.26871\n","Val 0000000067_0000000026.csv Epoch: 31, summary loss: 0.02045, class loss: 0.02045, box_loss: 0.00000, time: 171.27079\n","Val 0000000067_0000000027.csv Epoch: 31, summary loss: 0.03594, class loss: 0.03594, box_loss: 0.00000, time: 171.27291\n","Val 0000000067_0000000028.csv Epoch: 31, summary loss: 0.10295, class loss: 0.07286, box_loss: 0.00060, time: 171.27500\n","Val 0000000067_0000000029.csv Epoch: 31, summary loss: 0.04960, class loss: 0.04960, box_loss: 0.00000, time: 171.27712\n","Val 0000000067_0000000031.csv Epoch: 31, summary loss: 0.33559, class loss: 0.19058, box_loss: 0.00290, time: 171.27922\n","Val 0000000067_0000000032.csv Epoch: 31, summary loss: 0.46378, class loss: 0.31692, box_loss: 0.00294, time: 171.28129\n","Val 0000000067_0000000040.csv Epoch: 31, summary loss: 0.21743, class loss: 0.15822, box_loss: 0.00118, time: 171.28336\n","Val 0000000067_0000000041.csv Epoch: 31, summary loss: 0.71004, class loss: 0.37421, box_loss: 0.00672, time: 171.28549\n","Val 0000000067_0000000045.csv Epoch: 31, summary loss: 0.38461, class loss: 0.28868, box_loss: 0.00192, time: 171.28756\n","Val 0000000067_0000000046.csv Epoch: 31, summary loss: 0.15819, class loss: 0.10574, box_loss: 0.00105, time: 171.28961\n","Val 0000000067_0000000050.csv Epoch: 31, summary loss: 0.82172, class loss: 0.56239, box_loss: 0.00519, time: 171.29166\n","Val 0000000067_0000000052.csv Epoch: 31, summary loss: 0.59851, class loss: 0.44027, box_loss: 0.00316, time: 171.29374\n","Val 0000000067_0000000055.csv Epoch: 31, summary loss: 0.50084, class loss: 0.44137, box_loss: 0.00119, time: 171.29580\n","Val 0000000067_0000000058.csv Epoch: 31, summary loss: 0.62763, class loss: 0.37706, box_loss: 0.00501, time: 171.29788\n","Val 0000000067_0000000059.csv Epoch: 31, summary loss: 0.45396, class loss: 0.45396, box_loss: 0.00000, time: 171.29993\n","Val 0000000351_0000000000.csv Epoch: 31, summary loss: 0.76302, class loss: 0.53529, box_loss: 0.00455, time: 171.30199\n","Val 0000000354_0000000000.csv Epoch: 31, summary loss: 1.08062, class loss: 0.74089, box_loss: 0.00679, time: 171.30408\n","Val 0000000359_0000000000.csv Epoch: 31, summary loss: 0.84119, class loss: 0.49427, box_loss: 0.00694, time: 171.30616\n","Val 0000000363_0000000000.csv Epoch: 31, summary loss: 0.18646, class loss: 0.18646, box_loss: 0.00000, time: 171.30822\n","Val 0000000364_0000000000.csv Epoch: 31, summary loss: 0.23834, class loss: 0.19853, box_loss: 0.00080, time: 171.31031\n","Val 0000000367_0000000000.csv Epoch: 31, summary loss: 0.94227, class loss: 0.67218, box_loss: 0.00540, time: 171.31238\n","Val Epoch: 31, summary loss: 0.44615, class loss: 0.30547, box_loss: 0.00281, time: 171.31454\n","Adjusting learning rate of group 0 to 1.2191e-04.\n","\n","2021-06-01T14:19:31.161096\n","LR: 0.00012191378770821618\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.79465\n","Train Step 250/3810, summary_loss: 0.32248, class_loss: 0.20296, box_loss: 0.00239, time: 75.02749\n","Train Step 500/3810, summary_loss: 0.33360, class_loss: 0.20749, box_loss: 0.00252, time: 148.67759\n","Train Step 750/3810, summary_loss: 0.33585, class_loss: 0.20911, box_loss: 0.00253, time: 222.32809\n","Train Step 1000/3810, summary_loss: 0.33702, class_loss: 0.20973, box_loss: 0.00255, time: 296.17114\n","Train Step 1250/3810, summary_loss: 0.34079, class_loss: 0.21224, box_loss: 0.00257, time: 369.51572\n","Train Step 1500/3810, summary_loss: 0.34045, class_loss: 0.21195, box_loss: 0.00257, time: 443.31746\n","Train Step 1750/3810, summary_loss: 0.33889, class_loss: 0.21094, box_loss: 0.00256, time: 517.20707\n","Train Step 2000/3810, summary_loss: 0.33849, class_loss: 0.21013, box_loss: 0.00257, time: 591.26355\n","Train Step 2250/3810, summary_loss: 0.33840, class_loss: 0.21011, box_loss: 0.00257, time: 666.08666\n","Train Step 2500/3810, summary_loss: 0.34006, class_loss: 0.21092, box_loss: 0.00258, time: 740.30640\n","Train Step 2750/3810, summary_loss: 0.33993, class_loss: 0.21067, box_loss: 0.00259, time: 813.83436\n","Train Step 3000/3810, summary_loss: 0.34021, class_loss: 0.21075, box_loss: 0.00259, time: 887.36176\n","Train Step 3250/3810, summary_loss: 0.34054, class_loss: 0.21121, box_loss: 0.00259, time: 961.18079\n","Train Step 3500/3810, summary_loss: 0.34126, class_loss: 0.21162, box_loss: 0.00259, time: 1034.91797\n","Train Step 3750/3810, summary_loss: 0.34153, class_loss: 0.21190, box_loss: 0.00259, time: 1108.70246\n","Train Epoch: 32, summary loss: 0.34155, class loss: 0.21188, box_loss: 0.00259, time: 1126.38145\n","Val 0000000010_0000000000.csv Epoch: 32, summary loss: 1.48725, class loss: 1.21693, box_loss: 0.00541, time: 171.40450\n","Val 0000000054_0000000000.csv Epoch: 32, summary loss: 0.74843, class loss: 0.57816, box_loss: 0.00341, time: 171.40771\n","Val 0000000056_0000000000.csv Epoch: 32, summary loss: 0.42006, class loss: 0.34284, box_loss: 0.00154, time: 171.41029\n","Val 0000000062_0000000000.csv Epoch: 32, summary loss: 0.81524, class loss: 0.46827, box_loss: 0.00694, time: 171.41227\n","Val 0000000067_0000000005.csv Epoch: 32, summary loss: 0.03127, class loss: 0.03127, box_loss: 0.00000, time: 171.41420\n","Val 0000000067_0000000012.csv Epoch: 32, summary loss: 0.24570, class loss: 0.24570, box_loss: 0.00000, time: 171.41609\n","Val 0000000067_0000000014.csv Epoch: 32, summary loss: 0.19610, class loss: 0.11702, box_loss: 0.00158, time: 171.41802\n","Val 0000000067_0000000015.csv Epoch: 32, summary loss: 0.49423, class loss: 0.24612, box_loss: 0.00496, time: 171.41991\n","Val 0000000067_0000000019.csv Epoch: 32, summary loss: 0.39044, class loss: 0.21163, box_loss: 0.00358, time: 171.42181\n","Val 0000000067_0000000024.csv Epoch: 32, summary loss: 0.31382, class loss: 0.14992, box_loss: 0.00328, time: 171.42371\n","Val 0000000067_0000000025.csv Epoch: 32, summary loss: 0.34009, class loss: 0.18214, box_loss: 0.00316, time: 171.42581\n","Val 0000000067_0000000026.csv Epoch: 32, summary loss: 0.04772, class loss: 0.04772, box_loss: 0.00000, time: 171.42793\n","Val 0000000067_0000000027.csv Epoch: 32, summary loss: 0.07673, class loss: 0.07673, box_loss: 0.00000, time: 171.43001\n","Val 0000000067_0000000028.csv Epoch: 32, summary loss: 0.13820, class loss: 0.11307, box_loss: 0.00050, time: 171.43209\n","Val 0000000067_0000000029.csv Epoch: 32, summary loss: 0.09695, class loss: 0.09695, box_loss: 0.00000, time: 171.43417\n","Val 0000000067_0000000031.csv Epoch: 32, summary loss: 0.36863, class loss: 0.21611, box_loss: 0.00305, time: 171.43621\n","Val 0000000067_0000000032.csv Epoch: 32, summary loss: 0.46810, class loss: 0.33734, box_loss: 0.00262, time: 171.45646\n","Val 0000000067_0000000040.csv Epoch: 32, summary loss: 0.28793, class loss: 0.22782, box_loss: 0.00120, time: 171.46037\n","Val 0000000067_0000000041.csv Epoch: 32, summary loss: 0.75587, class loss: 0.40143, box_loss: 0.00709, time: 171.46248\n","Val 0000000067_0000000045.csv Epoch: 32, summary loss: 0.36471, class loss: 0.27668, box_loss: 0.00176, time: 171.46450\n","Val 0000000067_0000000046.csv Epoch: 32, summary loss: 0.17854, class loss: 0.11352, box_loss: 0.00130, time: 171.46655\n","Val 0000000067_0000000050.csv Epoch: 32, summary loss: 0.84237, class loss: 0.58109, box_loss: 0.00523, time: 171.46857\n","Val 0000000067_0000000052.csv Epoch: 32, summary loss: 0.59119, class loss: 0.47269, box_loss: 0.00237, time: 171.47069\n","Val 0000000067_0000000055.csv Epoch: 32, summary loss: 0.41885, class loss: 0.37761, box_loss: 0.00082, time: 171.47273\n","Val 0000000067_0000000058.csv Epoch: 32, summary loss: 0.70189, class loss: 0.45164, box_loss: 0.00501, time: 171.47480\n","Val 0000000067_0000000059.csv Epoch: 32, summary loss: 0.30917, class loss: 0.30917, box_loss: 0.00000, time: 171.47687\n","Val 0000000351_0000000000.csv Epoch: 32, summary loss: 0.94502, class loss: 0.65566, box_loss: 0.00579, time: 171.47894\n","Val 0000000354_0000000000.csv Epoch: 32, summary loss: 1.56883, class loss: 1.07988, box_loss: 0.00978, time: 171.48098\n","Val 0000000359_0000000000.csv Epoch: 32, summary loss: 1.16549, class loss: 0.60935, box_loss: 0.01112, time: 171.48306\n","Val 0000000363_0000000000.csv Epoch: 32, summary loss: 0.34145, class loss: 0.34145, box_loss: 0.00000, time: 171.48515\n","Val 0000000364_0000000000.csv Epoch: 32, summary loss: 0.95238, class loss: 0.92952, box_loss: 0.00046, time: 171.48721\n","Val 0000000367_0000000000.csv Epoch: 32, summary loss: 1.16707, class loss: 0.82397, box_loss: 0.00686, time: 171.48928\n","Val Epoch: 32, summary loss: 0.53968, class loss: 0.38529, box_loss: 0.00309, time: 171.49155\n","Adjusting learning rate of group 0 to 1.2004e-04.\n","\n","2021-06-01T14:41:09.354879\n","LR: 0.0001200424326714591\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.65250\n","Train Step 250/3810, summary_loss: 0.33261, class_loss: 0.20484, box_loss: 0.00256, time: 75.08597\n","Train Step 500/3810, summary_loss: 0.34067, class_loss: 0.20843, box_loss: 0.00264, time: 149.52027\n","Train Step 750/3810, summary_loss: 0.34519, class_loss: 0.21291, box_loss: 0.00265, time: 223.08232\n","Train Step 1000/3810, summary_loss: 0.33973, class_loss: 0.21032, box_loss: 0.00259, time: 297.14128\n","Train Step 1250/3810, summary_loss: 0.33916, class_loss: 0.21099, box_loss: 0.00256, time: 370.93461\n","Train Step 1500/3810, summary_loss: 0.33936, class_loss: 0.21130, box_loss: 0.00256, time: 444.80976\n","Train Step 1750/3810, summary_loss: 0.33867, class_loss: 0.21082, box_loss: 0.00256, time: 518.81001\n","Train Step 2000/3810, summary_loss: 0.33959, class_loss: 0.21146, box_loss: 0.00256, time: 592.54135\n","Train Step 2250/3810, summary_loss: 0.33957, class_loss: 0.21092, box_loss: 0.00257, time: 666.20860\n","Train Step 2500/3810, summary_loss: 0.33938, class_loss: 0.21077, box_loss: 0.00257, time: 740.23934\n","Train Step 2750/3810, summary_loss: 0.33870, class_loss: 0.21015, box_loss: 0.00257, time: 814.11879\n","Train Step 3000/3810, summary_loss: 0.33907, class_loss: 0.21028, box_loss: 0.00258, time: 887.20589\n","Train Step 3250/3810, summary_loss: 0.33915, class_loss: 0.21045, box_loss: 0.00257, time: 960.94358\n","Train Step 3500/3810, summary_loss: 0.34009, class_loss: 0.21151, box_loss: 0.00257, time: 1034.25092\n","Train Step 3750/3810, summary_loss: 0.34089, class_loss: 0.21241, box_loss: 0.00257, time: 1108.32036\n","Train Epoch: 33, summary loss: 0.34150, class loss: 0.21280, box_loss: 0.00257, time: 1125.72587\n","Val 0000000010_0000000000.csv Epoch: 33, summary loss: 1.30098, class loss: 0.91143, box_loss: 0.00779, time: 171.52644\n","Val 0000000054_0000000000.csv Epoch: 33, summary loss: 0.57634, class loss: 0.45316, box_loss: 0.00246, time: 171.52940\n","Val 0000000056_0000000000.csv Epoch: 33, summary loss: 0.42645, class loss: 0.33957, box_loss: 0.00174, time: 171.53187\n","Val 0000000062_0000000000.csv Epoch: 33, summary loss: 1.00256, class loss: 0.71507, box_loss: 0.00575, time: 171.53382\n","Val 0000000067_0000000005.csv Epoch: 33, summary loss: 0.01353, class loss: 0.01353, box_loss: 0.00000, time: 171.53574\n","Val 0000000067_0000000012.csv Epoch: 33, summary loss: 0.16032, class loss: 0.16032, box_loss: 0.00000, time: 171.53765\n","Val 0000000067_0000000014.csv Epoch: 33, summary loss: 0.21662, class loss: 0.12153, box_loss: 0.00190, time: 171.53954\n","Val 0000000067_0000000015.csv Epoch: 33, summary loss: 0.45227, class loss: 0.25010, box_loss: 0.00404, time: 171.54139\n","Val 0000000067_0000000019.csv Epoch: 33, summary loss: 0.40735, class loss: 0.21679, box_loss: 0.00381, time: 171.54325\n","Val 0000000067_0000000024.csv Epoch: 33, summary loss: 0.22329, class loss: 0.11834, box_loss: 0.00210, time: 171.54510\n","Val 0000000067_0000000025.csv Epoch: 33, summary loss: 0.32969, class loss: 0.21344, box_loss: 0.00233, time: 171.54700\n","Val 0000000067_0000000026.csv Epoch: 33, summary loss: 0.03100, class loss: 0.03100, box_loss: 0.00000, time: 171.54889\n","Val 0000000067_0000000027.csv Epoch: 33, summary loss: 0.06501, class loss: 0.06501, box_loss: 0.00000, time: 171.55095\n","Val 0000000067_0000000028.csv Epoch: 33, summary loss: 0.11642, class loss: 0.09697, box_loss: 0.00039, time: 171.55338\n","Val 0000000067_0000000029.csv Epoch: 33, summary loss: 0.07563, class loss: 0.07563, box_loss: 0.00000, time: 171.55552\n","Val 0000000067_0000000031.csv Epoch: 33, summary loss: 0.30154, class loss: 0.18568, box_loss: 0.00232, time: 171.55772\n","Val 0000000067_0000000032.csv Epoch: 33, summary loss: 0.52513, class loss: 0.38916, box_loss: 0.00272, time: 171.55975\n","Val 0000000067_0000000040.csv Epoch: 33, summary loss: 0.34479, class loss: 0.28035, box_loss: 0.00129, time: 171.56180\n","Val 0000000067_0000000041.csv Epoch: 33, summary loss: 0.66170, class loss: 0.35580, box_loss: 0.00612, time: 171.56387\n","Val 0000000067_0000000045.csv Epoch: 33, summary loss: 0.41721, class loss: 0.34530, box_loss: 0.00144, time: 171.56600\n","Val 0000000067_0000000046.csv Epoch: 33, summary loss: 0.18190, class loss: 0.11567, box_loss: 0.00132, time: 171.56804\n","Val 0000000067_0000000050.csv Epoch: 33, summary loss: 0.89176, class loss: 0.59029, box_loss: 0.00603, time: 171.57007\n","Val 0000000067_0000000052.csv Epoch: 33, summary loss: 0.64932, class loss: 0.49809, box_loss: 0.00302, time: 171.57213\n","Val 0000000067_0000000055.csv Epoch: 33, summary loss: 0.35478, class loss: 0.32133, box_loss: 0.00067, time: 171.57417\n","Val 0000000067_0000000058.csv Epoch: 33, summary loss: 0.59030, class loss: 0.35106, box_loss: 0.00478, time: 171.57623\n","Val 0000000067_0000000059.csv Epoch: 33, summary loss: 0.58233, class loss: 0.58233, box_loss: 0.00000, time: 171.57830\n","Val 0000000351_0000000000.csv Epoch: 33, summary loss: 0.62327, class loss: 0.41038, box_loss: 0.00426, time: 171.58034\n","Val 0000000354_0000000000.csv Epoch: 33, summary loss: 1.05715, class loss: 0.61548, box_loss: 0.00883, time: 171.58241\n","Val 0000000359_0000000000.csv Epoch: 33, summary loss: 0.85267, class loss: 0.44189, box_loss: 0.00822, time: 171.58450\n","Val 0000000363_0000000000.csv Epoch: 33, summary loss: 1.12883, class loss: 1.12883, box_loss: 0.00000, time: 171.60722\n","Val 0000000364_0000000000.csv Epoch: 33, summary loss: 1.92762, class loss: 1.90378, box_loss: 0.00048, time: 171.61101\n","Val 0000000367_0000000000.csv Epoch: 33, summary loss: 0.87020, class loss: 0.60483, box_loss: 0.00531, time: 171.61312\n","Val Epoch: 33, summary loss: 0.54244, class loss: 0.40319, box_loss: 0.00278, time: 171.61530\n","Adjusting learning rate of group 0 to 1.1820e-04.\n","\n","2021-06-01T15:02:46.995866\n","LR: 0.00011819980260289\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.88857\n","Train Step 250/3810, summary_loss: 0.33828, class_loss: 0.21036, box_loss: 0.00256, time: 74.91536\n","Train Step 500/3810, summary_loss: 0.33804, class_loss: 0.20997, box_loss: 0.00256, time: 148.56014\n","Train Step 750/3810, summary_loss: 0.33743, class_loss: 0.20992, box_loss: 0.00255, time: 222.41084\n","Train Step 1000/3810, summary_loss: 0.34063, class_loss: 0.21170, box_loss: 0.00258, time: 296.39580\n","Train Step 1250/3810, summary_loss: 0.33975, class_loss: 0.21193, box_loss: 0.00256, time: 370.25389\n","Train Step 1500/3810, summary_loss: 0.33991, class_loss: 0.21214, box_loss: 0.00256, time: 444.12283\n","Train Step 1750/3810, summary_loss: 0.34163, class_loss: 0.21167, box_loss: 0.00260, time: 517.61072\n","Train Step 2000/3810, summary_loss: 0.34040, class_loss: 0.21077, box_loss: 0.00259, time: 591.52099\n","Train Step 2250/3810, summary_loss: 0.34156, class_loss: 0.21117, box_loss: 0.00261, time: 665.10570\n","Train Step 2500/3810, summary_loss: 0.34029, class_loss: 0.21057, box_loss: 0.00259, time: 738.94307\n","Train Step 2750/3810, summary_loss: 0.33964, class_loss: 0.21029, box_loss: 0.00259, time: 812.83973\n","Train Step 3000/3810, summary_loss: 0.33842, class_loss: 0.20972, box_loss: 0.00257, time: 886.55603\n","Train Step 3250/3810, summary_loss: 0.33736, class_loss: 0.20908, box_loss: 0.00257, time: 960.27840\n","Train Step 3500/3810, summary_loss: 0.33730, class_loss: 0.20899, box_loss: 0.00257, time: 1034.04607\n","Train Step 3750/3810, summary_loss: 0.33926, class_loss: 0.21043, box_loss: 0.00258, time: 1107.69468\n","Train Epoch: 34, summary loss: 0.33940, class loss: 0.21046, box_loss: 0.00258, time: 1125.19758\n","Val 0000000010_0000000000.csv Epoch: 34, summary loss: 1.50313, class loss: 1.06276, box_loss: 0.00881, time: 171.66362\n","Val 0000000054_0000000000.csv Epoch: 34, summary loss: 0.68078, class loss: 0.50476, box_loss: 0.00352, time: 171.66677\n","Val 0000000056_0000000000.csv Epoch: 34, summary loss: 0.58375, class loss: 0.48049, box_loss: 0.00207, time: 171.66912\n","Val 0000000062_0000000000.csv Epoch: 34, summary loss: 0.65380, class loss: 0.41201, box_loss: 0.00484, time: 171.67126\n","Val 0000000067_0000000005.csv Epoch: 34, summary loss: 0.02239, class loss: 0.02239, box_loss: 0.00000, time: 171.67348\n","Val 0000000067_0000000012.csv Epoch: 34, summary loss: 0.13411, class loss: 0.13411, box_loss: 0.00000, time: 171.67561\n","Val 0000000067_0000000014.csv Epoch: 34, summary loss: 0.17307, class loss: 0.09889, box_loss: 0.00148, time: 171.67767\n","Val 0000000067_0000000015.csv Epoch: 34, summary loss: 0.54408, class loss: 0.21414, box_loss: 0.00660, time: 171.67975\n","Val 0000000067_0000000019.csv Epoch: 34, summary loss: 0.41167, class loss: 0.22320, box_loss: 0.00377, time: 171.68181\n","Val 0000000067_0000000024.csv Epoch: 34, summary loss: 0.28131, class loss: 0.15141, box_loss: 0.00260, time: 171.68380\n","Val 0000000067_0000000025.csv Epoch: 34, summary loss: 0.34416, class loss: 0.19537, box_loss: 0.00298, time: 171.68579\n","Val 0000000067_0000000026.csv Epoch: 34, summary loss: 0.02389, class loss: 0.02389, box_loss: 0.00000, time: 171.68779\n","Val 0000000067_0000000027.csv Epoch: 34, summary loss: 0.05400, class loss: 0.05400, box_loss: 0.00000, time: 171.68978\n","Val 0000000067_0000000028.csv Epoch: 34, summary loss: 0.10073, class loss: 0.07427, box_loss: 0.00053, time: 171.70773\n","Val 0000000067_0000000029.csv Epoch: 34, summary loss: 0.06231, class loss: 0.06231, box_loss: 0.00000, time: 171.71134\n","Val 0000000067_0000000031.csv Epoch: 34, summary loss: 0.31080, class loss: 0.18189, box_loss: 0.00258, time: 171.71345\n","Val 0000000067_0000000032.csv Epoch: 34, summary loss: 0.44887, class loss: 0.32211, box_loss: 0.00254, time: 171.71550\n","Val 0000000067_0000000040.csv Epoch: 34, summary loss: 0.30603, class loss: 0.25100, box_loss: 0.00110, time: 171.71758\n","Val 0000000067_0000000041.csv Epoch: 34, summary loss: 0.67265, class loss: 0.37182, box_loss: 0.00602, time: 171.71965\n","Val 0000000067_0000000045.csv Epoch: 34, summary loss: 0.35183, class loss: 0.27797, box_loss: 0.00148, time: 171.72179\n","Val 0000000067_0000000046.csv Epoch: 34, summary loss: 0.16911, class loss: 0.11203, box_loss: 0.00114, time: 171.72388\n","Val 0000000067_0000000050.csv Epoch: 34, summary loss: 0.93002, class loss: 0.64939, box_loss: 0.00561, time: 171.72595\n","Val 0000000067_0000000052.csv Epoch: 34, summary loss: 0.62891, class loss: 0.52623, box_loss: 0.00205, time: 171.72804\n","Val 0000000067_0000000055.csv Epoch: 34, summary loss: 0.43242, class loss: 0.39883, box_loss: 0.00067, time: 171.73010\n","Val 0000000067_0000000058.csv Epoch: 34, summary loss: 0.69359, class loss: 0.50868, box_loss: 0.00370, time: 171.73219\n","Val 0000000067_0000000059.csv Epoch: 34, summary loss: 0.47195, class loss: 0.47195, box_loss: 0.00000, time: 171.73428\n","Val 0000000351_0000000000.csv Epoch: 34, summary loss: 0.67965, class loss: 0.44972, box_loss: 0.00460, time: 171.73640\n","Val 0000000354_0000000000.csv Epoch: 34, summary loss: 0.89005, class loss: 0.44163, box_loss: 0.00897, time: 171.73850\n","Val 0000000359_0000000000.csv Epoch: 34, summary loss: 0.92643, class loss: 0.45936, box_loss: 0.00934, time: 171.74059\n","Val 0000000363_0000000000.csv Epoch: 34, summary loss: 0.38675, class loss: 0.38675, box_loss: 0.00000, time: 171.74265\n","Val 0000000364_0000000000.csv Epoch: 34, summary loss: 0.87134, class loss: 0.84392, box_loss: 0.00055, time: 171.74475\n","Val 0000000367_0000000000.csv Epoch: 34, summary loss: 0.93321, class loss: 0.65070, box_loss: 0.00565, time: 171.74686\n","Val Epoch: 34, summary loss: 0.48990, class loss: 0.34431, box_loss: 0.00291, time: 171.74905\n","Adjusting learning rate of group 0 to 1.1639e-04.\n","\n","2021-06-01T15:24:24.248863\n","LR: 0.00011638545657933762\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.77427\n","Train Step 250/3810, summary_loss: 0.34198, class_loss: 0.21334, box_loss: 0.00257, time: 75.01296\n","Train Step 500/3810, summary_loss: 0.34255, class_loss: 0.21213, box_loss: 0.00261, time: 149.21394\n","Train Step 750/3810, summary_loss: 0.33754, class_loss: 0.21057, box_loss: 0.00254, time: 222.95133\n","Train Step 1000/3810, summary_loss: 0.33907, class_loss: 0.21114, box_loss: 0.00256, time: 296.72326\n","Train Step 1250/3810, summary_loss: 0.33663, class_loss: 0.20942, box_loss: 0.00254, time: 370.54814\n","Train Step 1500/3810, summary_loss: 0.33551, class_loss: 0.20891, box_loss: 0.00253, time: 444.75039\n","Train Step 1750/3810, summary_loss: 0.33683, class_loss: 0.20948, box_loss: 0.00255, time: 518.66494\n","Train Step 2000/3810, summary_loss: 0.34068, class_loss: 0.21284, box_loss: 0.00256, time: 592.36783\n","Train Step 2250/3810, summary_loss: 0.34043, class_loss: 0.21299, box_loss: 0.00255, time: 666.07520\n","Train Step 2500/3810, summary_loss: 0.34035, class_loss: 0.21258, box_loss: 0.00256, time: 739.58959\n","Train Step 2750/3810, summary_loss: 0.33917, class_loss: 0.21147, box_loss: 0.00255, time: 813.87093\n","Train Step 3000/3810, summary_loss: 0.33779, class_loss: 0.21073, box_loss: 0.00254, time: 887.11016\n","Train Step 3250/3810, summary_loss: 0.34044, class_loss: 0.21274, box_loss: 0.00255, time: 961.34777\n","Train Step 3500/3810, summary_loss: 0.34166, class_loss: 0.21347, box_loss: 0.00256, time: 1035.40795\n","Train Step 3750/3810, summary_loss: 0.34212, class_loss: 0.21364, box_loss: 0.00257, time: 1109.04405\n","Train Epoch: 35, summary loss: 0.34190, class loss: 0.21342, box_loss: 0.00257, time: 1126.44592\n","Val 0000000010_0000000000.csv Epoch: 35, summary loss: 0.84066, class loss: 0.52672, box_loss: 0.00628, time: 171.49140\n","Val 0000000054_0000000000.csv Epoch: 35, summary loss: 0.50860, class loss: 0.43181, box_loss: 0.00154, time: 171.49418\n","Val 0000000056_0000000000.csv Epoch: 35, summary loss: 0.45702, class loss: 0.36282, box_loss: 0.00188, time: 171.49655\n","Val 0000000062_0000000000.csv Epoch: 35, summary loss: 1.09563, class loss: 0.71820, box_loss: 0.00755, time: 171.49846\n","Val 0000000067_0000000005.csv Epoch: 35, summary loss: 0.10059, class loss: 0.10059, box_loss: 0.00000, time: 171.50037\n","Val 0000000067_0000000012.csv Epoch: 35, summary loss: 0.28755, class loss: 0.28755, box_loss: 0.00000, time: 171.50229\n","Val 0000000067_0000000014.csv Epoch: 35, summary loss: 0.25985, class loss: 0.15639, box_loss: 0.00207, time: 171.50415\n","Val 0000000067_0000000015.csv Epoch: 35, summary loss: 0.59014, class loss: 0.28964, box_loss: 0.00601, time: 171.50601\n","Val 0000000067_0000000019.csv Epoch: 35, summary loss: 0.42538, class loss: 0.21581, box_loss: 0.00419, time: 171.50810\n","Val 0000000067_0000000024.csv Epoch: 35, summary loss: 0.28872, class loss: 0.15494, box_loss: 0.00268, time: 171.51013\n","Val 0000000067_0000000025.csv Epoch: 35, summary loss: 0.35344, class loss: 0.20416, box_loss: 0.00299, time: 171.51220\n","Val 0000000067_0000000026.csv Epoch: 35, summary loss: 0.08983, class loss: 0.08983, box_loss: 0.00000, time: 171.51426\n","Val 0000000067_0000000027.csv Epoch: 35, summary loss: 0.18679, class loss: 0.18679, box_loss: 0.00000, time: 171.51635\n","Val 0000000067_0000000028.csv Epoch: 35, summary loss: 0.19998, class loss: 0.16815, box_loss: 0.00064, time: 171.51842\n","Val 0000000067_0000000029.csv Epoch: 35, summary loss: 0.11174, class loss: 0.11174, box_loss: 0.00000, time: 171.52047\n","Val 0000000067_0000000031.csv Epoch: 35, summary loss: 0.38352, class loss: 0.23015, box_loss: 0.00307, time: 171.52254\n","Val 0000000067_0000000032.csv Epoch: 35, summary loss: 0.52990, class loss: 0.40325, box_loss: 0.00253, time: 171.52471\n","Val 0000000067_0000000040.csv Epoch: 35, summary loss: 0.38067, class loss: 0.32380, box_loss: 0.00114, time: 171.52679\n","Val 0000000067_0000000041.csv Epoch: 35, summary loss: 0.65339, class loss: 0.34964, box_loss: 0.00608, time: 171.52886\n","Val 0000000067_0000000045.csv Epoch: 35, summary loss: 0.51217, class loss: 0.38820, box_loss: 0.00248, time: 171.53092\n","Val 0000000067_0000000046.csv Epoch: 35, summary loss: 0.17345, class loss: 0.11384, box_loss: 0.00119, time: 171.53299\n","Val 0000000067_0000000050.csv Epoch: 35, summary loss: 0.99988, class loss: 0.70588, box_loss: 0.00588, time: 171.53509\n","Val 0000000067_0000000052.csv Epoch: 35, summary loss: 0.95366, class loss: 0.82716, box_loss: 0.00253, time: 171.53716\n","Val 0000000067_0000000055.csv Epoch: 35, summary loss: 0.71457, class loss: 0.66026, box_loss: 0.00109, time: 171.53919\n","Val 0000000067_0000000058.csv Epoch: 35, summary loss: 0.80620, class loss: 0.51358, box_loss: 0.00585, time: 171.54126\n","Val 0000000067_0000000059.csv Epoch: 35, summary loss: 0.78329, class loss: 0.78329, box_loss: 0.00000, time: 171.56284\n","Val 0000000351_0000000000.csv Epoch: 35, summary loss: 0.57865, class loss: 0.35990, box_loss: 0.00437, time: 171.56504\n","Val 0000000354_0000000000.csv Epoch: 35, summary loss: 1.29272, class loss: 0.75650, box_loss: 0.01072, time: 171.56714\n","Val 0000000359_0000000000.csv Epoch: 35, summary loss: 0.85488, class loss: 0.41219, box_loss: 0.00885, time: 171.56921\n","Val 0000000363_0000000000.csv Epoch: 35, summary loss: 0.95550, class loss: 0.95550, box_loss: 0.00000, time: 171.57125\n","Val 0000000364_0000000000.csv Epoch: 35, summary loss: 1.33738, class loss: 1.30828, box_loss: 0.00058, time: 171.57333\n","Val 0000000367_0000000000.csv Epoch: 35, summary loss: 0.89778, class loss: 0.63836, box_loss: 0.00519, time: 171.57537\n","Val Epoch: 35, summary loss: 0.58136, class loss: 0.42922, box_loss: 0.00304, time: 171.57764\n","Adjusting learning rate of group 0 to 1.1460e-04.\n","\n","2021-06-01T15:46:02.582967\n","LR: 0.0001145989604457232\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.69310\n","Train Step 250/3810, summary_loss: 0.34544, class_loss: 0.21728, box_loss: 0.00256, time: 75.17173\n","Train Step 500/3810, summary_loss: 0.34949, class_loss: 0.21875, box_loss: 0.00261, time: 148.95800\n","Train Step 750/3810, summary_loss: 0.34191, class_loss: 0.21469, box_loss: 0.00254, time: 221.99702\n","Train Step 1000/3810, summary_loss: 0.34536, class_loss: 0.21487, box_loss: 0.00261, time: 295.83339\n","Train Step 1250/3810, summary_loss: 0.34526, class_loss: 0.21428, box_loss: 0.00262, time: 369.92352\n","Train Step 1500/3810, summary_loss: 0.34392, class_loss: 0.21389, box_loss: 0.00260, time: 443.31728\n","Train Step 1750/3810, summary_loss: 0.34175, class_loss: 0.21262, box_loss: 0.00258, time: 517.57884\n","Train Step 2000/3810, summary_loss: 0.33861, class_loss: 0.21097, box_loss: 0.00255, time: 591.10999\n","Train Step 2250/3810, summary_loss: 0.33881, class_loss: 0.21096, box_loss: 0.00256, time: 664.84303\n","Train Step 2500/3810, summary_loss: 0.33824, class_loss: 0.21063, box_loss: 0.00255, time: 738.35553\n","Train Step 2750/3810, summary_loss: 0.33589, class_loss: 0.20936, box_loss: 0.00253, time: 812.69918\n","Train Step 3000/3810, summary_loss: 0.33555, class_loss: 0.20910, box_loss: 0.00253, time: 887.08444\n","Train Step 3250/3810, summary_loss: 0.33446, class_loss: 0.20846, box_loss: 0.00252, time: 961.03781\n","Train Step 3500/3810, summary_loss: 0.33437, class_loss: 0.20853, box_loss: 0.00252, time: 1034.91836\n","Train Step 3750/3810, summary_loss: 0.33509, class_loss: 0.20886, box_loss: 0.00252, time: 1108.44980\n","Train Epoch: 36, summary loss: 0.33499, class loss: 0.20879, box_loss: 0.00252, time: 1126.09609\n","Val 0000000010_0000000000.csv Epoch: 36, summary loss: 0.73495, class loss: 0.42924, box_loss: 0.00611, time: 171.87217\n","Val 0000000054_0000000000.csv Epoch: 36, summary loss: 0.61617, class loss: 0.48325, box_loss: 0.00266, time: 171.87491\n","Val 0000000056_0000000000.csv Epoch: 36, summary loss: 0.37990, class loss: 0.30038, box_loss: 0.00159, time: 171.87727\n","Val 0000000062_0000000000.csv Epoch: 36, summary loss: 0.75532, class loss: 0.47305, box_loss: 0.00565, time: 171.87938\n","Val 0000000067_0000000005.csv Epoch: 36, summary loss: 0.02020, class loss: 0.02020, box_loss: 0.00000, time: 171.88148\n","Val 0000000067_0000000012.csv Epoch: 36, summary loss: 0.20414, class loss: 0.20414, box_loss: 0.00000, time: 171.88358\n","Val 0000000067_0000000014.csv Epoch: 36, summary loss: 0.21698, class loss: 0.12349, box_loss: 0.00187, time: 171.88565\n","Val 0000000067_0000000015.csv Epoch: 36, summary loss: 0.44329, class loss: 0.23295, box_loss: 0.00421, time: 171.90545\n","Val 0000000067_0000000019.csv Epoch: 36, summary loss: 0.41547, class loss: 0.23555, box_loss: 0.00360, time: 171.90755\n","Val 0000000067_0000000024.csv Epoch: 36, summary loss: 0.33654, class loss: 0.19336, box_loss: 0.00286, time: 171.90974\n","Val 0000000067_0000000025.csv Epoch: 36, summary loss: 0.34478, class loss: 0.20668, box_loss: 0.00276, time: 171.91177\n","Val 0000000067_0000000026.csv Epoch: 36, summary loss: 0.02619, class loss: 0.02619, box_loss: 0.00000, time: 171.91397\n","Val 0000000067_0000000027.csv Epoch: 36, summary loss: 0.05736, class loss: 0.05736, box_loss: 0.00000, time: 171.91605\n","Val 0000000067_0000000028.csv Epoch: 36, summary loss: 0.10726, class loss: 0.08471, box_loss: 0.00045, time: 171.91808\n","Val 0000000067_0000000029.csv Epoch: 36, summary loss: 0.06519, class loss: 0.06519, box_loss: 0.00000, time: 171.92036\n","Val 0000000067_0000000031.csv Epoch: 36, summary loss: 0.33252, class loss: 0.19519, box_loss: 0.00275, time: 171.92265\n","Val 0000000067_0000000032.csv Epoch: 36, summary loss: 0.43237, class loss: 0.32903, box_loss: 0.00207, time: 171.92479\n","Val 0000000067_0000000040.csv Epoch: 36, summary loss: 0.23796, class loss: 0.18411, box_loss: 0.00108, time: 171.92696\n","Val 0000000067_0000000041.csv Epoch: 36, summary loss: 0.69637, class loss: 0.38676, box_loss: 0.00619, time: 171.92913\n","Val 0000000067_0000000045.csv Epoch: 36, summary loss: 0.41569, class loss: 0.30658, box_loss: 0.00218, time: 171.93123\n","Val 0000000067_0000000046.csv Epoch: 36, summary loss: 0.16762, class loss: 0.11064, box_loss: 0.00114, time: 171.93336\n","Val 0000000067_0000000050.csv Epoch: 36, summary loss: 1.08614, class loss: 0.75631, box_loss: 0.00660, time: 171.93553\n","Val 0000000067_0000000052.csv Epoch: 36, summary loss: 0.73708, class loss: 0.62579, box_loss: 0.00223, time: 171.93762\n","Val 0000000067_0000000055.csv Epoch: 36, summary loss: 0.47747, class loss: 0.44351, box_loss: 0.00068, time: 171.93972\n","Val 0000000067_0000000058.csv Epoch: 36, summary loss: 0.68066, class loss: 0.45962, box_loss: 0.00442, time: 171.94186\n","Val 0000000067_0000000059.csv Epoch: 36, summary loss: 0.51454, class loss: 0.51454, box_loss: 0.00000, time: 171.94398\n","Val 0000000351_0000000000.csv Epoch: 36, summary loss: 0.57142, class loss: 0.36480, box_loss: 0.00413, time: 171.94610\n","Val 0000000354_0000000000.csv Epoch: 36, summary loss: 1.12263, class loss: 0.66246, box_loss: 0.00920, time: 171.94821\n","Val 0000000359_0000000000.csv Epoch: 36, summary loss: 0.71505, class loss: 0.35891, box_loss: 0.00712, time: 171.95034\n","Val 0000000363_0000000000.csv Epoch: 36, summary loss: 0.25420, class loss: 0.25420, box_loss: 0.00000, time: 171.95247\n","Val 0000000364_0000000000.csv Epoch: 36, summary loss: 0.45485, class loss: 0.42869, box_loss: 0.00052, time: 171.95463\n","Val 0000000367_0000000000.csv Epoch: 36, summary loss: 0.92160, class loss: 0.70870, box_loss: 0.00426, time: 171.95675\n","Val Epoch: 36, summary loss: 0.45444, class loss: 0.31955, box_loss: 0.00270, time: 171.95898\n","Adjusting learning rate of group 0 to 1.1284e-04.\n","\n","2021-06-01T16:07:40.944327\n","LR: 0.00011283988671117154\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.84585\n","Train Step 250/3810, summary_loss: 0.38965, class_loss: 0.26111, box_loss: 0.00257, time: 75.88727\n","Train Step 500/3810, summary_loss: 0.36920, class_loss: 0.24037, box_loss: 0.00258, time: 149.51253\n","Train Step 750/3810, summary_loss: 0.35802, class_loss: 0.22934, box_loss: 0.00257, time: 223.21570\n","Train Step 1000/3810, summary_loss: 0.35140, class_loss: 0.22355, box_loss: 0.00256, time: 296.92599\n","Train Step 1250/3810, summary_loss: 0.34482, class_loss: 0.21905, box_loss: 0.00252, time: 371.19750\n","Train Step 1500/3810, summary_loss: 0.34065, class_loss: 0.21599, box_loss: 0.00249, time: 445.18524\n","Train Step 1750/3810, summary_loss: 0.33974, class_loss: 0.21465, box_loss: 0.00250, time: 518.73979\n","Train Step 2000/3810, summary_loss: 0.33847, class_loss: 0.21341, box_loss: 0.00250, time: 592.70421\n","Train Step 2250/3810, summary_loss: 0.33576, class_loss: 0.21173, box_loss: 0.00248, time: 666.38271\n","Train Step 2500/3810, summary_loss: 0.33411, class_loss: 0.21069, box_loss: 0.00247, time: 740.16991\n","Train Step 2750/3810, summary_loss: 0.33441, class_loss: 0.21076, box_loss: 0.00247, time: 813.89783\n","Train Step 3000/3810, summary_loss: 0.33435, class_loss: 0.21043, box_loss: 0.00248, time: 887.66318\n","Train Step 3250/3810, summary_loss: 0.33351, class_loss: 0.20998, box_loss: 0.00247, time: 961.43679\n","Train Step 3500/3810, summary_loss: 0.33233, class_loss: 0.20900, box_loss: 0.00247, time: 1035.52110\n","Train Step 3750/3810, summary_loss: 0.33169, class_loss: 0.20865, box_loss: 0.00246, time: 1109.50408\n","Train Epoch: 37, summary loss: 0.33177, class loss: 0.20870, box_loss: 0.00246, time: 1127.11645\n","Val 0000000010_0000000000.csv Epoch: 37, summary loss: 0.48055, class loss: 0.32324, box_loss: 0.00315, time: 171.95292\n","Val 0000000054_0000000000.csv Epoch: 37, summary loss: 0.50832, class loss: 0.38931, box_loss: 0.00238, time: 171.95553\n","Val 0000000056_0000000000.csv Epoch: 37, summary loss: 0.39465, class loss: 0.30206, box_loss: 0.00185, time: 171.95803\n","Val 0000000062_0000000000.csv Epoch: 37, summary loss: 0.99071, class loss: 0.68612, box_loss: 0.00609, time: 171.96016\n","Val 0000000067_0000000005.csv Epoch: 37, summary loss: 0.01680, class loss: 0.01680, box_loss: 0.00000, time: 171.96222\n","Val 0000000067_0000000012.csv Epoch: 37, summary loss: 0.22508, class loss: 0.22508, box_loss: 0.00000, time: 171.96429\n","Val 0000000067_0000000014.csv Epoch: 37, summary loss: 0.19361, class loss: 0.13667, box_loss: 0.00114, time: 171.96630\n","Val 0000000067_0000000015.csv Epoch: 37, summary loss: 0.47980, class loss: 0.23681, box_loss: 0.00486, time: 171.96830\n","Val 0000000067_0000000019.csv Epoch: 37, summary loss: 0.38666, class loss: 0.22414, box_loss: 0.00325, time: 171.97038\n","Val 0000000067_0000000024.csv Epoch: 37, summary loss: 0.28921, class loss: 0.14660, box_loss: 0.00285, time: 171.97247\n","Val 0000000067_0000000025.csv Epoch: 37, summary loss: 0.38541, class loss: 0.22356, box_loss: 0.00324, time: 171.97450\n","Val 0000000067_0000000026.csv Epoch: 37, summary loss: 0.03289, class loss: 0.03289, box_loss: 0.00000, time: 171.97652\n","Val 0000000067_0000000027.csv Epoch: 37, summary loss: 0.04445, class loss: 0.04445, box_loss: 0.00000, time: 171.97855\n","Val 0000000067_0000000028.csv Epoch: 37, summary loss: 0.10536, class loss: 0.07048, box_loss: 0.00070, time: 171.98058\n","Val 0000000067_0000000029.csv Epoch: 37, summary loss: 0.04188, class loss: 0.04188, box_loss: 0.00000, time: 171.98263\n","Val 0000000067_0000000031.csv Epoch: 37, summary loss: 0.40430, class loss: 0.23035, box_loss: 0.00348, time: 171.98468\n","Val 0000000067_0000000032.csv Epoch: 37, summary loss: 0.44766, class loss: 0.32578, box_loss: 0.00244, time: 171.98670\n","Val 0000000067_0000000040.csv Epoch: 37, summary loss: 0.41944, class loss: 0.34731, box_loss: 0.00144, time: 171.98873\n","Val 0000000067_0000000041.csv Epoch: 37, summary loss: 0.71549, class loss: 0.36226, box_loss: 0.00706, time: 171.99076\n","Val 0000000067_0000000045.csv Epoch: 37, summary loss: 0.35132, class loss: 0.24082, box_loss: 0.00221, time: 171.99279\n","Val 0000000067_0000000046.csv Epoch: 37, summary loss: 0.16413, class loss: 0.10694, box_loss: 0.00114, time: 172.01376\n","Val 0000000067_0000000050.csv Epoch: 37, summary loss: 0.97102, class loss: 0.70685, box_loss: 0.00528, time: 172.01593\n","Val 0000000067_0000000052.csv Epoch: 37, summary loss: 0.79980, class loss: 0.66053, box_loss: 0.00279, time: 172.01808\n","Val 0000000067_0000000055.csv Epoch: 37, summary loss: 0.54692, class loss: 0.50131, box_loss: 0.00091, time: 172.02022\n","Val 0000000067_0000000058.csv Epoch: 37, summary loss: 0.71470, class loss: 0.41667, box_loss: 0.00596, time: 172.02233\n","Val 0000000067_0000000059.csv Epoch: 37, summary loss: 0.56801, class loss: 0.56801, box_loss: 0.00000, time: 172.02441\n","Val 0000000351_0000000000.csv Epoch: 37, summary loss: 0.63432, class loss: 0.40634, box_loss: 0.00456, time: 172.02654\n","Val 0000000354_0000000000.csv Epoch: 37, summary loss: 1.51201, class loss: 0.81610, box_loss: 0.01392, time: 172.02869\n","Val 0000000359_0000000000.csv Epoch: 37, summary loss: 1.20509, class loss: 0.60481, box_loss: 0.01201, time: 172.03081\n","Val 0000000363_0000000000.csv Epoch: 37, summary loss: 0.37932, class loss: 0.37932, box_loss: 0.00000, time: 172.03296\n","Val 0000000364_0000000000.csv Epoch: 37, summary loss: 0.52624, class loss: 0.50133, box_loss: 0.00050, time: 172.03507\n","Val 0000000367_0000000000.csv Epoch: 37, summary loss: 1.04598, class loss: 0.80740, box_loss: 0.00477, time: 172.03717\n","Val Epoch: 37, summary loss: 0.49941, class loss: 0.34632, box_loss: 0.00306, time: 172.03940\n","Adjusting learning rate of group 0 to 1.1111e-04.\n","\n","2021-06-01T16:29:20.405934\n","LR: 0.00011110781444671659\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.98390\n","Train Step 250/3810, summary_loss: 0.31920, class_loss: 0.19985, box_loss: 0.00239, time: 74.80967\n","Train Step 500/3810, summary_loss: 0.31835, class_loss: 0.20022, box_loss: 0.00236, time: 148.47558\n","Train Step 750/3810, summary_loss: 0.32564, class_loss: 0.20494, box_loss: 0.00241, time: 222.42869\n","Train Step 1000/3810, summary_loss: 0.32708, class_loss: 0.20548, box_loss: 0.00243, time: 296.18583\n","Train Step 1250/3810, summary_loss: 0.32530, class_loss: 0.20391, box_loss: 0.00243, time: 370.61174\n","Train Step 1500/3810, summary_loss: 0.32649, class_loss: 0.20492, box_loss: 0.00243, time: 444.98433\n","Train Step 1750/3810, summary_loss: 0.32777, class_loss: 0.20481, box_loss: 0.00246, time: 519.18654\n","Train Step 2000/3810, summary_loss: 0.32774, class_loss: 0.20480, box_loss: 0.00246, time: 593.52200\n","Train Step 2250/3810, summary_loss: 0.32770, class_loss: 0.20502, box_loss: 0.00245, time: 667.54999\n","Train Step 2500/3810, summary_loss: 0.32667, class_loss: 0.20460, box_loss: 0.00244, time: 741.25944\n","Train Step 2750/3810, summary_loss: 0.32773, class_loss: 0.20529, box_loss: 0.00245, time: 814.68950\n","Train Step 3000/3810, summary_loss: 0.32911, class_loss: 0.20579, box_loss: 0.00247, time: 888.09122\n","Train Step 3250/3810, summary_loss: 0.32896, class_loss: 0.20598, box_loss: 0.00246, time: 961.42191\n","Train Step 3500/3810, summary_loss: 0.32899, class_loss: 0.20592, box_loss: 0.00246, time: 1035.31950\n","Train Step 3750/3810, summary_loss: 0.32919, class_loss: 0.20621, box_loss: 0.00246, time: 1108.67728\n","Train Epoch: 38, summary loss: 0.32876, class loss: 0.20599, box_loss: 0.00246, time: 1126.05318\n","Val 0000000010_0000000000.csv Epoch: 38, summary loss: 2.14569, class loss: 1.47898, box_loss: 0.01333, time: 172.23848\n","Val 0000000054_0000000000.csv Epoch: 38, summary loss: 0.50644, class loss: 0.40094, box_loss: 0.00211, time: 172.24213\n","Val 0000000056_0000000000.csv Epoch: 38, summary loss: 0.39777, class loss: 0.31700, box_loss: 0.00162, time: 172.24471\n","Val 0000000062_0000000000.csv Epoch: 38, summary loss: 0.74001, class loss: 0.40299, box_loss: 0.00674, time: 172.25708\n","Val 0000000067_0000000005.csv Epoch: 38, summary loss: 0.02231, class loss: 0.02231, box_loss: 0.00000, time: 172.26091\n","Val 0000000067_0000000012.csv Epoch: 38, summary loss: 0.07895, class loss: 0.07895, box_loss: 0.00000, time: 172.26307\n","Val 0000000067_0000000014.csv Epoch: 38, summary loss: 0.20609, class loss: 0.14422, box_loss: 0.00124, time: 172.26517\n","Val 0000000067_0000000015.csv Epoch: 38, summary loss: 0.45132, class loss: 0.22161, box_loss: 0.00459, time: 172.26726\n","Val 0000000067_0000000019.csv Epoch: 38, summary loss: 0.37022, class loss: 0.19497, box_loss: 0.00351, time: 172.26938\n","Val 0000000067_0000000024.csv Epoch: 38, summary loss: 0.37312, class loss: 0.19696, box_loss: 0.00352, time: 172.27153\n","Val 0000000067_0000000025.csv Epoch: 38, summary loss: 0.35710, class loss: 0.21111, box_loss: 0.00292, time: 172.27364\n","Val 0000000067_0000000026.csv Epoch: 38, summary loss: 0.03772, class loss: 0.03772, box_loss: 0.00000, time: 172.27575\n","Val 0000000067_0000000027.csv Epoch: 38, summary loss: 0.06275, class loss: 0.06275, box_loss: 0.00000, time: 172.27783\n","Val 0000000067_0000000028.csv Epoch: 38, summary loss: 0.09537, class loss: 0.07055, box_loss: 0.00050, time: 172.27994\n","Val 0000000067_0000000029.csv Epoch: 38, summary loss: 0.06271, class loss: 0.06271, box_loss: 0.00000, time: 172.28203\n","Val 0000000067_0000000031.csv Epoch: 38, summary loss: 0.36887, class loss: 0.20770, box_loss: 0.00322, time: 172.28411\n","Val 0000000067_0000000032.csv Epoch: 38, summary loss: 0.41522, class loss: 0.29753, box_loss: 0.00235, time: 172.28621\n","Val 0000000067_0000000040.csv Epoch: 38, summary loss: 0.23228, class loss: 0.16691, box_loss: 0.00131, time: 172.28832\n","Val 0000000067_0000000041.csv Epoch: 38, summary loss: 0.68639, class loss: 0.37260, box_loss: 0.00628, time: 172.29057\n","Val 0000000067_0000000045.csv Epoch: 38, summary loss: 0.42836, class loss: 0.33797, box_loss: 0.00181, time: 172.29267\n","Val 0000000067_0000000046.csv Epoch: 38, summary loss: 0.17724, class loss: 0.12153, box_loss: 0.00111, time: 172.29479\n","Val 0000000067_0000000050.csv Epoch: 38, summary loss: 0.87057, class loss: 0.54331, box_loss: 0.00655, time: 172.29691\n","Val 0000000067_0000000052.csv Epoch: 38, summary loss: 0.73487, class loss: 0.59115, box_loss: 0.00287, time: 172.29908\n","Val 0000000067_0000000055.csv Epoch: 38, summary loss: 0.37814, class loss: 0.33837, box_loss: 0.00080, time: 172.30127\n","Val 0000000067_0000000058.csv Epoch: 38, summary loss: 0.55264, class loss: 0.32821, box_loss: 0.00449, time: 172.30342\n","Val 0000000067_0000000059.csv Epoch: 38, summary loss: 0.47182, class loss: 0.47182, box_loss: 0.00000, time: 172.30572\n","Val 0000000351_0000000000.csv Epoch: 38, summary loss: 0.62445, class loss: 0.39775, box_loss: 0.00453, time: 172.30787\n","Val 0000000354_0000000000.csv Epoch: 38, summary loss: 1.64571, class loss: 0.73548, box_loss: 0.01820, time: 172.30998\n","Val 0000000359_0000000000.csv Epoch: 38, summary loss: 1.17650, class loss: 0.64783, box_loss: 0.01057, time: 172.31206\n","Val 0000000363_0000000000.csv Epoch: 38, summary loss: 0.08549, class loss: 0.08549, box_loss: 0.00000, time: 172.31421\n","Val 0000000364_0000000000.csv Epoch: 38, summary loss: 0.40702, class loss: 0.37098, box_loss: 0.00072, time: 172.31627\n","Val 0000000367_0000000000.csv Epoch: 38, summary loss: 1.05850, class loss: 0.83809, box_loss: 0.00441, time: 172.31840\n","Val Epoch: 38, summary loss: 0.50693, class loss: 0.33614, box_loss: 0.00342, time: 172.32069\n","Adjusting learning rate of group 0 to 1.0940e-04.\n","\n","2021-06-01T16:50:59.080936\n","LR: 0.00010940232918457735\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.74328\n","Train Step 250/3810, summary_loss: 0.31659, class_loss: 0.19785, box_loss: 0.00237, time: 75.85445\n","Train Step 500/3810, summary_loss: 0.32229, class_loss: 0.20049, box_loss: 0.00244, time: 149.94346\n","Train Step 750/3810, summary_loss: 0.32255, class_loss: 0.20177, box_loss: 0.00242, time: 223.29052\n","Train Step 1000/3810, summary_loss: 0.32453, class_loss: 0.20262, box_loss: 0.00244, time: 296.82862\n","Train Step 1250/3810, summary_loss: 0.32338, class_loss: 0.20248, box_loss: 0.00242, time: 370.48960\n","Train Step 1500/3810, summary_loss: 0.32745, class_loss: 0.20595, box_loss: 0.00243, time: 444.60188\n","Train Step 1750/3810, summary_loss: 0.32904, class_loss: 0.20672, box_loss: 0.00245, time: 518.06814\n","Train Step 2000/3810, summary_loss: 0.32780, class_loss: 0.20584, box_loss: 0.00244, time: 591.79345\n","Train Step 2250/3810, summary_loss: 0.32784, class_loss: 0.20590, box_loss: 0.00244, time: 666.27804\n","Train Step 2500/3810, summary_loss: 0.32651, class_loss: 0.20518, box_loss: 0.00243, time: 740.47028\n","Train Step 2750/3810, summary_loss: 0.32824, class_loss: 0.20641, box_loss: 0.00244, time: 815.08394\n","Train Step 3000/3810, summary_loss: 0.32795, class_loss: 0.20589, box_loss: 0.00244, time: 889.81266\n","Train Step 3250/3810, summary_loss: 0.32763, class_loss: 0.20547, box_loss: 0.00244, time: 963.29334\n","Train Step 3500/3810, summary_loss: 0.32723, class_loss: 0.20521, box_loss: 0.00244, time: 1037.37700\n","Train Step 3750/3810, summary_loss: 0.32725, class_loss: 0.20492, box_loss: 0.00245, time: 1111.18715\n","Train Epoch: 39, summary loss: 0.32720, class loss: 0.20492, box_loss: 0.00245, time: 1129.24230\n","Val 0000000010_0000000000.csv Epoch: 39, summary loss: 0.46318, class loss: 0.32774, box_loss: 0.00271, time: 172.65396\n","Val 0000000054_0000000000.csv Epoch: 39, summary loss: 0.59061, class loss: 0.46526, box_loss: 0.00251, time: 172.65704\n","Val 0000000056_0000000000.csv Epoch: 39, summary loss: 0.31376, class loss: 0.23664, box_loss: 0.00154, time: 172.65967\n","Val 0000000062_0000000000.csv Epoch: 39, summary loss: 1.10690, class loss: 0.88290, box_loss: 0.00448, time: 172.66176\n","Val 0000000067_0000000005.csv Epoch: 39, summary loss: 0.01997, class loss: 0.01997, box_loss: 0.00000, time: 172.66385\n","Val 0000000067_0000000012.csv Epoch: 39, summary loss: 0.17259, class loss: 0.17259, box_loss: 0.00000, time: 172.66595\n","Val 0000000067_0000000014.csv Epoch: 39, summary loss: 0.19519, class loss: 0.11874, box_loss: 0.00153, time: 172.66803\n","Val 0000000067_0000000015.csv Epoch: 39, summary loss: 0.53670, class loss: 0.24503, box_loss: 0.00583, time: 172.67009\n","Val 0000000067_0000000019.csv Epoch: 39, summary loss: 0.37409, class loss: 0.19276, box_loss: 0.00363, time: 172.67216\n","Val 0000000067_0000000024.csv Epoch: 39, summary loss: 0.34426, class loss: 0.14655, box_loss: 0.00395, time: 172.67421\n","Val 0000000067_0000000025.csv Epoch: 39, summary loss: 0.36064, class loss: 0.19904, box_loss: 0.00323, time: 172.67635\n","Val 0000000067_0000000026.csv Epoch: 39, summary loss: 0.01902, class loss: 0.01902, box_loss: 0.00000, time: 172.67850\n","Val 0000000067_0000000027.csv Epoch: 39, summary loss: 0.04305, class loss: 0.04305, box_loss: 0.00000, time: 172.68057\n","Val 0000000067_0000000028.csv Epoch: 39, summary loss: 0.08116, class loss: 0.05397, box_loss: 0.00054, time: 172.68266\n","Val 0000000067_0000000029.csv Epoch: 39, summary loss: 0.07668, class loss: 0.07668, box_loss: 0.00000, time: 172.68475\n","Val 0000000067_0000000031.csv Epoch: 39, summary loss: 0.38536, class loss: 0.21417, box_loss: 0.00342, time: 172.68683\n","Val 0000000067_0000000032.csv Epoch: 39, summary loss: 0.49919, class loss: 0.34980, box_loss: 0.00299, time: 172.68889\n","Val 0000000067_0000000040.csv Epoch: 39, summary loss: 0.22890, class loss: 0.16252, box_loss: 0.00133, time: 172.70541\n","Val 0000000067_0000000041.csv Epoch: 39, summary loss: 0.72199, class loss: 0.37341, box_loss: 0.00697, time: 172.70755\n","Val 0000000067_0000000045.csv Epoch: 39, summary loss: 0.51271, class loss: 0.37668, box_loss: 0.00272, time: 172.70963\n","Val 0000000067_0000000046.csv Epoch: 39, summary loss: 0.17963, class loss: 0.11688, box_loss: 0.00126, time: 172.71167\n","Val 0000000067_0000000050.csv Epoch: 39, summary loss: 0.95808, class loss: 0.64891, box_loss: 0.00618, time: 172.71373\n","Val 0000000067_0000000052.csv Epoch: 39, summary loss: 0.76148, class loss: 0.64989, box_loss: 0.00223, time: 172.71573\n","Val 0000000067_0000000055.csv Epoch: 39, summary loss: 0.52113, class loss: 0.46977, box_loss: 0.00103, time: 172.71780\n","Val 0000000067_0000000058.csv Epoch: 39, summary loss: 0.72328, class loss: 0.42972, box_loss: 0.00587, time: 172.71998\n","Val 0000000067_0000000059.csv Epoch: 39, summary loss: 0.60393, class loss: 0.60393, box_loss: 0.00000, time: 172.72210\n","Val 0000000351_0000000000.csv Epoch: 39, summary loss: 0.65881, class loss: 0.45094, box_loss: 0.00416, time: 172.72421\n","Val 0000000354_0000000000.csv Epoch: 39, summary loss: 1.03991, class loss: 0.67449, box_loss: 0.00731, time: 172.72629\n","Val 0000000359_0000000000.csv Epoch: 39, summary loss: 0.75855, class loss: 0.44535, box_loss: 0.00626, time: 172.72837\n","Val 0000000363_0000000000.csv Epoch: 39, summary loss: 0.09048, class loss: 0.09048, box_loss: 0.00000, time: 172.73049\n","Val 0000000364_0000000000.csv Epoch: 39, summary loss: 0.33197, class loss: 0.29899, box_loss: 0.00066, time: 172.73264\n","Val 0000000367_0000000000.csv Epoch: 39, summary loss: 0.65617, class loss: 0.45164, box_loss: 0.00409, time: 172.73471\n","Val Epoch: 39, summary loss: 0.44779, class loss: 0.31273, box_loss: 0.00270, time: 172.73692\n","Adjusting learning rate of group 0 to 1.0772e-04.\n","\n","2021-06-01T17:12:41.383512\n","LR: 0.0001077230228189798\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 1.12816\n","Train Step 250/3810, summary_loss: 0.31832, class_loss: 0.20002, box_loss: 0.00237, time: 75.27230\n","Train Step 500/3810, summary_loss: 0.32310, class_loss: 0.20408, box_loss: 0.00238, time: 148.75256\n","Train Step 750/3810, summary_loss: 0.32394, class_loss: 0.20362, box_loss: 0.00241, time: 223.13377\n","Train Step 1000/3810, summary_loss: 0.32402, class_loss: 0.20328, box_loss: 0.00241, time: 296.45026\n","Train Step 1250/3810, summary_loss: 0.32463, class_loss: 0.20396, box_loss: 0.00241, time: 370.06347\n","Train Step 1500/3810, summary_loss: 0.32610, class_loss: 0.20444, box_loss: 0.00243, time: 443.88176\n","Train Step 1750/3810, summary_loss: 0.32592, class_loss: 0.20435, box_loss: 0.00243, time: 517.63951\n","Train Step 2000/3810, summary_loss: 0.32440, class_loss: 0.20351, box_loss: 0.00242, time: 591.31792\n","Train Step 2250/3810, summary_loss: 0.32383, class_loss: 0.20308, box_loss: 0.00242, time: 665.17014\n","Train Step 2500/3810, summary_loss: 0.32293, class_loss: 0.20271, box_loss: 0.00240, time: 739.30351\n","Train Step 2750/3810, summary_loss: 0.32289, class_loss: 0.20268, box_loss: 0.00240, time: 813.89230\n","Train Step 3000/3810, summary_loss: 0.32407, class_loss: 0.20334, box_loss: 0.00241, time: 888.07293\n","Train Step 3250/3810, summary_loss: 0.32476, class_loss: 0.20380, box_loss: 0.00242, time: 961.46037\n","Train Step 3500/3810, summary_loss: 0.32515, class_loss: 0.20398, box_loss: 0.00242, time: 1036.19779\n","Train Step 3750/3810, summary_loss: 0.32391, class_loss: 0.20328, box_loss: 0.00241, time: 1110.19420\n","Train Epoch: 40, summary loss: 0.32424, class loss: 0.20350, box_loss: 0.00241, time: 1127.88981\n","Val 0000000010_0000000000.csv Epoch: 40, summary loss: 0.63350, class loss: 0.44217, box_loss: 0.00383, time: 172.69924\n","Val 0000000054_0000000000.csv Epoch: 40, summary loss: 0.50573, class loss: 0.40333, box_loss: 0.00205, time: 172.70275\n","Val 0000000056_0000000000.csv Epoch: 40, summary loss: 0.31557, class loss: 0.23468, box_loss: 0.00162, time: 172.70507\n","Val 0000000062_0000000000.csv Epoch: 40, summary loss: 0.87301, class loss: 0.61222, box_loss: 0.00522, time: 172.70702\n","Val 0000000067_0000000005.csv Epoch: 40, summary loss: 0.02818, class loss: 0.02818, box_loss: 0.00000, time: 172.70899\n","Val 0000000067_0000000012.csv Epoch: 40, summary loss: 0.42635, class loss: 0.42635, box_loss: 0.00000, time: 172.71093\n","Val 0000000067_0000000014.csv Epoch: 40, summary loss: 0.20426, class loss: 0.15219, box_loss: 0.00104, time: 172.71284\n","Val 0000000067_0000000015.csv Epoch: 40, summary loss: 0.50279, class loss: 0.26343, box_loss: 0.00479, time: 172.71474\n","Val 0000000067_0000000019.csv Epoch: 40, summary loss: 0.45234, class loss: 0.28474, box_loss: 0.00335, time: 172.71665\n","Val 0000000067_0000000024.csv Epoch: 40, summary loss: 0.35748, class loss: 0.14453, box_loss: 0.00426, time: 172.71856\n","Val 0000000067_0000000025.csv Epoch: 40, summary loss: 0.41720, class loss: 0.19686, box_loss: 0.00441, time: 172.72046\n","Val 0000000067_0000000026.csv Epoch: 40, summary loss: 0.03480, class loss: 0.03480, box_loss: 0.00000, time: 172.72239\n","Val 0000000067_0000000027.csv Epoch: 40, summary loss: 0.18902, class loss: 0.18902, box_loss: 0.00000, time: 172.72427\n","Val 0000000067_0000000028.csv Epoch: 40, summary loss: 0.18090, class loss: 0.15044, box_loss: 0.00061, time: 172.72618\n","Val 0000000067_0000000029.csv Epoch: 40, summary loss: 0.13562, class loss: 0.13562, box_loss: 0.00000, time: 172.72808\n","Val 0000000067_0000000031.csv Epoch: 40, summary loss: 0.37477, class loss: 0.21860, box_loss: 0.00312, time: 172.73000\n","Val 0000000067_0000000032.csv Epoch: 40, summary loss: 0.63308, class loss: 0.47896, box_loss: 0.00308, time: 172.73191\n","Val 0000000067_0000000040.csv Epoch: 40, summary loss: 0.38182, class loss: 0.30472, box_loss: 0.00154, time: 172.73381\n","Val 0000000067_0000000041.csv Epoch: 40, summary loss: 0.79348, class loss: 0.41048, box_loss: 0.00766, time: 172.73599\n","Val 0000000067_0000000045.csv Epoch: 40, summary loss: 0.54428, class loss: 0.41544, box_loss: 0.00258, time: 172.73811\n","Val 0000000067_0000000046.csv Epoch: 40, summary loss: 0.17982, class loss: 0.11814, box_loss: 0.00123, time: 172.74027\n","Val 0000000067_0000000050.csv Epoch: 40, summary loss: 0.89433, class loss: 0.63373, box_loss: 0.00521, time: 172.74243\n","Val 0000000067_0000000052.csv Epoch: 40, summary loss: 0.85015, class loss: 0.74011, box_loss: 0.00220, time: 172.74457\n","Val 0000000067_0000000055.csv Epoch: 40, summary loss: 0.78557, class loss: 0.73703, box_loss: 0.00097, time: 172.74675\n","Val 0000000067_0000000058.csv Epoch: 40, summary loss: 0.84933, class loss: 0.57037, box_loss: 0.00558, time: 172.74891\n","Val 0000000067_0000000059.csv Epoch: 40, summary loss: 0.84629, class loss: 0.84629, box_loss: 0.00000, time: 172.77201\n","Val 0000000351_0000000000.csv Epoch: 40, summary loss: 0.50493, class loss: 0.30065, box_loss: 0.00409, time: 172.77429\n","Val 0000000354_0000000000.csv Epoch: 40, summary loss: 1.07784, class loss: 0.63919, box_loss: 0.00877, time: 172.77641\n","Val 0000000359_0000000000.csv Epoch: 40, summary loss: 0.99223, class loss: 0.54225, box_loss: 0.00900, time: 172.77852\n","Val 0000000363_0000000000.csv Epoch: 40, summary loss: 0.05874, class loss: 0.05874, box_loss: 0.00000, time: 172.78063\n","Val 0000000364_0000000000.csv Epoch: 40, summary loss: 0.59542, class loss: 0.56762, box_loss: 0.00056, time: 172.78277\n","Val 0000000367_0000000000.csv Epoch: 40, summary loss: 0.84324, class loss: 0.63837, box_loss: 0.00410, time: 172.78486\n","Val Epoch: 40, summary loss: 0.51444, class loss: 0.37248, box_loss: 0.00284, time: 172.78713\n","Adjusting learning rate of group 0 to 1.0607e-04.\n","\n","2021-06-01T17:34:22.365813\n","LR: 0.00010606949350850124\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.81150\n","Train Step 250/3810, summary_loss: 0.32487, class_loss: 0.20234, box_loss: 0.00245, time: 75.52284\n","Train Step 500/3810, summary_loss: 0.32361, class_loss: 0.20117, box_loss: 0.00245, time: 149.60922\n","Train Step 750/3810, summary_loss: 0.32366, class_loss: 0.20169, box_loss: 0.00244, time: 223.43186\n","Train Step 1000/3810, summary_loss: 0.32077, class_loss: 0.20053, box_loss: 0.00240, time: 296.77552\n","Train Step 1250/3810, summary_loss: 0.32750, class_loss: 0.20595, box_loss: 0.00243, time: 370.45837\n","Train Step 1500/3810, summary_loss: 0.32616, class_loss: 0.20513, box_loss: 0.00242, time: 443.68695\n","Train Step 1750/3810, summary_loss: 0.33077, class_loss: 0.20959, box_loss: 0.00242, time: 517.93056\n","Train Step 2000/3810, summary_loss: 0.33011, class_loss: 0.20917, box_loss: 0.00242, time: 592.05987\n","Train Step 2250/3810, summary_loss: 0.32975, class_loss: 0.20899, box_loss: 0.00242, time: 666.60477\n","Train Step 2500/3810, summary_loss: 0.32829, class_loss: 0.20830, box_loss: 0.00240, time: 740.69549\n","Train Step 2750/3810, summary_loss: 0.32754, class_loss: 0.20748, box_loss: 0.00240, time: 814.46846\n","Train Step 3000/3810, summary_loss: 0.32633, class_loss: 0.20655, box_loss: 0.00240, time: 887.97478\n","Train Step 3250/3810, summary_loss: 0.32587, class_loss: 0.20626, box_loss: 0.00239, time: 962.24927\n","Train Step 3500/3810, summary_loss: 0.32493, class_loss: 0.20564, box_loss: 0.00239, time: 1036.08320\n","Train Step 3750/3810, summary_loss: 0.32391, class_loss: 0.20480, box_loss: 0.00238, time: 1110.07891\n","Train Epoch: 41, summary loss: 0.32392, class loss: 0.20474, box_loss: 0.00238, time: 1127.82996\n","Val 0000000010_0000000000.csv Epoch: 41, summary loss: 0.46913, class loss: 0.31946, box_loss: 0.00299, time: 173.05182\n","Val 0000000054_0000000000.csv Epoch: 41, summary loss: 0.47808, class loss: 0.36941, box_loss: 0.00217, time: 173.05514\n","Val 0000000056_0000000000.csv Epoch: 41, summary loss: 0.30581, class loss: 0.22565, box_loss: 0.00160, time: 173.05765\n","Val 0000000062_0000000000.csv Epoch: 41, summary loss: 0.78782, class loss: 0.56715, box_loss: 0.00441, time: 173.05976\n","Val 0000000067_0000000005.csv Epoch: 41, summary loss: 0.01644, class loss: 0.01644, box_loss: 0.00000, time: 173.06194\n","Val 0000000067_0000000012.csv Epoch: 41, summary loss: 0.11384, class loss: 0.11384, box_loss: 0.00000, time: 173.06411\n","Val 0000000067_0000000014.csv Epoch: 41, summary loss: 0.15829, class loss: 0.09438, box_loss: 0.00128, time: 173.06630\n","Val 0000000067_0000000015.csv Epoch: 41, summary loss: 0.39682, class loss: 0.21202, box_loss: 0.00370, time: 173.08365\n","Val 0000000067_0000000019.csv Epoch: 41, summary loss: 0.37658, class loss: 0.19372, box_loss: 0.00366, time: 173.08769\n","Val 0000000067_0000000024.csv Epoch: 41, summary loss: 0.29615, class loss: 0.15989, box_loss: 0.00273, time: 173.08987\n","Val 0000000067_0000000025.csv Epoch: 41, summary loss: 0.32751, class loss: 0.19313, box_loss: 0.00269, time: 173.09192\n","Val 0000000067_0000000026.csv Epoch: 41, summary loss: 0.01628, class loss: 0.01628, box_loss: 0.00000, time: 173.09401\n","Val 0000000067_0000000027.csv Epoch: 41, summary loss: 0.03165, class loss: 0.03165, box_loss: 0.00000, time: 173.09607\n","Val 0000000067_0000000028.csv Epoch: 41, summary loss: 0.07576, class loss: 0.05438, box_loss: 0.00043, time: 173.09820\n","Val 0000000067_0000000029.csv Epoch: 41, summary loss: 0.03464, class loss: 0.03464, box_loss: 0.00000, time: 173.10030\n","Val 0000000067_0000000031.csv Epoch: 41, summary loss: 0.28629, class loss: 0.18008, box_loss: 0.00212, time: 173.10240\n","Val 0000000067_0000000032.csv Epoch: 41, summary loss: 0.40030, class loss: 0.29103, box_loss: 0.00219, time: 173.10450\n","Val 0000000067_0000000040.csv Epoch: 41, summary loss: 0.21545, class loss: 0.15586, box_loss: 0.00119, time: 173.10657\n","Val 0000000067_0000000041.csv Epoch: 41, summary loss: 0.68476, class loss: 0.37911, box_loss: 0.00611, time: 173.10863\n","Val 0000000067_0000000045.csv Epoch: 41, summary loss: 0.36143, class loss: 0.28791, box_loss: 0.00147, time: 173.11071\n","Val 0000000067_0000000046.csv Epoch: 41, summary loss: 0.16205, class loss: 0.10302, box_loss: 0.00118, time: 173.11289\n","Val 0000000067_0000000050.csv Epoch: 41, summary loss: 0.83650, class loss: 0.54494, box_loss: 0.00583, time: 173.11512\n","Val 0000000067_0000000052.csv Epoch: 41, summary loss: 0.50877, class loss: 0.41020, box_loss: 0.00197, time: 173.11723\n","Val 0000000067_0000000055.csv Epoch: 41, summary loss: 0.29112, class loss: 0.24843, box_loss: 0.00085, time: 173.11927\n","Val 0000000067_0000000058.csv Epoch: 41, summary loss: 0.53343, class loss: 0.34235, box_loss: 0.00382, time: 173.12134\n","Val 0000000067_0000000059.csv Epoch: 41, summary loss: 0.43647, class loss: 0.43647, box_loss: 0.00000, time: 173.12339\n","Val 0000000351_0000000000.csv Epoch: 41, summary loss: 0.61056, class loss: 0.39414, box_loss: 0.00433, time: 173.12545\n","Val 0000000354_0000000000.csv Epoch: 41, summary loss: 1.03228, class loss: 0.55840, box_loss: 0.00948, time: 173.12750\n","Val 0000000359_0000000000.csv Epoch: 41, summary loss: 1.05941, class loss: 0.56791, box_loss: 0.00983, time: 173.12955\n","Val 0000000363_0000000000.csv Epoch: 41, summary loss: 0.14152, class loss: 0.14152, box_loss: 0.00000, time: 173.13161\n","Val 0000000364_0000000000.csv Epoch: 41, summary loss: 0.36434, class loss: 0.33883, box_loss: 0.00051, time: 173.13365\n","Val 0000000367_0000000000.csv Epoch: 41, summary loss: 0.71903, class loss: 0.50606, box_loss: 0.00426, time: 173.13570\n","Val Epoch: 41, summary loss: 0.39152, class loss: 0.26526, box_loss: 0.00253, time: 173.13792\n","Adjusting learning rate of group 0 to 1.0444e-04.\n","\n","2021-06-01T17:56:03.929403\n","LR: 0.00010444134557991359\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.89045\n","Train Step 250/3810, summary_loss: 0.31326, class_loss: 0.19762, box_loss: 0.00231, time: 74.86301\n","Train Step 500/3810, summary_loss: 0.31432, class_loss: 0.19782, box_loss: 0.00233, time: 148.62982\n","Train Step 750/3810, summary_loss: 0.31519, class_loss: 0.19956, box_loss: 0.00231, time: 222.03748\n","Train Step 1000/3810, summary_loss: 0.32160, class_loss: 0.20335, box_loss: 0.00237, time: 295.95442\n","Train Step 1250/3810, summary_loss: 0.32069, class_loss: 0.20194, box_loss: 0.00238, time: 369.91167\n","Train Step 1500/3810, summary_loss: 0.32153, class_loss: 0.20232, box_loss: 0.00238, time: 444.29982\n","Train Step 1750/3810, summary_loss: 0.32174, class_loss: 0.20232, box_loss: 0.00239, time: 518.36547\n","Train Step 2000/3810, summary_loss: 0.32440, class_loss: 0.20414, box_loss: 0.00241, time: 592.14628\n","Train Step 2250/3810, summary_loss: 0.32361, class_loss: 0.20333, box_loss: 0.00241, time: 666.19010\n","Train Step 2500/3810, summary_loss: 0.32246, class_loss: 0.20275, box_loss: 0.00239, time: 740.01701\n","Train Step 2750/3810, summary_loss: 0.32410, class_loss: 0.20348, box_loss: 0.00241, time: 814.74501\n","Train Step 3000/3810, summary_loss: 0.32376, class_loss: 0.20316, box_loss: 0.00241, time: 888.37115\n","Train Step 3250/3810, summary_loss: 0.32390, class_loss: 0.20361, box_loss: 0.00241, time: 961.85852\n","Train Step 3500/3810, summary_loss: 0.32402, class_loss: 0.20381, box_loss: 0.00240, time: 1035.81525\n","Train Step 3750/3810, summary_loss: 0.32356, class_loss: 0.20376, box_loss: 0.00240, time: 1109.28332\n","Train Epoch: 42, summary loss: 0.32378, class loss: 0.20391, box_loss: 0.00240, time: 1126.75273\n","Val 0000000010_0000000000.csv Epoch: 42, summary loss: 0.98637, class loss: 0.67168, box_loss: 0.00629, time: 172.02676\n","Val 0000000054_0000000000.csv Epoch: 42, summary loss: 0.58659, class loss: 0.43529, box_loss: 0.00303, time: 172.02964\n","Val 0000000056_0000000000.csv Epoch: 42, summary loss: 0.39562, class loss: 0.29739, box_loss: 0.00196, time: 172.03198\n","Val 0000000062_0000000000.csv Epoch: 42, summary loss: 0.62752, class loss: 0.41280, box_loss: 0.00429, time: 172.03407\n","Val 0000000067_0000000005.csv Epoch: 42, summary loss: 0.00951, class loss: 0.00951, box_loss: 0.00000, time: 172.03615\n","Val 0000000067_0000000012.csv Epoch: 42, summary loss: 0.17737, class loss: 0.17737, box_loss: 0.00000, time: 172.03828\n","Val 0000000067_0000000014.csv Epoch: 42, summary loss: 0.16928, class loss: 0.11036, box_loss: 0.00118, time: 172.04040\n","Val 0000000067_0000000015.csv Epoch: 42, summary loss: 0.42879, class loss: 0.22087, box_loss: 0.00416, time: 172.04251\n","Val 0000000067_0000000019.csv Epoch: 42, summary loss: 0.38700, class loss: 0.19141, box_loss: 0.00391, time: 172.04458\n","Val 0000000067_0000000024.csv Epoch: 42, summary loss: 0.31294, class loss: 0.13690, box_loss: 0.00352, time: 172.04669\n","Val 0000000067_0000000025.csv Epoch: 42, summary loss: 0.32027, class loss: 0.18763, box_loss: 0.00265, time: 172.04879\n","Val 0000000067_0000000026.csv Epoch: 42, summary loss: 0.02566, class loss: 0.02566, box_loss: 0.00000, time: 172.05086\n","Val 0000000067_0000000027.csv Epoch: 42, summary loss: 0.04359, class loss: 0.04359, box_loss: 0.00000, time: 172.05291\n","Val 0000000067_0000000028.csv Epoch: 42, summary loss: 0.08446, class loss: 0.06132, box_loss: 0.00046, time: 172.05499\n","Val 0000000067_0000000029.csv Epoch: 42, summary loss: 0.04605, class loss: 0.04605, box_loss: 0.00000, time: 172.05707\n","Val 0000000067_0000000031.csv Epoch: 42, summary loss: 0.31858, class loss: 0.20087, box_loss: 0.00235, time: 172.05921\n","Val 0000000067_0000000032.csv Epoch: 42, summary loss: 0.46254, class loss: 0.33100, box_loss: 0.00263, time: 172.06128\n","Val 0000000067_0000000040.csv Epoch: 42, summary loss: 0.22638, class loss: 0.16771, box_loss: 0.00117, time: 172.08624\n","Val 0000000067_0000000041.csv Epoch: 42, summary loss: 0.64152, class loss: 0.34708, box_loss: 0.00589, time: 172.08831\n","Val 0000000067_0000000045.csv Epoch: 42, summary loss: 0.33020, class loss: 0.26070, box_loss: 0.00139, time: 172.09037\n","Val 0000000067_0000000046.csv Epoch: 42, summary loss: 0.15693, class loss: 0.10100, box_loss: 0.00112, time: 172.09242\n","Val 0000000067_0000000050.csv Epoch: 42, summary loss: 0.86100, class loss: 0.54623, box_loss: 0.00630, time: 172.09461\n","Val 0000000067_0000000052.csv Epoch: 42, summary loss: 0.64344, class loss: 0.53686, box_loss: 0.00213, time: 172.09669\n","Val 0000000067_0000000055.csv Epoch: 42, summary loss: 0.38314, class loss: 0.34605, box_loss: 0.00074, time: 172.09890\n","Val 0000000067_0000000058.csv Epoch: 42, summary loss: 0.60948, class loss: 0.40361, box_loss: 0.00412, time: 172.10095\n","Val 0000000067_0000000059.csv Epoch: 42, summary loss: 0.55511, class loss: 0.55511, box_loss: 0.00000, time: 172.10299\n","Val 0000000351_0000000000.csv Epoch: 42, summary loss: 0.54166, class loss: 0.33870, box_loss: 0.00406, time: 172.10528\n","Val 0000000354_0000000000.csv Epoch: 42, summary loss: 1.83513, class loss: 1.42923, box_loss: 0.00812, time: 172.10754\n","Val 0000000359_0000000000.csv Epoch: 42, summary loss: 0.78983, class loss: 0.39795, box_loss: 0.00784, time: 172.10973\n","Val 0000000363_0000000000.csv Epoch: 42, summary loss: 0.24638, class loss: 0.24638, box_loss: 0.00000, time: 172.11182\n","Val 0000000364_0000000000.csv Epoch: 42, summary loss: 0.80261, class loss: 0.77748, box_loss: 0.00050, time: 172.11391\n","Val 0000000367_0000000000.csv Epoch: 42, summary loss: 0.75160, class loss: 0.52546, box_loss: 0.00452, time: 172.11601\n","Val Epoch: 42, summary loss: 0.46114, class loss: 0.32935, box_loss: 0.00264, time: 172.11840\n","Adjusting learning rate of group 0 to 1.0284e-04.\n","\n","2021-06-01T18:17:43.094490\n","LR: 0.0001028381894335027\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.79029\n","Train Step 250/3810, summary_loss: 0.30441, class_loss: 0.19295, box_loss: 0.00223, time: 74.99339\n","Train Step 500/3810, summary_loss: 0.31040, class_loss: 0.19655, box_loss: 0.00228, time: 148.98897\n","Train Step 750/3810, summary_loss: 0.31275, class_loss: 0.19757, box_loss: 0.00230, time: 222.88020\n","Train Step 1000/3810, summary_loss: 0.31532, class_loss: 0.19848, box_loss: 0.00234, time: 296.73617\n","Train Step 1250/3810, summary_loss: 0.31324, class_loss: 0.19761, box_loss: 0.00231, time: 370.52572\n","Train Step 1500/3810, summary_loss: 0.31625, class_loss: 0.20007, box_loss: 0.00232, time: 444.27742\n","Train Step 1750/3810, summary_loss: 0.31791, class_loss: 0.20102, box_loss: 0.00234, time: 517.81520\n","Train Step 2000/3810, summary_loss: 0.31672, class_loss: 0.20021, box_loss: 0.00233, time: 592.03387\n","Train Step 2250/3810, summary_loss: 0.31531, class_loss: 0.19942, box_loss: 0.00232, time: 665.73189\n","Train Step 2500/3810, summary_loss: 0.31529, class_loss: 0.19919, box_loss: 0.00232, time: 739.37166\n","Train Step 2750/3810, summary_loss: 0.31616, class_loss: 0.19944, box_loss: 0.00233, time: 813.00933\n","Train Step 3000/3810, summary_loss: 0.31635, class_loss: 0.19938, box_loss: 0.00234, time: 886.67591\n","Train Step 3250/3810, summary_loss: 0.31605, class_loss: 0.19912, box_loss: 0.00234, time: 960.47863\n","Train Step 3500/3810, summary_loss: 0.31611, class_loss: 0.19905, box_loss: 0.00234, time: 1034.46117\n","Train Step 3750/3810, summary_loss: 0.31632, class_loss: 0.19916, box_loss: 0.00234, time: 1108.38419\n","Train Epoch: 43, summary loss: 0.31634, class loss: 0.19911, box_loss: 0.00234, time: 1125.90641\n","Val 0000000010_0000000000.csv Epoch: 43, summary loss: 0.53326, class loss: 0.40013, box_loss: 0.00266, time: 172.27327\n","Val 0000000054_0000000000.csv Epoch: 43, summary loss: 0.47416, class loss: 0.36907, box_loss: 0.00210, time: 172.27661\n","Val 0000000056_0000000000.csv Epoch: 43, summary loss: 0.32702, class loss: 0.22333, box_loss: 0.00207, time: 172.27913\n","Val 0000000062_0000000000.csv Epoch: 43, summary loss: 0.74616, class loss: 0.54071, box_loss: 0.00411, time: 172.28122\n","Val 0000000067_0000000005.csv Epoch: 43, summary loss: 0.01873, class loss: 0.01873, box_loss: 0.00000, time: 172.28340\n","Val 0000000067_0000000012.csv Epoch: 43, summary loss: 0.08990, class loss: 0.08990, box_loss: 0.00000, time: 172.28556\n","Val 0000000067_0000000014.csv Epoch: 43, summary loss: 0.15898, class loss: 0.10206, box_loss: 0.00114, time: 172.28771\n","Val 0000000067_0000000015.csv Epoch: 43, summary loss: 0.42786, class loss: 0.20902, box_loss: 0.00438, time: 172.28986\n","Val 0000000067_0000000019.csv Epoch: 43, summary loss: 0.37407, class loss: 0.20116, box_loss: 0.00346, time: 172.29193\n","Val 0000000067_0000000024.csv Epoch: 43, summary loss: 0.37333, class loss: 0.17090, box_loss: 0.00405, time: 172.29408\n","Val 0000000067_0000000025.csv Epoch: 43, summary loss: 0.38975, class loss: 0.20708, box_loss: 0.00365, time: 172.29621\n","Val 0000000067_0000000026.csv Epoch: 43, summary loss: 0.01791, class loss: 0.01791, box_loss: 0.00000, time: 172.29836\n","Val 0000000067_0000000027.csv Epoch: 43, summary loss: 0.03161, class loss: 0.03161, box_loss: 0.00000, time: 172.30051\n","Val 0000000067_0000000028.csv Epoch: 43, summary loss: 0.07384, class loss: 0.04850, box_loss: 0.00051, time: 172.30292\n","Val 0000000067_0000000029.csv Epoch: 43, summary loss: 0.04746, class loss: 0.04746, box_loss: 0.00000, time: 172.30503\n","Val 0000000067_0000000031.csv Epoch: 43, summary loss: 0.33591, class loss: 0.19338, box_loss: 0.00285, time: 172.30716\n","Val 0000000067_0000000032.csv Epoch: 43, summary loss: 0.38216, class loss: 0.26130, box_loss: 0.00242, time: 172.30928\n","Val 0000000067_0000000040.csv Epoch: 43, summary loss: 0.19168, class loss: 0.12677, box_loss: 0.00130, time: 172.31143\n","Val 0000000067_0000000041.csv Epoch: 43, summary loss: 0.74130, class loss: 0.39998, box_loss: 0.00683, time: 172.31359\n","Val 0000000067_0000000045.csv Epoch: 43, summary loss: 0.33160, class loss: 0.23177, box_loss: 0.00200, time: 172.31572\n","Val 0000000067_0000000046.csv Epoch: 43, summary loss: 0.18411, class loss: 0.11341, box_loss: 0.00141, time: 172.31786\n","Val 0000000067_0000000050.csv Epoch: 43, summary loss: 0.81701, class loss: 0.52224, box_loss: 0.00590, time: 172.31999\n","Val 0000000067_0000000052.csv Epoch: 43, summary loss: 0.56193, class loss: 0.43613, box_loss: 0.00252, time: 172.32210\n","Val 0000000067_0000000055.csv Epoch: 43, summary loss: 0.30052, class loss: 0.24299, box_loss: 0.00115, time: 172.32423\n","Val 0000000067_0000000058.csv Epoch: 43, summary loss: 0.61256, class loss: 0.36548, box_loss: 0.00494, time: 172.32633\n","Val 0000000067_0000000059.csv Epoch: 43, summary loss: 0.35241, class loss: 0.35241, box_loss: 0.00000, time: 172.32846\n","Val 0000000351_0000000000.csv Epoch: 43, summary loss: 0.64052, class loss: 0.42416, box_loss: 0.00433, time: 172.33065\n","Val 0000000354_0000000000.csv Epoch: 43, summary loss: 0.96748, class loss: 0.61443, box_loss: 0.00706, time: 172.33280\n","Val 0000000359_0000000000.csv Epoch: 43, summary loss: 0.91997, class loss: 0.48178, box_loss: 0.00876, time: 172.33495\n","Val 0000000363_0000000000.csv Epoch: 43, summary loss: 0.15558, class loss: 0.15558, box_loss: 0.00000, time: 172.33706\n","Val 0000000364_0000000000.csv Epoch: 43, summary loss: 0.39916, class loss: 0.37662, box_loss: 0.00045, time: 172.35969\n","Val 0000000367_0000000000.csv Epoch: 43, summary loss: 0.67182, class loss: 0.48850, box_loss: 0.00367, time: 172.36184\n","Val Epoch: 43, summary loss: 0.39530, class loss: 0.26452, box_loss: 0.00262, time: 172.36401\n","Adjusting learning rate of group 0 to 1.0126e-04.\n","\n","2021-06-01T18:39:21.676217\n","LR: 0.00010125964144984101\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.86158\n","Train Step 250/3810, summary_loss: 0.31555, class_loss: 0.19578, box_loss: 0.00240, time: 75.11851\n","Train Step 500/3810, summary_loss: 0.30924, class_loss: 0.19237, box_loss: 0.00234, time: 148.90264\n","Train Step 750/3810, summary_loss: 0.31289, class_loss: 0.19498, box_loss: 0.00236, time: 222.82714\n","Train Step 1000/3810, summary_loss: 0.32028, class_loss: 0.20222, box_loss: 0.00236, time: 296.67718\n","Train Step 1250/3810, summary_loss: 0.31981, class_loss: 0.20257, box_loss: 0.00234, time: 370.54625\n","Train Step 1500/3810, summary_loss: 0.31692, class_loss: 0.20066, box_loss: 0.00233, time: 444.28207\n","Train Step 1750/3810, summary_loss: 0.31681, class_loss: 0.20048, box_loss: 0.00233, time: 518.00506\n","Train Step 2000/3810, summary_loss: 0.31820, class_loss: 0.20133, box_loss: 0.00234, time: 592.18803\n","Train Step 2250/3810, summary_loss: 0.31965, class_loss: 0.20235, box_loss: 0.00235, time: 665.51324\n","Train Step 2500/3810, summary_loss: 0.31960, class_loss: 0.20225, box_loss: 0.00235, time: 739.27141\n","Train Step 2750/3810, summary_loss: 0.32083, class_loss: 0.20324, box_loss: 0.00235, time: 812.99793\n","Train Step 3000/3810, summary_loss: 0.32081, class_loss: 0.20324, box_loss: 0.00235, time: 887.00604\n","Train Step 3250/3810, summary_loss: 0.32118, class_loss: 0.20356, box_loss: 0.00235, time: 960.77706\n","Train Step 3500/3810, summary_loss: 0.32048, class_loss: 0.20294, box_loss: 0.00235, time: 1034.28829\n","Train Step 3750/3810, summary_loss: 0.32034, class_loss: 0.20295, box_loss: 0.00235, time: 1108.21672\n","Train Epoch: 44, summary loss: 0.32029, class loss: 0.20286, box_loss: 0.00235, time: 1125.79560\n","Val 0000000010_0000000000.csv Epoch: 44, summary loss: 1.15800, class loss: 0.64384, box_loss: 0.01028, time: 174.52646\n","Val 0000000054_0000000000.csv Epoch: 44, summary loss: 0.79507, class loss: 0.69461, box_loss: 0.00201, time: 174.52999\n","Val 0000000056_0000000000.csv Epoch: 44, summary loss: 0.43390, class loss: 0.29239, box_loss: 0.00283, time: 174.53250\n","Val 0000000062_0000000000.csv Epoch: 44, summary loss: 0.77927, class loss: 0.45311, box_loss: 0.00652, time: 174.53462\n","Val 0000000067_0000000005.csv Epoch: 44, summary loss: 0.01598, class loss: 0.01598, box_loss: 0.00000, time: 174.53674\n","Val 0000000067_0000000012.csv Epoch: 44, summary loss: 0.33042, class loss: 0.33042, box_loss: 0.00000, time: 174.53889\n","Val 0000000067_0000000014.csv Epoch: 44, summary loss: 0.13881, class loss: 0.10435, box_loss: 0.00069, time: 174.54098\n","Val 0000000067_0000000015.csv Epoch: 44, summary loss: 0.40975, class loss: 0.22063, box_loss: 0.00378, time: 174.54305\n","Val 0000000067_0000000019.csv Epoch: 44, summary loss: 0.40894, class loss: 0.21331, box_loss: 0.00391, time: 174.54506\n","Val 0000000067_0000000024.csv Epoch: 44, summary loss: 0.32258, class loss: 0.13992, box_loss: 0.00365, time: 174.54711\n","Val 0000000067_0000000025.csv Epoch: 44, summary loss: 0.39907, class loss: 0.22217, box_loss: 0.00354, time: 174.56318\n","Val 0000000067_0000000026.csv Epoch: 44, summary loss: 0.01364, class loss: 0.01364, box_loss: 0.00000, time: 174.56686\n","Val 0000000067_0000000027.csv Epoch: 44, summary loss: 0.05705, class loss: 0.05705, box_loss: 0.00000, time: 174.56891\n","Val 0000000067_0000000028.csv Epoch: 44, summary loss: 0.07743, class loss: 0.05476, box_loss: 0.00045, time: 174.57101\n","Val 0000000067_0000000029.csv Epoch: 44, summary loss: 0.03898, class loss: 0.03898, box_loss: 0.00000, time: 174.57301\n","Val 0000000067_0000000031.csv Epoch: 44, summary loss: 0.32187, class loss: 0.18558, box_loss: 0.00273, time: 174.57501\n","Val 0000000067_0000000032.csv Epoch: 44, summary loss: 0.45279, class loss: 0.32231, box_loss: 0.00261, time: 174.57700\n","Val 0000000067_0000000040.csv Epoch: 44, summary loss: 0.49523, class loss: 0.42804, box_loss: 0.00134, time: 174.57902\n","Val 0000000067_0000000041.csv Epoch: 44, summary loss: 0.72064, class loss: 0.37631, box_loss: 0.00689, time: 174.58105\n","Val 0000000067_0000000045.csv Epoch: 44, summary loss: 0.37507, class loss: 0.28180, box_loss: 0.00187, time: 174.58305\n","Val 0000000067_0000000046.csv Epoch: 44, summary loss: 0.16091, class loss: 0.10404, box_loss: 0.00114, time: 174.58504\n","Val 0000000067_0000000050.csv Epoch: 44, summary loss: 0.89728, class loss: 0.60625, box_loss: 0.00582, time: 174.58705\n","Val 0000000067_0000000052.csv Epoch: 44, summary loss: 0.69068, class loss: 0.57675, box_loss: 0.00228, time: 174.58904\n","Val 0000000067_0000000055.csv Epoch: 44, summary loss: 0.36274, class loss: 0.31953, box_loss: 0.00086, time: 174.59103\n","Val 0000000067_0000000058.csv Epoch: 44, summary loss: 0.62252, class loss: 0.40155, box_loss: 0.00442, time: 174.59303\n","Val 0000000067_0000000059.csv Epoch: 44, summary loss: 0.51603, class loss: 0.51603, box_loss: 0.00000, time: 174.59501\n","Val 0000000351_0000000000.csv Epoch: 44, summary loss: 0.49302, class loss: 0.29561, box_loss: 0.00395, time: 174.59700\n","Val 0000000354_0000000000.csv Epoch: 44, summary loss: 1.23812, class loss: 0.64656, box_loss: 0.01183, time: 174.59901\n","Val 0000000359_0000000000.csv Epoch: 44, summary loss: 0.98399, class loss: 0.53797, box_loss: 0.00892, time: 174.60101\n","Val 0000000363_0000000000.csv Epoch: 44, summary loss: 0.13199, class loss: 0.13199, box_loss: 0.00000, time: 174.60300\n","Val 0000000364_0000000000.csv Epoch: 44, summary loss: 0.49034, class loss: 0.46479, box_loss: 0.00051, time: 174.60501\n","Val 0000000367_0000000000.csv Epoch: 44, summary loss: 0.71446, class loss: 0.47870, box_loss: 0.00472, time: 174.60701\n","Val Epoch: 44, summary loss: 0.47020, class loss: 0.31778, box_loss: 0.00305, time: 174.60914\n","Adjusting learning rate of group 0 to 9.9705e-05.\n","\n","2021-06-01T19:01:02.394494\n","LR: 9.970532389799116e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.87745\n","Train Step 250/3810, summary_loss: 0.30705, class_loss: 0.19470, box_loss: 0.00225, time: 75.49983\n","Train Step 500/3810, summary_loss: 0.30815, class_loss: 0.19467, box_loss: 0.00227, time: 149.58654\n","Train Step 750/3810, summary_loss: 0.31604, class_loss: 0.19891, box_loss: 0.00234, time: 223.21374\n","Train Step 1000/3810, summary_loss: 0.31807, class_loss: 0.19961, box_loss: 0.00237, time: 296.76752\n","Train Step 1250/3810, summary_loss: 0.31515, class_loss: 0.19778, box_loss: 0.00235, time: 370.40506\n","Train Step 1500/3810, summary_loss: 0.31607, class_loss: 0.19886, box_loss: 0.00234, time: 444.73390\n","Train Step 1750/3810, summary_loss: 0.31683, class_loss: 0.19987, box_loss: 0.00234, time: 518.81311\n","Train Step 2000/3810, summary_loss: 0.31641, class_loss: 0.19972, box_loss: 0.00233, time: 592.41442\n","Train Step 2250/3810, summary_loss: 0.31629, class_loss: 0.19965, box_loss: 0.00233, time: 666.20045\n","Train Step 2500/3810, summary_loss: 0.31531, class_loss: 0.19889, box_loss: 0.00233, time: 740.24081\n","Train Step 2750/3810, summary_loss: 0.31621, class_loss: 0.19952, box_loss: 0.00233, time: 813.70258\n","Train Step 3000/3810, summary_loss: 0.31584, class_loss: 0.19949, box_loss: 0.00233, time: 887.84019\n","Train Step 3250/3810, summary_loss: 0.31529, class_loss: 0.19904, box_loss: 0.00233, time: 961.69204\n","Train Step 3500/3810, summary_loss: 0.31655, class_loss: 0.19995, box_loss: 0.00233, time: 1035.54668\n","Train Step 3750/3810, summary_loss: 0.31570, class_loss: 0.19930, box_loss: 0.00233, time: 1109.28307\n","Train Epoch: 45, summary loss: 0.31526, class loss: 0.19897, box_loss: 0.00233, time: 1126.52101\n","Val 0000000010_0000000000.csv Epoch: 45, summary loss: 0.65597, class loss: 0.47200, box_loss: 0.00368, time: 172.95752\n","Val 0000000054_0000000000.csv Epoch: 45, summary loss: 0.47748, class loss: 0.41062, box_loss: 0.00134, time: 172.96076\n","Val 0000000056_0000000000.csv Epoch: 45, summary loss: 0.32555, class loss: 0.23571, box_loss: 0.00180, time: 172.96311\n","Val 0000000062_0000000000.csv Epoch: 45, summary loss: 0.74630, class loss: 0.50837, box_loss: 0.00476, time: 172.96502\n","Val 0000000067_0000000005.csv Epoch: 45, summary loss: 0.01298, class loss: 0.01298, box_loss: 0.00000, time: 172.96696\n","Val 0000000067_0000000012.csv Epoch: 45, summary loss: 0.12297, class loss: 0.12297, box_loss: 0.00000, time: 172.96886\n","Val 0000000067_0000000014.csv Epoch: 45, summary loss: 0.19839, class loss: 0.12391, box_loss: 0.00149, time: 172.97080\n","Val 0000000067_0000000015.csv Epoch: 45, summary loss: 0.40136, class loss: 0.22877, box_loss: 0.00345, time: 172.97266\n","Val 0000000067_0000000019.csv Epoch: 45, summary loss: 0.36571, class loss: 0.19108, box_loss: 0.00349, time: 172.97456\n","Val 0000000067_0000000024.csv Epoch: 45, summary loss: 0.34832, class loss: 0.16641, box_loss: 0.00364, time: 172.97648\n","Val 0000000067_0000000025.csv Epoch: 45, summary loss: 0.32696, class loss: 0.20196, box_loss: 0.00250, time: 172.97837\n","Val 0000000067_0000000026.csv Epoch: 45, summary loss: 0.04245, class loss: 0.04245, box_loss: 0.00000, time: 172.98054\n","Val 0000000067_0000000027.csv Epoch: 45, summary loss: 0.03776, class loss: 0.03776, box_loss: 0.00000, time: 172.98265\n","Val 0000000067_0000000028.csv Epoch: 45, summary loss: 0.07049, class loss: 0.05161, box_loss: 0.00038, time: 172.98474\n","Val 0000000067_0000000029.csv Epoch: 45, summary loss: 0.02167, class loss: 0.02167, box_loss: 0.00000, time: 172.98680\n","Val 0000000067_0000000031.csv Epoch: 45, summary loss: 0.29349, class loss: 0.18456, box_loss: 0.00218, time: 172.98890\n","Val 0000000067_0000000032.csv Epoch: 45, summary loss: 0.41250, class loss: 0.30221, box_loss: 0.00221, time: 172.99102\n","Val 0000000067_0000000040.csv Epoch: 45, summary loss: 0.72265, class loss: 0.65435, box_loss: 0.00137, time: 172.99308\n","Val 0000000067_0000000041.csv Epoch: 45, summary loss: 0.73288, class loss: 0.38055, box_loss: 0.00705, time: 172.99522\n","Val 0000000067_0000000045.csv Epoch: 45, summary loss: 0.38307, class loss: 0.29244, box_loss: 0.00181, time: 172.99732\n","Val 0000000067_0000000046.csv Epoch: 45, summary loss: 0.16676, class loss: 0.10902, box_loss: 0.00115, time: 173.00134\n","Val 0000000067_0000000050.csv Epoch: 45, summary loss: 0.86730, class loss: 0.62227, box_loss: 0.00490, time: 173.00345\n","Val 0000000067_0000000052.csv Epoch: 45, summary loss: 0.67690, class loss: 0.57385, box_loss: 0.00206, time: 173.00557\n","Val 0000000067_0000000055.csv Epoch: 45, summary loss: 0.41767, class loss: 0.36461, box_loss: 0.00106, time: 173.00769\n","Val 0000000067_0000000058.csv Epoch: 45, summary loss: 0.72933, class loss: 0.49402, box_loss: 0.00471, time: 173.02879\n","Val 0000000067_0000000059.csv Epoch: 45, summary loss: 0.40162, class loss: 0.40162, box_loss: 0.00000, time: 173.03098\n","Val 0000000351_0000000000.csv Epoch: 45, summary loss: 0.54376, class loss: 0.34613, box_loss: 0.00395, time: 173.03320\n","Val 0000000354_0000000000.csv Epoch: 45, summary loss: 1.25993, class loss: 0.68064, box_loss: 0.01159, time: 173.03527\n","Val 0000000359_0000000000.csv Epoch: 45, summary loss: 1.05379, class loss: 0.58464, box_loss: 0.00938, time: 173.03739\n","Val 0000000363_0000000000.csv Epoch: 45, summary loss: 0.38795, class loss: 0.38795, box_loss: 0.00000, time: 173.03943\n","Val 0000000364_0000000000.csv Epoch: 45, summary loss: 0.35246, class loss: 0.32279, box_loss: 0.00059, time: 173.04153\n","Val 0000000367_0000000000.csv Epoch: 45, summary loss: 0.70694, class loss: 0.49143, box_loss: 0.00431, time: 173.04367\n","Val Epoch: 45, summary loss: 0.44573, class loss: 0.31317, box_loss: 0.00265, time: 173.04597\n","Adjusting learning rate of group 0 to 9.8175e-05.\n","\n","2021-06-01T19:22:42.262149\n","LR: 9.817486484511876e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.88918\n","Train Step 250/3810, summary_loss: 0.32523, class_loss: 0.20282, box_loss: 0.00245, time: 74.97569\n","Train Step 500/3810, summary_loss: 0.32753, class_loss: 0.20644, box_loss: 0.00242, time: 148.72042\n","Train Step 750/3810, summary_loss: 0.32443, class_loss: 0.20365, box_loss: 0.00242, time: 222.50252\n","Train Step 1000/3810, summary_loss: 0.32090, class_loss: 0.20217, box_loss: 0.00237, time: 296.79275\n","Train Step 1250/3810, summary_loss: 0.32178, class_loss: 0.20239, box_loss: 0.00239, time: 369.99574\n","Train Step 1500/3810, summary_loss: 0.32171, class_loss: 0.20249, box_loss: 0.00238, time: 443.75061\n","Train Step 1750/3810, summary_loss: 0.31925, class_loss: 0.20114, box_loss: 0.00236, time: 517.82201\n","Train Step 2000/3810, summary_loss: 0.31778, class_loss: 0.20051, box_loss: 0.00235, time: 591.37629\n","Train Step 2250/3810, summary_loss: 0.31694, class_loss: 0.20025, box_loss: 0.00233, time: 665.20282\n","Train Step 2500/3810, summary_loss: 0.31802, class_loss: 0.20064, box_loss: 0.00235, time: 739.10223\n","Train Step 2750/3810, summary_loss: 0.31753, class_loss: 0.20036, box_loss: 0.00234, time: 812.90022\n","Train Step 3000/3810, summary_loss: 0.31709, class_loss: 0.20006, box_loss: 0.00234, time: 886.71419\n","Train Step 3250/3810, summary_loss: 0.31717, class_loss: 0.20003, box_loss: 0.00234, time: 960.33190\n","Train Step 3500/3810, summary_loss: 0.32060, class_loss: 0.20289, box_loss: 0.00235, time: 1034.23837\n","Train Step 3750/3810, summary_loss: 0.32053, class_loss: 0.20275, box_loss: 0.00236, time: 1108.31311\n","Train Epoch: 46, summary loss: 0.32033, class loss: 0.20258, box_loss: 0.00236, time: 1125.85738\n","Val 0000000010_0000000000.csv Epoch: 46, summary loss: 0.84279, class loss: 0.54048, box_loss: 0.00605, time: 173.27058\n","Val 0000000054_0000000000.csv Epoch: 46, summary loss: 0.46597, class loss: 0.36390, box_loss: 0.00204, time: 173.27371\n","Val 0000000056_0000000000.csv Epoch: 46, summary loss: 0.33416, class loss: 0.24883, box_loss: 0.00171, time: 173.27597\n","Val 0000000062_0000000000.csv Epoch: 46, summary loss: 0.85972, class loss: 0.56766, box_loss: 0.00584, time: 173.27787\n","Val 0000000067_0000000005.csv Epoch: 46, summary loss: 0.01758, class loss: 0.01758, box_loss: 0.00000, time: 173.29236\n","Val 0000000067_0000000012.csv Epoch: 46, summary loss: 0.17908, class loss: 0.17908, box_loss: 0.00000, time: 173.29448\n","Val 0000000067_0000000014.csv Epoch: 46, summary loss: 0.20068, class loss: 0.11189, box_loss: 0.00178, time: 173.29654\n","Val 0000000067_0000000015.csv Epoch: 46, summary loss: 0.41789, class loss: 0.22129, box_loss: 0.00393, time: 173.29859\n","Val 0000000067_0000000019.csv Epoch: 46, summary loss: 0.38531, class loss: 0.19621, box_loss: 0.00378, time: 173.30065\n","Val 0000000067_0000000024.csv Epoch: 46, summary loss: 0.30597, class loss: 0.13927, box_loss: 0.00333, time: 173.30268\n","Val 0000000067_0000000025.csv Epoch: 46, summary loss: 0.32165, class loss: 0.19818, box_loss: 0.00247, time: 173.30481\n","Val 0000000067_0000000026.csv Epoch: 46, summary loss: 0.01877, class loss: 0.01877, box_loss: 0.00000, time: 173.30694\n","Val 0000000067_0000000027.csv Epoch: 46, summary loss: 0.04851, class loss: 0.04851, box_loss: 0.00000, time: 173.30901\n","Val 0000000067_0000000028.csv Epoch: 46, summary loss: 0.08232, class loss: 0.05756, box_loss: 0.00050, time: 173.31105\n","Val 0000000067_0000000029.csv Epoch: 46, summary loss: 0.04028, class loss: 0.04028, box_loss: 0.00000, time: 173.31305\n","Val 0000000067_0000000031.csv Epoch: 46, summary loss: 0.31667, class loss: 0.18584, box_loss: 0.00262, time: 173.31508\n","Val 0000000067_0000000032.csv Epoch: 46, summary loss: 0.41038, class loss: 0.30342, box_loss: 0.00214, time: 173.31710\n","Val 0000000067_0000000040.csv Epoch: 46, summary loss: 0.25901, class loss: 0.19254, box_loss: 0.00133, time: 173.31913\n","Val 0000000067_0000000041.csv Epoch: 46, summary loss: 0.70021, class loss: 0.38651, box_loss: 0.00627, time: 173.32118\n","Val 0000000067_0000000045.csv Epoch: 46, summary loss: 0.34652, class loss: 0.26707, box_loss: 0.00159, time: 173.32319\n","Val 0000000067_0000000046.csv Epoch: 46, summary loss: 0.17238, class loss: 0.10301, box_loss: 0.00139, time: 173.32519\n","Val 0000000067_0000000050.csv Epoch: 46, summary loss: 0.85935, class loss: 0.58215, box_loss: 0.00554, time: 173.32721\n","Val 0000000067_0000000052.csv Epoch: 46, summary loss: 0.63747, class loss: 0.50782, box_loss: 0.00259, time: 173.32926\n","Val 0000000067_0000000055.csv Epoch: 46, summary loss: 0.34818, class loss: 0.27448, box_loss: 0.00147, time: 173.33136\n","Val 0000000067_0000000058.csv Epoch: 46, summary loss: 0.73531, class loss: 0.48693, box_loss: 0.00497, time: 173.33344\n","Val 0000000067_0000000059.csv Epoch: 46, summary loss: 0.38554, class loss: 0.38554, box_loss: 0.00000, time: 173.33556\n","Val 0000000351_0000000000.csv Epoch: 46, summary loss: 0.62829, class loss: 0.40107, box_loss: 0.00454, time: 173.33766\n","Val 0000000354_0000000000.csv Epoch: 46, summary loss: 1.53595, class loss: 0.81648, box_loss: 0.01439, time: 173.33976\n","Val 0000000359_0000000000.csv Epoch: 46, summary loss: 1.02121, class loss: 0.53578, box_loss: 0.00971, time: 173.34185\n","Val 0000000363_0000000000.csv Epoch: 46, summary loss: 0.36091, class loss: 0.36091, box_loss: 0.00000, time: 173.34394\n","Val 0000000364_0000000000.csv Epoch: 46, summary loss: 0.42286, class loss: 0.39552, box_loss: 0.00055, time: 173.34601\n","Val 0000000367_0000000000.csv Epoch: 46, summary loss: 0.69834, class loss: 0.48910, box_loss: 0.00418, time: 173.34810\n","Val Epoch: 46, summary loss: 0.44873, class loss: 0.30074, box_loss: 0.00296, time: 173.35031\n","Adjusting learning rate of group 0 to 9.6668e-05.\n","\n","2021-06-01T19:44:21.769382\n","LR: 9.666789806749252e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.84065\n","Train Step 250/3810, summary_loss: 0.29807, class_loss: 0.18804, box_loss: 0.00220, time: 75.39171\n","Train Step 500/3810, summary_loss: 0.30673, class_loss: 0.19377, box_loss: 0.00226, time: 149.24933\n","Train Step 750/3810, summary_loss: 0.30543, class_loss: 0.19334, box_loss: 0.00224, time: 223.54773\n","Train Step 1000/3810, summary_loss: 0.30525, class_loss: 0.19386, box_loss: 0.00223, time: 297.59073\n","Train Step 1250/3810, summary_loss: 0.30706, class_loss: 0.19533, box_loss: 0.00223, time: 371.31173\n","Train Step 1500/3810, summary_loss: 0.31051, class_loss: 0.19715, box_loss: 0.00227, time: 445.21413\n","Train Step 1750/3810, summary_loss: 0.31130, class_loss: 0.19716, box_loss: 0.00228, time: 519.09901\n","Train Step 2000/3810, summary_loss: 0.31110, class_loss: 0.19693, box_loss: 0.00228, time: 592.37996\n","Train Step 2250/3810, summary_loss: 0.31009, class_loss: 0.19633, box_loss: 0.00228, time: 665.94899\n","Train Step 2500/3810, summary_loss: 0.31046, class_loss: 0.19661, box_loss: 0.00228, time: 740.09368\n","Train Step 2750/3810, summary_loss: 0.31017, class_loss: 0.19644, box_loss: 0.00227, time: 813.78076\n","Train Step 3000/3810, summary_loss: 0.31066, class_loss: 0.19671, box_loss: 0.00228, time: 887.39521\n","Train Step 3250/3810, summary_loss: 0.31138, class_loss: 0.19723, box_loss: 0.00228, time: 961.48489\n","Train Step 3500/3810, summary_loss: 0.31086, class_loss: 0.19680, box_loss: 0.00228, time: 1035.07425\n","Train Step 3750/3810, summary_loss: 0.31191, class_loss: 0.19736, box_loss: 0.00229, time: 1108.64732\n","Train Epoch: 47, summary loss: 0.31217, class loss: 0.19748, box_loss: 0.00229, time: 1126.22720\n","Val 0000000010_0000000000.csv Epoch: 47, summary loss: 0.75065, class loss: 0.47258, box_loss: 0.00556, time: 173.17321\n","Val 0000000054_0000000000.csv Epoch: 47, summary loss: 0.56047, class loss: 0.39475, box_loss: 0.00331, time: 173.17670\n","Val 0000000056_0000000000.csv Epoch: 47, summary loss: 0.32219, class loss: 0.24672, box_loss: 0.00151, time: 173.17925\n","Val 0000000062_0000000000.csv Epoch: 47, summary loss: 0.88665, class loss: 0.52003, box_loss: 0.00733, time: 173.18152\n","Val 0000000067_0000000005.csv Epoch: 47, summary loss: 0.01489, class loss: 0.01489, box_loss: 0.00000, time: 173.18366\n","Val 0000000067_0000000012.csv Epoch: 47, summary loss: 0.19380, class loss: 0.19380, box_loss: 0.00000, time: 173.18575\n","Val 0000000067_0000000014.csv Epoch: 47, summary loss: 0.20606, class loss: 0.12372, box_loss: 0.00165, time: 173.18785\n","Val 0000000067_0000000015.csv Epoch: 47, summary loss: 0.47632, class loss: 0.21735, box_loss: 0.00518, time: 173.19006\n","Val 0000000067_0000000019.csv Epoch: 47, summary loss: 0.40577, class loss: 0.20602, box_loss: 0.00400, time: 173.19205\n","Val 0000000067_0000000024.csv Epoch: 47, summary loss: 0.33706, class loss: 0.15280, box_loss: 0.00369, time: 173.19418\n","Val 0000000067_0000000025.csv Epoch: 47, summary loss: 0.30422, class loss: 0.18854, box_loss: 0.00231, time: 173.19628\n","Val 0000000067_0000000026.csv Epoch: 47, summary loss: 0.01609, class loss: 0.01609, box_loss: 0.00000, time: 173.19838\n","Val 0000000067_0000000027.csv Epoch: 47, summary loss: 0.06005, class loss: 0.06005, box_loss: 0.00000, time: 173.20046\n","Val 0000000067_0000000028.csv Epoch: 47, summary loss: 0.09754, class loss: 0.07228, box_loss: 0.00051, time: 173.21909\n","Val 0000000067_0000000029.csv Epoch: 47, summary loss: 0.05478, class loss: 0.05478, box_loss: 0.00000, time: 173.22281\n","Val 0000000067_0000000031.csv Epoch: 47, summary loss: 0.32356, class loss: 0.19086, box_loss: 0.00265, time: 173.22485\n","Val 0000000067_0000000032.csv Epoch: 47, summary loss: 0.45561, class loss: 0.33452, box_loss: 0.00242, time: 173.22687\n","Val 0000000067_0000000040.csv Epoch: 47, summary loss: 0.24454, class loss: 0.18034, box_loss: 0.00128, time: 173.22892\n","Val 0000000067_0000000041.csv Epoch: 47, summary loss: 0.68626, class loss: 0.37236, box_loss: 0.00628, time: 173.23095\n","Val 0000000067_0000000045.csv Epoch: 47, summary loss: 0.41606, class loss: 0.27662, box_loss: 0.00279, time: 173.23299\n","Val 0000000067_0000000046.csv Epoch: 47, summary loss: 0.16695, class loss: 0.09627, box_loss: 0.00141, time: 173.23506\n","Val 0000000067_0000000050.csv Epoch: 47, summary loss: 0.84207, class loss: 0.54402, box_loss: 0.00596, time: 173.23716\n","Val 0000000067_0000000052.csv Epoch: 47, summary loss: 0.66108, class loss: 0.55506, box_loss: 0.00212, time: 173.23929\n","Val 0000000067_0000000055.csv Epoch: 47, summary loss: 0.39084, class loss: 0.34873, box_loss: 0.00084, time: 173.24133\n","Val 0000000067_0000000058.csv Epoch: 47, summary loss: 0.65643, class loss: 0.43679, box_loss: 0.00439, time: 173.24340\n","Val 0000000067_0000000059.csv Epoch: 47, summary loss: 0.49805, class loss: 0.49805, box_loss: 0.00000, time: 173.24546\n","Val 0000000351_0000000000.csv Epoch: 47, summary loss: 0.50602, class loss: 0.31332, box_loss: 0.00385, time: 173.24753\n","Val 0000000354_0000000000.csv Epoch: 47, summary loss: 1.05306, class loss: 0.47475, box_loss: 0.01157, time: 173.24958\n","Val 0000000359_0000000000.csv Epoch: 47, summary loss: 0.86419, class loss: 0.42175, box_loss: 0.00885, time: 173.25166\n","Val 0000000363_0000000000.csv Epoch: 47, summary loss: 0.27529, class loss: 0.27529, box_loss: 0.00000, time: 173.25375\n","Val 0000000364_0000000000.csv Epoch: 47, summary loss: 0.51165, class loss: 0.48710, box_loss: 0.00049, time: 173.25604\n","Val 0000000367_0000000000.csv Epoch: 47, summary loss: 0.72592, class loss: 0.51042, box_loss: 0.00431, time: 173.25815\n","Val Epoch: 47, summary loss: 0.43638, class loss: 0.28908, box_loss: 0.00295, time: 173.26051\n","Adjusting learning rate of group 0 to 9.5184e-05.\n","\n","2021-06-01T20:06:01.586995\n","LR: 9.518406296285053e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.82783\n","Train Step 250/3810, summary_loss: 0.32206, class_loss: 0.19953, box_loss: 0.00245, time: 75.25263\n","Train Step 500/3810, summary_loss: 0.31119, class_loss: 0.19513, box_loss: 0.00232, time: 148.74338\n","Train Step 750/3810, summary_loss: 0.30787, class_loss: 0.19375, box_loss: 0.00228, time: 222.60553\n","Train Step 1000/3810, summary_loss: 0.31077, class_loss: 0.19564, box_loss: 0.00230, time: 296.30549\n","Train Step 1250/3810, summary_loss: 0.30938, class_loss: 0.19542, box_loss: 0.00228, time: 370.12487\n","Train Step 1500/3810, summary_loss: 0.31132, class_loss: 0.19585, box_loss: 0.00231, time: 444.31872\n","Train Step 1750/3810, summary_loss: 0.31177, class_loss: 0.19629, box_loss: 0.00231, time: 518.13609\n","Train Step 2000/3810, summary_loss: 0.31299, class_loss: 0.19706, box_loss: 0.00232, time: 592.07123\n","Train Step 2250/3810, summary_loss: 0.31237, class_loss: 0.19676, box_loss: 0.00231, time: 666.42271\n","Train Step 2500/3810, summary_loss: 0.31241, class_loss: 0.19683, box_loss: 0.00231, time: 740.71543\n","Train Step 2750/3810, summary_loss: 0.31133, class_loss: 0.19620, box_loss: 0.00230, time: 814.21303\n","Train Step 3000/3810, summary_loss: 0.31057, class_loss: 0.19594, box_loss: 0.00229, time: 888.79559\n","Train Step 3250/3810, summary_loss: 0.30989, class_loss: 0.19555, box_loss: 0.00229, time: 962.63252\n","Train Step 3500/3810, summary_loss: 0.31189, class_loss: 0.19696, box_loss: 0.00230, time: 1036.90892\n","Train Step 3750/3810, summary_loss: 0.31152, class_loss: 0.19686, box_loss: 0.00229, time: 1110.83153\n","Train Epoch: 48, summary loss: 0.31153, class loss: 0.19683, box_loss: 0.00229, time: 1128.32306\n","Val 0000000010_0000000000.csv Epoch: 48, summary loss: 0.95155, class loss: 0.71603, box_loss: 0.00471, time: 173.77836\n","Val 0000000054_0000000000.csv Epoch: 48, summary loss: 0.52657, class loss: 0.39098, box_loss: 0.00271, time: 173.78169\n","Val 0000000056_0000000000.csv Epoch: 48, summary loss: 0.29997, class loss: 0.22394, box_loss: 0.00152, time: 173.78416\n","Val 0000000062_0000000000.csv Epoch: 48, summary loss: 0.81868, class loss: 0.54655, box_loss: 0.00544, time: 173.78626\n","Val 0000000067_0000000005.csv Epoch: 48, summary loss: 0.02194, class loss: 0.02194, box_loss: 0.00000, time: 173.78835\n","Val 0000000067_0000000012.csv Epoch: 48, summary loss: 0.14114, class loss: 0.14114, box_loss: 0.00000, time: 173.79048\n","Val 0000000067_0000000014.csv Epoch: 48, summary loss: 0.18108, class loss: 0.11520, box_loss: 0.00132, time: 173.79260\n","Val 0000000067_0000000015.csv Epoch: 48, summary loss: 0.43347, class loss: 0.21511, box_loss: 0.00437, time: 173.79471\n","Val 0000000067_0000000019.csv Epoch: 48, summary loss: 0.40451, class loss: 0.22096, box_loss: 0.00367, time: 173.79683\n","Val 0000000067_0000000024.csv Epoch: 48, summary loss: 0.34779, class loss: 0.15228, box_loss: 0.00391, time: 173.79894\n","Val 0000000067_0000000025.csv Epoch: 48, summary loss: 0.38102, class loss: 0.17507, box_loss: 0.00412, time: 173.80105\n","Val 0000000067_0000000026.csv Epoch: 48, summary loss: 0.01862, class loss: 0.01862, box_loss: 0.00000, time: 173.80316\n","Val 0000000067_0000000027.csv Epoch: 48, summary loss: 0.06486, class loss: 0.06486, box_loss: 0.00000, time: 173.80530\n","Val 0000000067_0000000028.csv Epoch: 48, summary loss: 0.09326, class loss: 0.06065, box_loss: 0.00065, time: 173.80739\n","Val 0000000067_0000000029.csv Epoch: 48, summary loss: 0.06287, class loss: 0.06287, box_loss: 0.00000, time: 173.80947\n","Val 0000000067_0000000031.csv Epoch: 48, summary loss: 0.56518, class loss: 0.42541, box_loss: 0.00280, time: 173.81155\n","Val 0000000067_0000000032.csv Epoch: 48, summary loss: 0.44678, class loss: 0.30856, box_loss: 0.00276, time: 173.81367\n","Val 0000000067_0000000040.csv Epoch: 48, summary loss: 0.28111, class loss: 0.21478, box_loss: 0.00133, time: 173.81577\n","Val 0000000067_0000000041.csv Epoch: 48, summary loss: 0.71902, class loss: 0.37752, box_loss: 0.00683, time: 173.81787\n","Val 0000000067_0000000045.csv Epoch: 48, summary loss: 0.41756, class loss: 0.30524, box_loss: 0.00225, time: 173.81993\n","Val 0000000067_0000000046.csv Epoch: 48, summary loss: 0.15379, class loss: 0.09958, box_loss: 0.00108, time: 173.82201\n","Val 0000000067_0000000050.csv Epoch: 48, summary loss: 0.88633, class loss: 0.57571, box_loss: 0.00621, time: 173.82408\n","Val 0000000067_0000000052.csv Epoch: 48, summary loss: 0.70947, class loss: 0.58648, box_loss: 0.00246, time: 173.82618\n","Val 0000000067_0000000055.csv Epoch: 48, summary loss: 0.49197, class loss: 0.41845, box_loss: 0.00147, time: 173.84860\n","Val 0000000067_0000000058.csv Epoch: 48, summary loss: 0.74373, class loss: 0.49187, box_loss: 0.00504, time: 173.85234\n","Val 0000000067_0000000059.csv Epoch: 48, summary loss: 0.48579, class loss: 0.48579, box_loss: 0.00000, time: 173.85445\n","Val 0000000351_0000000000.csv Epoch: 48, summary loss: 0.83668, class loss: 0.58484, box_loss: 0.00504, time: 173.85649\n","Val 0000000354_0000000000.csv Epoch: 48, summary loss: 1.17538, class loss: 0.61959, box_loss: 0.01112, time: 173.85858\n","Val 0000000359_0000000000.csv Epoch: 48, summary loss: 1.07237, class loss: 0.61429, box_loss: 0.00916, time: 173.86063\n","Val 0000000363_0000000000.csv Epoch: 48, summary loss: 0.21513, class loss: 0.21513, box_loss: 0.00000, time: 173.86270\n","Val 0000000364_0000000000.csv Epoch: 48, summary loss: 0.57229, class loss: 0.53805, box_loss: 0.00068, time: 173.86483\n","Val 0000000367_0000000000.csv Epoch: 48, summary loss: 0.67488, class loss: 0.47034, box_loss: 0.00409, time: 173.86697\n","Val Epoch: 48, summary loss: 0.47484, class loss: 0.32681, box_loss: 0.00296, time: 173.86922\n","Adjusting learning rate of group 0 to 9.3723e-05.\n","\n","2021-06-01T20:27:44.079600\n","LR: 9.372300446411167e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.86092\n","Train Step 250/3810, summary_loss: 0.31398, class_loss: 0.19955, box_loss: 0.00229, time: 75.69171\n","Train Step 500/3810, summary_loss: 0.31414, class_loss: 0.19973, box_loss: 0.00229, time: 149.40160\n","Train Step 750/3810, summary_loss: 0.31961, class_loss: 0.20270, box_loss: 0.00234, time: 223.19858\n","Train Step 1000/3810, summary_loss: 0.31840, class_loss: 0.20134, box_loss: 0.00234, time: 297.27682\n","Train Step 1250/3810, summary_loss: 0.31446, class_loss: 0.19898, box_loss: 0.00231, time: 371.16039\n","Train Step 1500/3810, summary_loss: 0.31278, class_loss: 0.19795, box_loss: 0.00230, time: 445.25497\n","Train Step 1750/3810, summary_loss: 0.31295, class_loss: 0.19809, box_loss: 0.00230, time: 519.33083\n","Train Step 2000/3810, summary_loss: 0.31389, class_loss: 0.19900, box_loss: 0.00230, time: 592.87570\n","Train Step 2250/3810, summary_loss: 0.31504, class_loss: 0.19953, box_loss: 0.00231, time: 666.07790\n","Train Step 2500/3810, summary_loss: 0.31395, class_loss: 0.19881, box_loss: 0.00230, time: 739.86902\n","Train Step 2750/3810, summary_loss: 0.31410, class_loss: 0.19864, box_loss: 0.00231, time: 813.29281\n","Train Step 3000/3810, summary_loss: 0.31527, class_loss: 0.19970, box_loss: 0.00231, time: 887.06529\n","Train Step 3250/3810, summary_loss: 0.31442, class_loss: 0.19932, box_loss: 0.00230, time: 960.39126\n","Train Step 3500/3810, summary_loss: 0.31520, class_loss: 0.19976, box_loss: 0.00231, time: 1034.21830\n","Train Step 3750/3810, summary_loss: 0.31457, class_loss: 0.19939, box_loss: 0.00230, time: 1107.94165\n","Train Epoch: 49, summary loss: 0.31403, class loss: 0.19911, box_loss: 0.00230, time: 1125.34221\n","Val 0000000010_0000000000.csv Epoch: 49, summary loss: 0.98892, class loss: 0.67907, box_loss: 0.00620, time: 172.25438\n","Val 0000000054_0000000000.csv Epoch: 49, summary loss: 0.41741, class loss: 0.36268, box_loss: 0.00109, time: 172.25733\n","Val 0000000056_0000000000.csv Epoch: 49, summary loss: 0.46150, class loss: 0.34553, box_loss: 0.00232, time: 172.25973\n","Val 0000000062_0000000000.csv Epoch: 49, summary loss: 0.84103, class loss: 0.50625, box_loss: 0.00670, time: 172.26190\n","Val 0000000067_0000000005.csv Epoch: 49, summary loss: 0.01912, class loss: 0.01912, box_loss: 0.00000, time: 172.26401\n","Val 0000000067_0000000012.csv Epoch: 49, summary loss: 0.05874, class loss: 0.05874, box_loss: 0.00000, time: 172.26614\n","Val 0000000067_0000000014.csv Epoch: 49, summary loss: 0.22399, class loss: 0.12649, box_loss: 0.00195, time: 172.26821\n","Val 0000000067_0000000015.csv Epoch: 49, summary loss: 0.46493, class loss: 0.21453, box_loss: 0.00501, time: 172.28730\n","Val 0000000067_0000000019.csv Epoch: 49, summary loss: 0.38865, class loss: 0.18906, box_loss: 0.00399, time: 172.28947\n","Val 0000000067_0000000024.csv Epoch: 49, summary loss: 0.35261, class loss: 0.15800, box_loss: 0.00389, time: 172.29167\n","Val 0000000067_0000000025.csv Epoch: 49, summary loss: 0.33042, class loss: 0.18934, box_loss: 0.00282, time: 172.29382\n","Val 0000000067_0000000026.csv Epoch: 49, summary loss: 0.01746, class loss: 0.01746, box_loss: 0.00000, time: 172.29597\n","Val 0000000067_0000000027.csv Epoch: 49, summary loss: 0.04405, class loss: 0.04405, box_loss: 0.00000, time: 172.29808\n","Val 0000000067_0000000028.csv Epoch: 49, summary loss: 0.08967, class loss: 0.05907, box_loss: 0.00061, time: 172.30022\n","Val 0000000067_0000000029.csv Epoch: 49, summary loss: 0.05319, class loss: 0.05319, box_loss: 0.00000, time: 172.30238\n","Val 0000000067_0000000031.csv Epoch: 49, summary loss: 0.41208, class loss: 0.24179, box_loss: 0.00341, time: 172.30452\n","Val 0000000067_0000000032.csv Epoch: 49, summary loss: 0.40113, class loss: 0.25114, box_loss: 0.00300, time: 172.30670\n","Val 0000000067_0000000040.csv Epoch: 49, summary loss: 0.21274, class loss: 0.15168, box_loss: 0.00122, time: 172.30888\n","Val 0000000067_0000000041.csv Epoch: 49, summary loss: 0.77969, class loss: 0.40292, box_loss: 0.00754, time: 172.31103\n","Val 0000000067_0000000045.csv Epoch: 49, summary loss: 0.39807, class loss: 0.26542, box_loss: 0.00265, time: 172.31318\n","Val 0000000067_0000000046.csv Epoch: 49, summary loss: 0.15900, class loss: 0.10010, box_loss: 0.00118, time: 172.31531\n","Val 0000000067_0000000050.csv Epoch: 49, summary loss: 0.83771, class loss: 0.55675, box_loss: 0.00562, time: 172.31745\n","Val 0000000067_0000000052.csv Epoch: 49, summary loss: 0.64370, class loss: 0.52353, box_loss: 0.00240, time: 172.31958\n","Val 0000000067_0000000055.csv Epoch: 49, summary loss: 0.41931, class loss: 0.35263, box_loss: 0.00133, time: 172.32172\n","Val 0000000067_0000000058.csv Epoch: 49, summary loss: 0.70907, class loss: 0.40783, box_loss: 0.00602, time: 172.32385\n","Val 0000000067_0000000059.csv Epoch: 49, summary loss: 0.43945, class loss: 0.43945, box_loss: 0.00000, time: 172.32597\n","Val 0000000351_0000000000.csv Epoch: 49, summary loss: 0.60784, class loss: 0.38888, box_loss: 0.00438, time: 172.32810\n","Val 0000000354_0000000000.csv Epoch: 49, summary loss: 1.03724, class loss: 0.59320, box_loss: 0.00888, time: 172.33022\n","Val 0000000359_0000000000.csv Epoch: 49, summary loss: 1.17981, class loss: 0.68904, box_loss: 0.00982, time: 172.33238\n","Val 0000000363_0000000000.csv Epoch: 49, summary loss: 0.10308, class loss: 0.10308, box_loss: 0.00000, time: 172.33452\n","Val 0000000364_0000000000.csv Epoch: 49, summary loss: 0.60531, class loss: 0.57949, box_loss: 0.00052, time: 172.33670\n","Val 0000000367_0000000000.csv Epoch: 49, summary loss: 0.78020, class loss: 0.57221, box_loss: 0.00416, time: 172.33883\n","Val Epoch: 49, summary loss: 0.45241, class loss: 0.30130, box_loss: 0.00302, time: 172.34109\n","Adjusting learning rate of group 0 to 9.2284e-05.\n","\n","2021-06-01T20:49:22.062262\n","LR: 9.228437295441162e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.91331\n","Train Step 250/3810, summary_loss: 0.30196, class_loss: 0.19394, box_loss: 0.00216, time: 74.62836\n","Train Step 500/3810, summary_loss: 0.30754, class_loss: 0.19383, box_loss: 0.00227, time: 148.12335\n","Train Step 750/3810, summary_loss: 0.30557, class_loss: 0.19238, box_loss: 0.00226, time: 221.90474\n","Train Step 1000/3810, summary_loss: 0.30668, class_loss: 0.19361, box_loss: 0.00226, time: 295.72206\n","Train Step 1250/3810, summary_loss: 0.30395, class_loss: 0.19209, box_loss: 0.00224, time: 369.00496\n","Train Step 1500/3810, summary_loss: 0.30658, class_loss: 0.19389, box_loss: 0.00225, time: 442.71134\n","Train Step 1750/3810, summary_loss: 0.30752, class_loss: 0.19395, box_loss: 0.00227, time: 516.25517\n","Train Step 2000/3810, summary_loss: 0.30672, class_loss: 0.19361, box_loss: 0.00226, time: 589.16966\n","Train Step 2250/3810, summary_loss: 0.30730, class_loss: 0.19411, box_loss: 0.00226, time: 662.56267\n","Train Step 2500/3810, summary_loss: 0.30779, class_loss: 0.19453, box_loss: 0.00227, time: 736.34597\n","Train Step 2750/3810, summary_loss: 0.30731, class_loss: 0.19418, box_loss: 0.00226, time: 809.99396\n","Train Step 3000/3810, summary_loss: 0.30710, class_loss: 0.19397, box_loss: 0.00226, time: 883.60775\n","Train Step 3250/3810, summary_loss: 0.30713, class_loss: 0.19396, box_loss: 0.00226, time: 957.38672\n","Train Step 3500/3810, summary_loss: 0.30657, class_loss: 0.19373, box_loss: 0.00226, time: 1031.04379\n","Train Step 3750/3810, summary_loss: 0.30667, class_loss: 0.19385, box_loss: 0.00226, time: 1104.19195\n","Train Epoch: 50, summary loss: 0.30666, class loss: 0.19381, box_loss: 0.00226, time: 1121.81614\n","Val 0000000010_0000000000.csv Epoch: 50, summary loss: 0.49042, class loss: 0.34556, box_loss: 0.00290, time: 172.00990\n","Val 0000000054_0000000000.csv Epoch: 50, summary loss: 0.49314, class loss: 0.39614, box_loss: 0.00194, time: 172.01286\n","Val 0000000056_0000000000.csv Epoch: 50, summary loss: 0.33038, class loss: 0.23524, box_loss: 0.00190, time: 172.01525\n","Val 0000000062_0000000000.csv Epoch: 50, summary loss: 0.85360, class loss: 0.50164, box_loss: 0.00704, time: 172.01734\n","Val 0000000067_0000000005.csv Epoch: 50, summary loss: 0.01358, class loss: 0.01358, box_loss: 0.00000, time: 172.01942\n","Val 0000000067_0000000012.csv Epoch: 50, summary loss: 0.25233, class loss: 0.25233, box_loss: 0.00000, time: 172.02146\n","Val 0000000067_0000000014.csv Epoch: 50, summary loss: 0.16318, class loss: 0.10807, box_loss: 0.00110, time: 172.02347\n","Val 0000000067_0000000015.csv Epoch: 50, summary loss: 0.41471, class loss: 0.22549, box_loss: 0.00378, time: 172.02553\n","Val 0000000067_0000000019.csv Epoch: 50, summary loss: 0.44616, class loss: 0.23227, box_loss: 0.00428, time: 172.02757\n","Val 0000000067_0000000024.csv Epoch: 50, summary loss: 0.25346, class loss: 0.12937, box_loss: 0.00248, time: 172.02967\n","Val 0000000067_0000000025.csv Epoch: 50, summary loss: 0.34763, class loss: 0.19566, box_loss: 0.00304, time: 172.03172\n","Val 0000000067_0000000026.csv Epoch: 50, summary loss: 0.01202, class loss: 0.01202, box_loss: 0.00000, time: 172.03379\n","Val 0000000067_0000000027.csv Epoch: 50, summary loss: 0.04684, class loss: 0.04684, box_loss: 0.00000, time: 172.03584\n","Val 0000000067_0000000028.csv Epoch: 50, summary loss: 0.07734, class loss: 0.05424, box_loss: 0.00046, time: 172.03786\n","Val 0000000067_0000000029.csv Epoch: 50, summary loss: 0.06423, class loss: 0.06423, box_loss: 0.00000, time: 172.03991\n","Val 0000000067_0000000031.csv Epoch: 50, summary loss: 0.29593, class loss: 0.17935, box_loss: 0.00233, time: 172.06310\n","Val 0000000067_0000000032.csv Epoch: 50, summary loss: 0.42752, class loss: 0.33763, box_loss: 0.00180, time: 172.06516\n","Val 0000000067_0000000040.csv Epoch: 50, summary loss: 0.26411, class loss: 0.19721, box_loss: 0.00134, time: 172.06725\n","Val 0000000067_0000000041.csv Epoch: 50, summary loss: 0.73720, class loss: 0.37675, box_loss: 0.00721, time: 172.06926\n","Val 0000000067_0000000045.csv Epoch: 50, summary loss: 0.42632, class loss: 0.34171, box_loss: 0.00169, time: 172.07126\n","Val 0000000067_0000000046.csv Epoch: 50, summary loss: 0.16673, class loss: 0.10587, box_loss: 0.00122, time: 172.07339\n","Val 0000000067_0000000050.csv Epoch: 50, summary loss: 0.93974, class loss: 0.68542, box_loss: 0.00509, time: 172.07541\n","Val 0000000067_0000000052.csv Epoch: 50, summary loss: 0.90823, class loss: 0.78302, box_loss: 0.00250, time: 172.07742\n","Val 0000000067_0000000055.csv Epoch: 50, summary loss: 0.55992, class loss: 0.52438, box_loss: 0.00071, time: 172.07943\n","Val 0000000067_0000000058.csv Epoch: 50, summary loss: 0.74754, class loss: 0.49552, box_loss: 0.00504, time: 172.08151\n","Val 0000000067_0000000059.csv Epoch: 50, summary loss: 0.57701, class loss: 0.57701, box_loss: 0.00000, time: 172.08351\n","Val 0000000351_0000000000.csv Epoch: 50, summary loss: 0.74689, class loss: 0.48855, box_loss: 0.00517, time: 172.08550\n","Val 0000000354_0000000000.csv Epoch: 50, summary loss: 1.00126, class loss: 0.71237, box_loss: 0.00578, time: 172.08750\n","Val 0000000359_0000000000.csv Epoch: 50, summary loss: 1.06799, class loss: 0.59206, box_loss: 0.00952, time: 172.08949\n","Val 0000000363_0000000000.csv Epoch: 50, summary loss: 0.25321, class loss: 0.25321, box_loss: 0.00000, time: 172.09149\n","Val 0000000364_0000000000.csv Epoch: 50, summary loss: 0.42072, class loss: 0.39586, box_loss: 0.00050, time: 172.09350\n","Val 0000000367_0000000000.csv Epoch: 50, summary loss: 0.76546, class loss: 0.54910, box_loss: 0.00433, time: 172.09549\n","Val Epoch: 50, summary loss: 0.45515, class loss: 0.32524, box_loss: 0.00260, time: 172.09759\n","Adjusting learning rate of group 0 to 9.0868e-05.\n","\n","2021-06-01T21:10:56.280192\n","LR: 9.086782418344295e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.85375\n","Train Step 250/3810, summary_loss: 0.29843, class_loss: 0.18903, box_loss: 0.00219, time: 75.19286\n","Train Step 500/3810, summary_loss: 0.29661, class_loss: 0.18921, box_loss: 0.00215, time: 149.89145\n","Train Step 750/3810, summary_loss: 0.30360, class_loss: 0.19411, box_loss: 0.00219, time: 223.21022\n","Train Step 1000/3810, summary_loss: 0.30438, class_loss: 0.19446, box_loss: 0.00220, time: 296.44679\n","Train Step 1250/3810, summary_loss: 0.30523, class_loss: 0.19454, box_loss: 0.00221, time: 369.94155\n","Train Step 1500/3810, summary_loss: 0.30699, class_loss: 0.19556, box_loss: 0.00223, time: 443.42099\n","Train Step 1750/3810, summary_loss: 0.30766, class_loss: 0.19586, box_loss: 0.00224, time: 517.08033\n","Train Step 2000/3810, summary_loss: 0.30768, class_loss: 0.19586, box_loss: 0.00224, time: 590.52440\n","Train Step 2250/3810, summary_loss: 0.31046, class_loss: 0.19725, box_loss: 0.00226, time: 664.68389\n","Train Step 2500/3810, summary_loss: 0.31069, class_loss: 0.19688, box_loss: 0.00228, time: 738.50847\n","Train Step 2750/3810, summary_loss: 0.31127, class_loss: 0.19722, box_loss: 0.00228, time: 812.50457\n","Train Step 3000/3810, summary_loss: 0.31184, class_loss: 0.19723, box_loss: 0.00229, time: 886.61667\n","Train Step 3250/3810, summary_loss: 0.31216, class_loss: 0.19725, box_loss: 0.00230, time: 960.08627\n","Train Step 3500/3810, summary_loss: 0.31173, class_loss: 0.19697, box_loss: 0.00230, time: 1033.32623\n","Train Step 3750/3810, summary_loss: 0.31095, class_loss: 0.19670, box_loss: 0.00228, time: 1106.45869\n","Train Epoch: 51, summary loss: 0.31122, class loss: 0.19690, box_loss: 0.00229, time: 1123.92504\n","Val 0000000010_0000000000.csv Epoch: 51, summary loss: 0.53309, class loss: 0.32741, box_loss: 0.00411, time: 172.21986\n","Val 0000000054_0000000000.csv Epoch: 51, summary loss: 0.53250, class loss: 0.42310, box_loss: 0.00219, time: 172.22310\n","Val 0000000056_0000000000.csv Epoch: 51, summary loss: 0.32335, class loss: 0.21480, box_loss: 0.00217, time: 172.22562\n","Val 0000000062_0000000000.csv Epoch: 51, summary loss: 0.88038, class loss: 0.52924, box_loss: 0.00702, time: 172.22775\n","Val 0000000067_0000000005.csv Epoch: 51, summary loss: 0.01783, class loss: 0.01783, box_loss: 0.00000, time: 172.22986\n","Val 0000000067_0000000012.csv Epoch: 51, summary loss: 0.11182, class loss: 0.11182, box_loss: 0.00000, time: 172.23199\n","Val 0000000067_0000000014.csv Epoch: 51, summary loss: 0.28819, class loss: 0.21157, box_loss: 0.00153, time: 172.23409\n","Val 0000000067_0000000015.csv Epoch: 51, summary loss: 0.41015, class loss: 0.24429, box_loss: 0.00332, time: 172.23615\n","Val 0000000067_0000000019.csv Epoch: 51, summary loss: 0.53541, class loss: 0.25956, box_loss: 0.00552, time: 172.23825\n","Val 0000000067_0000000024.csv Epoch: 51, summary loss: 0.29702, class loss: 0.15752, box_loss: 0.00279, time: 172.24038\n","Val 0000000067_0000000025.csv Epoch: 51, summary loss: 0.30823, class loss: 0.19614, box_loss: 0.00224, time: 172.24249\n","Val 0000000067_0000000026.csv Epoch: 51, summary loss: 0.03367, class loss: 0.03367, box_loss: 0.00000, time: 172.24460\n","Val 0000000067_0000000027.csv Epoch: 51, summary loss: 0.06175, class loss: 0.06175, box_loss: 0.00000, time: 172.24665\n","Val 0000000067_0000000028.csv Epoch: 51, summary loss: 0.10974, class loss: 0.09556, box_loss: 0.00028, time: 172.24868\n","Val 0000000067_0000000029.csv Epoch: 51, summary loss: 0.07367, class loss: 0.07367, box_loss: 0.00000, time: 172.25079\n","Val 0000000067_0000000031.csv Epoch: 51, summary loss: 0.23880, class loss: 0.15892, box_loss: 0.00160, time: 172.25294\n","Val 0000000067_0000000032.csv Epoch: 51, summary loss: 0.39914, class loss: 0.29725, box_loss: 0.00204, time: 172.25510\n","Val 0000000067_0000000040.csv Epoch: 51, summary loss: 0.30563, class loss: 0.24171, box_loss: 0.00128, time: 172.25726\n","Val 0000000067_0000000041.csv Epoch: 51, summary loss: 0.68171, class loss: 0.36947, box_loss: 0.00624, time: 172.25942\n","Val 0000000067_0000000045.csv Epoch: 51, summary loss: 0.43505, class loss: 0.36039, box_loss: 0.00149, time: 172.26158\n","Val 0000000067_0000000046.csv Epoch: 51, summary loss: 0.19483, class loss: 0.12199, box_loss: 0.00146, time: 172.26376\n","Val 0000000067_0000000050.csv Epoch: 51, summary loss: 0.83333, class loss: 0.54847, box_loss: 0.00570, time: 172.26588\n","Val 0000000067_0000000052.csv Epoch: 51, summary loss: 0.65947, class loss: 0.52900, box_loss: 0.00261, time: 172.26797\n","Val 0000000067_0000000055.csv Epoch: 51, summary loss: 0.36233, class loss: 0.31045, box_loss: 0.00104, time: 172.27009\n","Val 0000000067_0000000058.csv Epoch: 51, summary loss: 0.53798, class loss: 0.34329, box_loss: 0.00389, time: 172.27221\n","Val 0000000067_0000000059.csv Epoch: 51, summary loss: 0.41899, class loss: 0.41899, box_loss: 0.00000, time: 172.27432\n","Val 0000000351_0000000000.csv Epoch: 51, summary loss: 0.85734, class loss: 0.53485, box_loss: 0.00645, time: 172.27645\n","Val 0000000354_0000000000.csv Epoch: 51, summary loss: 1.16050, class loss: 0.83230, box_loss: 0.00656, time: 172.27857\n","Val 0000000359_0000000000.csv Epoch: 51, summary loss: 1.10726, class loss: 0.46991, box_loss: 0.01275, time: 172.30181\n","Val 0000000363_0000000000.csv Epoch: 51, summary loss: 0.13814, class loss: 0.13814, box_loss: 0.00000, time: 172.30391\n","Val 0000000364_0000000000.csv Epoch: 51, summary loss: 0.35983, class loss: 0.33561, box_loss: 0.00048, time: 172.30605\n","Val 0000000367_0000000000.csv Epoch: 51, summary loss: 0.55424, class loss: 0.38925, box_loss: 0.00330, time: 172.30810\n","Val Epoch: 51, summary loss: 0.43004, class loss: 0.29243, box_loss: 0.00275, time: 172.31036\n","Adjusting learning rate of group 0 to 8.9473e-05.\n","\n","2021-06-01T21:32:32.827711\n","LR: 8.947301918507947e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 0.72884\n","Train Step 250/3810, summary_loss: 0.30493, class_loss: 0.19513, box_loss: 0.00220, time: 75.13395\n","Train Step 500/3810, summary_loss: 0.30292, class_loss: 0.19316, box_loss: 0.00220, time: 148.38647\n","Train Step 750/3810, summary_loss: 0.30367, class_loss: 0.19518, box_loss: 0.00217, time: 222.40745\n","Train Step 1000/3810, summary_loss: 0.30644, class_loss: 0.19705, box_loss: 0.00219, time: 295.65068\n","Train Step 1250/3810, summary_loss: 0.30590, class_loss: 0.19682, box_loss: 0.00218, time: 369.35748\n","Train Step 1500/3810, summary_loss: 0.30474, class_loss: 0.19590, box_loss: 0.00218, time: 443.04598\n","Train Step 1750/3810, summary_loss: 0.30709, class_loss: 0.19713, box_loss: 0.00220, time: 516.85617\n","Train Step 2000/3810, summary_loss: 0.30656, class_loss: 0.19671, box_loss: 0.00220, time: 590.36052\n","Train Step 2250/3810, summary_loss: 0.30584, class_loss: 0.19617, box_loss: 0.00219, time: 664.09044\n","Train Step 2500/3810, summary_loss: 0.30594, class_loss: 0.19602, box_loss: 0.00220, time: 737.39599\n","Train Step 2750/3810, summary_loss: 0.30665, class_loss: 0.19619, box_loss: 0.00221, time: 811.01979\n","Train Step 3000/3810, summary_loss: 0.30547, class_loss: 0.19535, box_loss: 0.00220, time: 884.25823\n","Train Step 3250/3810, summary_loss: 0.30571, class_loss: 0.19540, box_loss: 0.00221, time: 957.93005\n","Train Step 3500/3810, summary_loss: 0.30622, class_loss: 0.19563, box_loss: 0.00221, time: 1031.24384\n","Train Step 3750/3810, summary_loss: 0.30644, class_loss: 0.19563, box_loss: 0.00222, time: 1104.95925\n","Train Epoch: 52, summary loss: 0.30634, class loss: 0.19552, box_loss: 0.00222, time: 1122.31067\n","Val 0000000010_0000000000.csv Epoch: 52, summary loss: 0.56838, class loss: 0.41708, box_loss: 0.00303, time: 172.61031\n","Val 0000000054_0000000000.csv Epoch: 52, summary loss: 0.67997, class loss: 0.56121, box_loss: 0.00238, time: 172.61309\n","Val 0000000056_0000000000.csv Epoch: 52, summary loss: 0.29972, class loss: 0.21335, box_loss: 0.00173, time: 172.61565\n","Val 0000000062_0000000000.csv Epoch: 52, summary loss: 0.74069, class loss: 0.49256, box_loss: 0.00496, time: 172.61778\n","Val 0000000067_0000000005.csv Epoch: 52, summary loss: 0.01131, class loss: 0.01131, box_loss: 0.00000, time: 172.61986\n","Val 0000000067_0000000012.csv Epoch: 52, summary loss: 0.03711, class loss: 0.03711, box_loss: 0.00000, time: 172.62206\n","Val 0000000067_0000000014.csv Epoch: 52, summary loss: 0.19154, class loss: 0.13093, box_loss: 0.00121, time: 172.62412\n","Val 0000000067_0000000015.csv Epoch: 52, summary loss: 0.52829, class loss: 0.21748, box_loss: 0.00622, time: 172.62624\n","Val 0000000067_0000000019.csv Epoch: 52, summary loss: 0.35209, class loss: 0.17742, box_loss: 0.00349, time: 172.62834\n","Val 0000000067_0000000024.csv Epoch: 52, summary loss: 0.37084, class loss: 0.17515, box_loss: 0.00391, time: 172.64186\n","Val 0000000067_0000000025.csv Epoch: 52, summary loss: 0.44346, class loss: 0.18092, box_loss: 0.00525, time: 172.64559\n","Val 0000000067_0000000026.csv Epoch: 52, summary loss: 0.01708, class loss: 0.01708, box_loss: 0.00000, time: 172.64764\n","Val 0000000067_0000000027.csv Epoch: 52, summary loss: 0.04511, class loss: 0.04511, box_loss: 0.00000, time: 172.64968\n","Val 0000000067_0000000028.csv Epoch: 52, summary loss: 0.09502, class loss: 0.05929, box_loss: 0.00071, time: 172.65177\n","Val 0000000067_0000000029.csv Epoch: 52, summary loss: 0.03905, class loss: 0.03905, box_loss: 0.00000, time: 172.65383\n","Val 0000000067_0000000031.csv Epoch: 52, summary loss: 0.39818, class loss: 0.21539, box_loss: 0.00366, time: 172.65597\n","Val 0000000067_0000000032.csv Epoch: 52, summary loss: 0.41347, class loss: 0.22350, box_loss: 0.00380, time: 172.65845\n","Val 0000000067_0000000040.csv Epoch: 52, summary loss: 0.21765, class loss: 0.15552, box_loss: 0.00124, time: 172.66061\n","Val 0000000067_0000000041.csv Epoch: 52, summary loss: 0.71495, class loss: 0.38361, box_loss: 0.00663, time: 172.66274\n","Val 0000000067_0000000045.csv Epoch: 52, summary loss: 0.34750, class loss: 0.20379, box_loss: 0.00287, time: 172.66484\n","Val 0000000067_0000000046.csv Epoch: 52, summary loss: 0.19623, class loss: 0.10547, box_loss: 0.00182, time: 172.66703\n","Val 0000000067_0000000050.csv Epoch: 52, summary loss: 0.81737, class loss: 0.53088, box_loss: 0.00573, time: 172.66912\n","Val 0000000067_0000000052.csv Epoch: 52, summary loss: 0.47248, class loss: 0.34411, box_loss: 0.00257, time: 172.67125\n","Val 0000000067_0000000055.csv Epoch: 52, summary loss: 0.32809, class loss: 0.26610, box_loss: 0.00124, time: 172.67335\n","Val 0000000067_0000000058.csv Epoch: 52, summary loss: 0.71559, class loss: 0.33982, box_loss: 0.00752, time: 172.67546\n","Val 0000000067_0000000059.csv Epoch: 52, summary loss: 0.25220, class loss: 0.25220, box_loss: 0.00000, time: 172.67759\n","Val 0000000351_0000000000.csv Epoch: 52, summary loss: 0.66294, class loss: 0.45495, box_loss: 0.00416, time: 172.67970\n","Val 0000000354_0000000000.csv Epoch: 52, summary loss: 1.14986, class loss: 0.66269, box_loss: 0.00974, time: 172.68178\n","Val 0000000359_0000000000.csv Epoch: 52, summary loss: 1.07519, class loss: 0.62457, box_loss: 0.00901, time: 172.68386\n","Val 0000000363_0000000000.csv Epoch: 52, summary loss: 0.04060, class loss: 0.04060, box_loss: 0.00000, time: 172.68595\n","Val 0000000364_0000000000.csv Epoch: 52, summary loss: 0.29594, class loss: 0.26640, box_loss: 0.00059, time: 172.68801\n","Val 0000000367_0000000000.csv Epoch: 52, summary loss: 0.65247, class loss: 0.44152, box_loss: 0.00422, time: 172.69008\n","Val Epoch: 52, summary loss: 0.41157, class loss: 0.25894, box_loss: 0.00305, time: 172.69231\n","Adjusting learning rate of group 0 to 8.8100e-05.\n","\n","2021-06-01T21:54:08.140266\n","LR: 8.809962419626495e-05\n","Train Step 0/3810, summary_loss: 0.00000, class_loss: 0.00000, box_loss: 0.00000, time: 1.30187\n","Train Step 250/3810, summary_loss: 0.31484, class_loss: 0.20214, box_loss: 0.00225, time: 74.78668\n","Train Step 500/3810, summary_loss: 0.30583, class_loss: 0.19528, box_loss: 0.00221, time: 148.62941\n","Train Step 750/3810, summary_loss: 0.30331, class_loss: 0.19371, box_loss: 0.00219, time: 222.34032\n","Train Step 1000/3810, summary_loss: 0.30349, class_loss: 0.19408, box_loss: 0.00219, time: 295.87358\n","Train Step 1250/3810, summary_loss: 0.30466, class_loss: 0.19413, box_loss: 0.00221, time: 369.28943\n","Train Step 1500/3810, summary_loss: 0.30617, class_loss: 0.19511, box_loss: 0.00222, time: 442.43283\n","Train Step 1750/3810, summary_loss: 0.30608, class_loss: 0.19504, box_loss: 0.00222, time: 515.97167\n","Train Step 2000/3810, summary_loss: 0.30686, class_loss: 0.19548, box_loss: 0.00223, time: 589.25340\n","Train Step 2250/3810, summary_loss: 0.30538, class_loss: 0.19453, box_loss: 0.00222, time: 663.37624\n","Train Step 2500/3810, summary_loss: 0.30468, class_loss: 0.19404, box_loss: 0.00221, time: 736.81704\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uUmIMHANirRn"},"source":[""],"execution_count":null,"outputs":[]}]}